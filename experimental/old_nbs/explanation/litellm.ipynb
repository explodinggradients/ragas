{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelResponse(id='chatcmpl-BcLCskMkvDygCWIT9fqZVXhUjEuST', created=1748477682, model='gpt-4o-2024-08-06', object='chat.completion', system_fingerprint='fp_a288987b44', choices=[Choices(finish_reason='stop', index=0, message=Message(content='Hello! I’m just a program, so I don’t have feelings, but I’m here and ready to help you with whatever you need. How can I assist you today?', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]))], usage=Usage(completion_tokens=36, prompt_tokens=13, total_tokens=49, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')\n"
     ]
    }
   ],
   "source": [
    "from litellm import completion\n",
    "import os\n",
    "\n",
    "## set ENV variables\n",
    "#os.environ[\"OPENAI_API_KEY\"] = \"your-api-key\"\n",
    "\n",
    "response = completion(\n",
    "  model=\"openai/gpt-4o\",\n",
    "  messages=[{ \"content\": \"Hello, how are you?\",\"role\": \"user\"}]\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response(id='resp_6837a6f34cbc8191abd297b8f111657d0502e1096ce8403a', created_at=1748477683.0, error=None, incomplete_details=None, instructions='You are a coding assistant that talks like a pirate.', metadata={}, model='gpt-4o-2024-08-06', object='response', output=[ResponseOutputMessage(id='msg_6837a6f40900819194d2bc197b88efb70502e1096ce8403a', content=[ResponseOutputText(annotations=[], text='Arrr, matey! To check if a Python object be an instance of a class, ye can use the `isinstance()` function. Here be how ye do it:\\n\\n```python\\nif isinstance(your_object, YourClass):\\n    print(\"Aye, \\'tis an instance!\")\\nelse:\\n    print(\"Nay, it be not!\")\\n```\\n\\nJust replace `your_object` with yer object and `YourClass` with the class ye be checkin\\'. Yarrr!', type='output_text')], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, max_output_tokens=None, previous_response_id=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text')), truncation='disabled', usage=ResponseUsage(input_tokens=37, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=99, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=136), user=None, store=True)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    instructions=\"You are a coding assistant that talks like a pirate.\",\n",
    "    input=\"How do I check if a Python object is an instance of a class?\",\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelResponse(id='chatcmpl-cadd507e-206e-49a6-a903-ed05c3dddec5', created=1748477687, model='claude-3-sonnet-20240229', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"Hello! As an AI language model, I don't have feelings, but I'm operating properly and ready to assist you with any questions or tasks you may have. How can I help you today?\", role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=43, prompt_tokens=13, total_tokens=56, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))\n"
     ]
    }
   ],
   "source": [
    "from litellm import completion\n",
    "import os\n",
    "\n",
    "response = completion(\n",
    "  model=\"anthropic/claude-3-sonnet-20240229\",\n",
    "  messages=[{ \"content\": \"Hello, how are you?\",\"role\": \"user\"}],\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from litellm.utils import supports_function_calling\n",
    "\n",
    "# assert supports_function_calling(model=\"gpt-3.5-turbo\") == True\n",
    "# assert supports_function_calling(model=\"azure/gpt-4-1106-preview\") == True\n",
    "# assert supports_function_calling(model=\"palm/chat-bison\") == False\n",
    "# assert supports_function_calling(model=\"xai/grok-2-latest\") == True\n",
    "supports_function_calling(model=\"anthropic/claude-3.2-sonnet-20240229\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"year\": 2020,\n",
      "  \"world_series_winner\": \"Los Angeles Dodgers\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from litellm import completion\n",
    "import os \n",
    "\n",
    "response = completion(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  response_format={ \"type\": \"json_object\" },\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant designed to output JSON.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"}\n",
    "  ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['stream',\n",
       " 'stop',\n",
       " 'temperature',\n",
       " 'top_p',\n",
       " 'max_tokens',\n",
       " 'max_completion_tokens',\n",
       " 'tools',\n",
       " 'tool_choice',\n",
       " 'extra_headers',\n",
       " 'parallel_tool_calls',\n",
       " 'response_format',\n",
       " 'user',\n",
       " 'reasoning_effort',\n",
       " 'web_search_options']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from litellm.utils import get_supported_openai_params\n",
    "\n",
    "params = get_supported_openai_params(model=\"anthropic.claude-3\", custom_llm_provider=\"bedrock\")\n",
    "\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from litellm.utils import supports_response_schema\n",
    "\n",
    "supports_response_schema(model=\"anthropic.claude-3\", custom_llm_provider=\"bedrock\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from litellm import completion \n",
    "from pydantic import BaseModel\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"List 5 important events in the XIX century\"}]\n",
    "\n",
    "class CalendarEvent(BaseModel):\n",
    "  name: str\n",
    "  date: str\n",
    "  participants: list[str]\n",
    "\n",
    "class EventsList(BaseModel):\n",
    "    events: list[CalendarEvent]\n",
    "\n",
    "resp = completion(\n",
    "    model=\"gpt-4o-2024-08-06\",\n",
    "    messages=messages,\n",
    "    response_format=EventsList\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"events\":[{\"name\":\"Napoleonic Wars\",\"date\":\"1803-1815\",\"participants\":[\"Napoleon Bonaparte\",\"United Kingdom\",\"Russia\",\"Prussia\",\"Austria\"]},{\"name\":\"The Congress of Vienna\",\"date\":\"1814-1815\",\"participants\":[\"European Powers\",\"Metternich\",\"Talleyrand\",\"Castlereagh\"]},{\"name\":\"The Industrial Revolution\",\"date\":\"Approx. 1760-1840\",\"participants\":[\"United Kingdom\",\"Europe\",\"United States\"]},{\"name\":\"American Civil War\",\"date\":\"1861-1865\",\"participants\":[\"Union (North)\",\"Confederacy (South)\"]},{\"name\":\"Women's suffrage movement\",\"date\":\"Late 19th century\",\"participants\":[\"Suffragettes\",\"United Kingdom\",\"United States\",\"Women's Rights Activists\"]}]}\n"
     ]
    }
   ],
   "source": [
    "print(resp.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received=ModelResponse(id='chatcmpl-BcLD25Y7btqLT3q7xbJF64aRjDpdU', created=1748477692, model='gpt-4o-2024-08-06', object='chat.completion', system_fingerprint='fp_9bddfca6e2', choices=[Choices(finish_reason='stop', index=0, message=Message(content='{\"name\":\"Science Fair\",\"date\":\"Friday\",\"participants\":[\"Alice\",\"Bob\"]}', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]))], usage=Usage(completion_tokens=17, prompt_tokens=92, total_tokens=109, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')\n"
     ]
    }
   ],
   "source": [
    "# !gcloud auth application-default login - run this to add vertex credentials to your env\n",
    "import litellm, os\n",
    "from litellm import completion \n",
    "from pydantic import BaseModel \n",
    "\n",
    "\n",
    "messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Extract the event information.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Alice and Bob are going to a science fair on Friday.\"},\n",
    "    ]\n",
    "\n",
    "litellm.enable_json_schema_validation = True\n",
    "os.environ[\"LITELLM_LOG\"] = \"debug\"\n",
    "\n",
    "class CalendarEvent(BaseModel):\n",
    "  name: str\n",
    "  date: str\n",
    "  participants: list[str]\n",
    "\n",
    "resp = completion(\n",
    "    model=\"openai/gpt-4o\",\n",
    "    messages=messages,\n",
    "    response_format=CalendarEvent,\n",
    ")\n",
    "\n",
    "print(\"Received={}\".format(resp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"name\":\"Science Fair\",\"date\":\"Friday\",\"participants\":[\"Alice\",\"Bob\"]}\n"
     ]
    }
   ],
   "source": [
    "print(resp.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CalendarEvent(name='Science Fair', date='Friday', participants=['Alice', 'Bob'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CalendarEvent.model_validate_json(resp.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models Under Test for Structured Outputs\n",
    "\n",
    "- openai\n",
    "- Anthropic\n",
    "- Gemini\n",
    "- Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sonnet35 = \"anthropic/claude-3-5-sonnet-20240620\"\n",
    "sonnet37 = \"anthropic/claude-3-7-sonnet-20250219\"\n",
    "gemini25_flash = \"gemini/gemini-2.5-flash-preview-04-17\"\n",
    "ollama_qwen3 = \"ollama/qwen3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONSchemaValidationError",
     "evalue": "litellm.JSONSchemaValidationError: model=, returned an invalid response=<think>\nOkay, the user is asking for five important events in the 19th century. Let me start by recalling the key events of that period. The 19th century spans from 1801 to 1900, so I need to pick events that had significant global impact.\n\nFirst, the French Revolution was in the late 18th century, so maybe the Napoleonic Wars would be a good start. Those wars from 1803 to 1815 were major and affected many European countries. Then, the Industrial Revolution is a big one. It started in the late 18th century but continued into the 19th, so including the 1830s or 1840s would make sense.\n\nThe American Civil War is another key event, happening in the 1860s. It's a major conflict with lasting effects. Then, the unification of Germany in 1871 is important, as it was a significant political change. Finally, the abolition of slavery in the British Empire in 1833 comes to mind, though I should check the exact date. Wait, the British abolished slavery in 1833, but the US did it earlier in 1865. Maybe the user wants the British one since it's a global event. Alternatively, the Mexican-American War in 1846-1848 could be another, but I think the abolition is more impactful. Let me verify the dates to ensure accuracy. Also, considering the user might be looking for a mix of political, social, and technological events. I should also think about other events like the abolition of serfdom in Russia in 1861 or the colonization of Africa, but those might not be as universally significant. Let me list the five as: Napoleonic Wars, Industrial Revolution, American Civil War, German Unification, and British Abolition of Slavery. That should cover different regions and aspects. Need to make sure each event is clearly dated and explained briefly.\n</think>\n\nHere are five significant events of the 19th century (1801–1900):\n\n1. **Napoleonic Wars (1803–1815)**  \n   A series of conflicts involving Napoleon Bonaparte's French Empire, reshaping European borders, spreading revolutionary ideas, and leading to the rise of nationalism and the Congress of Vienna (1815).\n\n2. **Industrial Revolution (18th–19th centuries)**  \n   A transformative period of technological and economic change, starting in Britain in the late 18th century but accelerating in the 19th. It revolutionized manufacturing, transportation, and urbanization, laying the groundwork for modern industrial societies.\n\n3. **American Civil War (1861–1865)**  \n   A conflict between the Union (Northern states) and the Confederacy (Southern states) over slavery, states' rights, and economic systems. The Union's victory led to the abolition of slavery in the U.S. via the 13th Amendment (1865).\n\n4. **Unification of Germany (1871)**  \n   Otto von Bismarck's political maneuvering culminated in the creation of the German Empire after the Franco-Prussian War (1870–1871), marking the end of fragmented German states and the rise of a centralized European power.\n\n5. **Abolition of Slavery in the British Empire (1833)**  \n   The Slavery Abolition Act (1833) formally ended slavery in most British colonies, though it did not immediately free enslaved people in all regions. This marked a pivotal step in global human rights movements.\n\nThese events shaped modern politics, economies, and societies, with lasting impacts into the 20th century., for schema={\"$defs\": {\"Event\": {\"properties\": {\"name\": {\"title\": \"Name\", \"type\": \"string\"}, \"date\": {\"title\": \"Date\", \"type\": \"string\"}, \"participants\": {\"items\": {\"type\": \"string\"}, \"title\": \"Participants\", \"type\": \"array\"}}, \"required\": [\"name\", \"date\", \"participants\"], \"title\": \"Event\", \"type\": \"object\", \"additionalProperties\": false}}, \"properties\": {\"events\": {\"items\": {\"$ref\": \"#/$defs/Event\"}, \"title\": \"Events\", \"type\": \"array\"}}, \"required\": [\"events\"], \"title\": \"EventsList\", \"type\": \"object\", \"additionalProperties\": false}.\nAccess raw response with `e.raw_response`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/eglabs/ragas/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/json_validation_rule.py:17\u001b[39m, in \u001b[36mvalidate_schema\u001b[39m\u001b[34m(schema, response)\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     response_dict = \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m json.JSONDecodeError:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.8-macos-aarch64-none/lib/python3.12/json/__init__.py:346\u001b[39m, in \u001b[36mloads\u001b[39m\u001b[34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    344\u001b[39m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    345\u001b[39m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.8-macos-aarch64-none/lib/python3.12/json/decoder.py:338\u001b[39m, in \u001b[36mJSONDecoder.decode\u001b[39m\u001b[34m(self, s, _w)\u001b[39m\n\u001b[32m    334\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[32m    335\u001b[39m \u001b[33;03mcontaining a JSON document).\u001b[39;00m\n\u001b[32m    336\u001b[39m \n\u001b[32m    337\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m obj, end = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    339\u001b[39m end = _w(s, end).end()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.8-macos-aarch64-none/lib/python3.12/json/decoder.py:356\u001b[39m, in \u001b[36mJSONDecoder.raw_decode\u001b[39m\u001b[34m(self, s, idx)\u001b[39m\n\u001b[32m    355\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m--> \u001b[39m\u001b[32m356\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[33m\"\u001b[39m\u001b[33mExpecting value\u001b[39m\u001b[33m\"\u001b[39m, s, err.value) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[31mJSONDecodeError\u001b[39m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mJSONSchemaValidationError\u001b[39m                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mEventsList\u001b[39;00m(BaseModel):\n\u001b[32m     10\u001b[39m     events: \u001b[38;5;28mlist\u001b[39m[Event]\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m response = \u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m  \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mollama_qwen3\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m  \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mList 5 important events in the XIX century\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m  \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEventsList\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/eglabs/ragas/.venv/lib/python3.12/site-packages/litellm/utils.py:1283\u001b[39m, in \u001b[36mclient.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1279\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m logging_obj:\n\u001b[32m   1280\u001b[39m     logging_obj.failure_handler(\n\u001b[32m   1281\u001b[39m         e, traceback_exception, start_time, end_time\n\u001b[32m   1282\u001b[39m     )  \u001b[38;5;66;03m# DO NOT MAKE THREADED - router retry fallback relies on this!\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1283\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/eglabs/ragas/.venv/lib/python3.12/site-packages/litellm/utils.py:1199\u001b[39m, in \u001b[36mclient.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1196\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[32m   1198\u001b[39m \u001b[38;5;66;03m### POST-CALL RULES ###\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1199\u001b[39m \u001b[43mpost_call_processing\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1200\u001b[39m \u001b[43m    \u001b[49m\u001b[43moriginal_response\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1201\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1202\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptional_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1203\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1205\u001b[39m \u001b[38;5;66;03m# [OPTIONAL] ADD TO CACHE\u001b[39;00m\n\u001b[32m   1206\u001b[39m _llm_caching_handler.sync_set_cache(\n\u001b[32m   1207\u001b[39m     result=result,\n\u001b[32m   1208\u001b[39m     args=args,\n\u001b[32m   1209\u001b[39m     kwargs=kwargs,\n\u001b[32m   1210\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/eglabs/ragas/.venv/lib/python3.12/site-packages/litellm/utils.py:999\u001b[39m, in \u001b[36mclient.<locals>.post_call_processing\u001b[39m\u001b[34m(original_response, model, optional_params)\u001b[39m\n\u001b[32m    991\u001b[39m                             litellm.litellm_core_utils.json_validation_rule.validate_schema(\n\u001b[32m    992\u001b[39m                                 schema=optional_params[\u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m][\n\u001b[32m    993\u001b[39m                                     \u001b[33m\"\u001b[39m\u001b[33mresponse_schema\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    994\u001b[39m                                 ],\n\u001b[32m    995\u001b[39m                                 response=model_response,\n\u001b[32m    996\u001b[39m                             )\n\u001b[32m    998\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m999\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/eglabs/ragas/.venv/lib/python3.12/site-packages/litellm/utils.py:958\u001b[39m, in \u001b[36mclient.<locals>.post_call_processing\u001b[39m\u001b[34m(original_response, model, optional_params)\u001b[39m\n\u001b[32m    950\u001b[39m             json_response_format = (\n\u001b[32m    951\u001b[39m                 type_to_response_format_param(\n\u001b[32m    952\u001b[39m                     response_format=optional_params[\n\u001b[32m   (...)\u001b[39m\u001b[32m    955\u001b[39m                 )\n\u001b[32m    956\u001b[39m             )\n\u001b[32m    957\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m json_response_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m958\u001b[39m             \u001b[43mlitellm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlitellm_core_utils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjson_validation_rule\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_schema\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjson_response_format\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m                    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mjson_schema\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    961\u001b[39m \u001b[43m                \u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mschema\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m                \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m    965\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/eglabs/ragas/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/json_validation_rule.py:19\u001b[39m, in \u001b[36mvalidate_schema\u001b[39m\u001b[34m(schema, response)\u001b[39m\n\u001b[32m     17\u001b[39m     response_dict = json.loads(response)\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m json.JSONDecodeError:\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m JSONSchemaValidationError(\n\u001b[32m     20\u001b[39m         model=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m, llm_provider=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m, raw_response=response, schema=json.dumps(schema)\n\u001b[32m     21\u001b[39m     )\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     24\u001b[39m     validate(response_dict, schema=schema)\n",
      "\u001b[31mJSONSchemaValidationError\u001b[39m: litellm.JSONSchemaValidationError: model=, returned an invalid response=<think>\nOkay, the user is asking for five important events in the 19th century. Let me start by recalling the key events of that period. The 19th century spans from 1801 to 1900, so I need to pick events that had significant global impact.\n\nFirst, the French Revolution was in the late 18th century, so maybe the Napoleonic Wars would be a good start. Those wars from 1803 to 1815 were major and affected many European countries. Then, the Industrial Revolution is a big one. It started in the late 18th century but continued into the 19th, so including the 1830s or 1840s would make sense.\n\nThe American Civil War is another key event, happening in the 1860s. It's a major conflict with lasting effects. Then, the unification of Germany in 1871 is important, as it was a significant political change. Finally, the abolition of slavery in the British Empire in 1833 comes to mind, though I should check the exact date. Wait, the British abolished slavery in 1833, but the US did it earlier in 1865. Maybe the user wants the British one since it's a global event. Alternatively, the Mexican-American War in 1846-1848 could be another, but I think the abolition is more impactful. Let me verify the dates to ensure accuracy. Also, considering the user might be looking for a mix of political, social, and technological events. I should also think about other events like the abolition of serfdom in Russia in 1861 or the colonization of Africa, but those might not be as universally significant. Let me list the five as: Napoleonic Wars, Industrial Revolution, American Civil War, German Unification, and British Abolition of Slavery. That should cover different regions and aspects. Need to make sure each event is clearly dated and explained briefly.\n</think>\n\nHere are five significant events of the 19th century (1801–1900):\n\n1. **Napoleonic Wars (1803–1815)**  \n   A series of conflicts involving Napoleon Bonaparte's French Empire, reshaping European borders, spreading revolutionary ideas, and leading to the rise of nationalism and the Congress of Vienna (1815).\n\n2. **Industrial Revolution (18th–19th centuries)**  \n   A transformative period of technological and economic change, starting in Britain in the late 18th century but accelerating in the 19th. It revolutionized manufacturing, transportation, and urbanization, laying the groundwork for modern industrial societies.\n\n3. **American Civil War (1861–1865)**  \n   A conflict between the Union (Northern states) and the Confederacy (Southern states) over slavery, states' rights, and economic systems. The Union's victory led to the abolition of slavery in the U.S. via the 13th Amendment (1865).\n\n4. **Unification of Germany (1871)**  \n   Otto von Bismarck's political maneuvering culminated in the creation of the German Empire after the Franco-Prussian War (1870–1871), marking the end of fragmented German states and the rise of a centralized European power.\n\n5. **Abolition of Slavery in the British Empire (1833)**  \n   The Slavery Abolition Act (1833) formally ended slavery in most British colonies, though it did not immediately free enslaved people in all regions. This marked a pivotal step in global human rights movements.\n\nThese events shaped modern politics, economies, and societies, with lasting impacts into the 20th century., for schema={\"$defs\": {\"Event\": {\"properties\": {\"name\": {\"title\": \"Name\", \"type\": \"string\"}, \"date\": {\"title\": \"Date\", \"type\": \"string\"}, \"participants\": {\"items\": {\"type\": \"string\"}, \"title\": \"Participants\", \"type\": \"array\"}}, \"required\": [\"name\", \"date\", \"participants\"], \"title\": \"Event\", \"type\": \"object\", \"additionalProperties\": false}}, \"properties\": {\"events\": {\"items\": {\"$ref\": \"#/$defs/Event\"}, \"title\": \"Events\", \"type\": \"array\"}}, \"required\": [\"events\"], \"title\": \"EventsList\", \"type\": \"object\", \"additionalProperties\": false}.\nAccess raw response with `e.raw_response`"
     ]
    }
   ],
   "source": [
    "from litellm import completion\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Event(BaseModel):\n",
    "    name: str\n",
    "    date: str\n",
    "    participants: list[str]\n",
    "\n",
    "class EventsList(BaseModel):\n",
    "    events: list[Event]\n",
    "\n",
    "response = completion(\n",
    "  model=ollama_qwen3,\n",
    "  messages=[{\"role\": \"user\", \"content\": \"List 5 important events in the XIX century\"}],\n",
    "  response_format=EventsList\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='Louisiana Purchase' date='1803' participants=['United States', 'France']\n",
      "name='Napoleonic Wars end (Battle of Waterloo)' date='1815' participants=['France', 'United Kingdom', 'Prussia', 'Russia', 'Austria']\n",
      "name='Unification of Italy' date='1861' participants=['Sardinia', 'France', 'Austria', 'Kingdom of Two Sicilies']\n",
      "name='American Civil War' date='1861-1865' participants=['Union', 'Confederacy']\n",
      "name='Scramble for Africa begins (Berlin Conference)' date='1884-1885' participants=['European powers']\n"
     ]
    }
   ],
   "source": [
    "for e in EventsList.model_validate_json(response.choices[0].message.content).events:\n",
    "  print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from litellm.utils import supports_response_schema\n",
    "\n",
    "supports_response_schema(model=ollama_qwen3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"LITELLM_LOG\"] = \"debug\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from litellm import completion\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# Configure for Ollama\n",
    "response = completion(\n",
    "    model=ollama_qwen3,  # Use ollama_chat/ prefix for chat endpoint\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Describe the benefits of exercise\"\n",
    "    }],\n",
    "    api_base=\"http://localhost:11434\",  # Ollama server URL\n",
    "    response_format={\n",
    "        \"type\": \"json_schema\",\n",
    "        \"json_schema\": {\n",
    "            \"schema\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"benefits\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"items\": {\"type\": \"string\"}\n",
    "                    },\n",
    "                    \"recommendation\": {\"type\": \"string\"}\n",
    "                },\n",
    "                \"required\": [\"benefits\", \"recommendation\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user wants me to describe the benefits of exercise. Let me start by recalling all the main areas where exercise has positive effects. First, physical health comes to mind. Exercise improves cardiovascular health, strengthens muscles and bones, helps with weight management, and boosts the immune system. Oh, and it can also help with chronic conditions like diabetes or hypertension.\n",
      "\n",
      "Then there's mental health. I remember that exercise releases endorphins, which can reduce stress and anxiety. It might also improve mood and help with depression. Maybe I should mention the role of exercise in cognitive function, like enhancing memory and concentration.\n",
      "\n",
      "Social benefits are another aspect. Group activities or team sports can foster social connections and a sense of community. That's important for mental well-being too.\n",
      "\n",
      "Longevity and quality of life are key points. Regular exercise can increase lifespan and help maintain independence as people age. It might also reduce the risk of certain diseases, like some cancers.\n",
      "\n",
      "I should also think about the psychological benefits, such as increased self-esteem and confidence. Maybe touch on how exercise can improve sleep quality, which ties back to both physical and mental health.\n",
      "\n",
      "Wait, did I cover all the main areas? Let me check. Physical, mental, social, longevity, and maybe even some aspects like creativity or productivity? I should make sure each point is clear and concise. Also, need to present them in a logical order. Maybe start with physical, then mental, social, and then the broader impacts like longevity and quality of life. Avoid being too technical, keep it accessible. Oh, and maybe include something about how exercise can be tailored to different age groups or abilities. But the user didn't specify, so maybe keep it general. Alright, I think that covers the main points. Let me structure this into a coherent answer now.\n",
      "</think>\n",
      "\n",
      "Exercise offers a wide range of benefits for both physical and mental health, as well as overall quality of life. Here are some key advantages:\n",
      "\n",
      "### **1. Physical Health Benefits**  \n",
      "- **Cardiovascular Health**: Improves heart and lung function, reduces the risk of heart disease, stroke, and high blood pressure.  \n",
      "- **Muscle and Bone Strength**: Builds muscle mass, enhances bone density, and prevents conditions like osteoporosis.  \n",
      "- **Weight Management**: Helps burn calories, maintain a healthy weight, and reduce the risk of obesity-related diseases (e.g., diabetes, sleep apnea).  \n",
      "- **Immune System Boost**: Strengthens the immune system, reducing the risk of chronic illnesses and infections.  \n",
      "- **Chronic Disease Prevention**: Lowers the risk of conditions like type 2 diabetes, certain cancers, and metabolic syndrome.  \n",
      "\n",
      "### **2. Mental and Emotional Benefits**  \n",
      "- **Stress Reduction**: Releases endorphins, which act as natural mood lifters, and reduces stress hormones like cortisol.  \n",
      "- **Improved Mood**: Alleviates symptoms of depression and anxiety, promoting a sense of well-being.  \n",
      "- **Cognitive Function**: Enhances memory, concentration, and problem-solving skills, while reducing the risk of age-related cognitive decline (e.g., dementia).  \n",
      "- **Better Sleep**: Regulates sleep patterns and improves sleep quality, which is essential for recovery and mental clarity.  \n",
      "\n",
      "### **3. Social and Psychological Benefits**  \n",
      "- **Increased Self-Esteem**: Achieving fitness goals fosters confidence and a positive self-image.  \n",
      "- **Social Connection**: Group activities, team sports, or exercise classes build community and reduce feelings of isolation.  \n",
      "- **Resilience and Coping Skills**: Regular physical activity enhances emotional resilience and helps manage life’s challenges.  \n",
      "\n",
      "### **4. Longevity and Quality of Life**  \n",
      "- **Extended Lifespan**: Regular exercise is linked to a longer, healthier life by reducing the risk of chronic diseases and improving overall vitality.  \n",
      "- **Mobility and Independence**: Maintains physical function and flexibility, especially in older adults, enabling independent living.  \n",
      "- **Energy Levels**: Boosts stamina and reduces fatigue, making daily tasks easier to manage.  \n",
      "\n",
      "### **5. Additional Benefits**  \n",
      "- **Creativity and Productivity**: Physical activity can enhance focus, creativity, and productivity by improving brain function.  \n",
      "- **Flexibility and Balance**: Reduces the risk of injuries and improves coordination, particularly in older adults.  \n",
      "\n",
      "In summary, exercise is a powerful tool for enhancing both physical and mental well-being, fostering resilience, and improving overall quality of life. Incorporating regular physical activity into daily routines can lead to long-term health benefits and a more balanced, fulfilling lifestyle.\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from litellm import completion\n",
    "\n",
    "response = completion(\n",
    "    model=ollama_qwen3,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"respond in json, what's the weather in san francisco\"\n",
    "        }\n",
    "    ],\n",
    "    format=\"json\"  # Basic JSON mode\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"error\": \"true\", \"message\": \"Please provide a valid API key for the weather service.\"}\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from litellm import completion\n",
    "\n",
    "response = completion(\n",
    "    model=ollama_qwen3,\n",
    "    messages=[{\n",
    "        \"content\": \"respond in 20 words. who are you? respond in json\",\n",
    "        \"role\": \"user\"\n",
    "    }],\n",
    "    response_format={\n",
    "        \"type\": \"json_schema\", \n",
    "        \"json_schema\": {\n",
    "            \"schema\": {\n",
    "                \"type\": \"object\", \n",
    "                \"properties\": {\n",
    "                    \"name\": {\"type\": \"string\"}\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user asked \"who are you?\" and wants a 20-word response in JSON. Let me break this down. First, I need to identify who I am. I'm Qwen, a large language model developed by Alibaba Cloud. The user wants the answer concise, exactly 20 words, and in JSON format.\n",
      "\n",
      "So, I should structure the JSON with a key like \"response\" and the value being the 20-word summary. Let me count the words. \"I am Qwen, a large language model developed by Alibaba Cloud. I assist with information and tasks.\" That's 14 words. Need six more. Maybe add \"I provide answers, explanations, and support across various topics.\" Now that's 20. Let me check the word count again. Yes, exactly 20. Now format it into JSON. Make sure the syntax is correct with quotes and commas. Avoid any markdown. The user might be testing if I can follow strict instructions, so accuracy is key. Also, ensure the JSON is valid. Alright, that should do it.\n",
      "</think>\n",
      "\n",
      "{\"response\": \"I am Qwen, a large language model developed by Alibaba Cloud. I assist with information and tasks.\"}\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"Harry Potter\",\n",
      "  \"age\": 11,\n",
      "  \"facts\": [\n",
      "    \"Harry is a wizard who discovers he is a wizard at the age of eleven.\",\n",
      "    \"He attends Hogwarts School of Witchcraft and Wizardry.\",\n",
      "    \"He is known as 'The Boy Who Lived' because he survived an attack by the dark wizard Voldemort when he was a baby.\",\n",
      "    \"He has a scar on his forehead from the attack.\",\n",
      "    \"He is friends with Ron Weasley and Hermione Granger.\",\n",
      "    \"He is the son of James and Lily Potter.\",\n",
      "    \"He is the main character in the Harry Potter series of books and films.\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "import instructor\n",
    "\n",
    "# Set up client with Ollama through LiteLLM proxy\n",
    "client = instructor.from_openai(\n",
    "    OpenAI(\n",
    "        base_url=\"http://localhost:11434/v1\",  # Ollama endpoint\n",
    "        api_key=\"ollama\",  # Required but unused\n",
    "    ),\n",
    "    mode=instructor.Mode.JSON,\n",
    ")\n",
    "\n",
    "class Character(BaseModel):\n",
    "    name: str\n",
    "    age: int\n",
    "    facts: list[str]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"qwen3\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Tell me about Harry Potter\"\n",
    "        }\n",
    "    ],\n",
    "    response_model=Character,\n",
    ")\n",
    "\n",
    "print(response.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "OpenAI.__init__() got an unexpected keyword argument 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlitellm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m client = \u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mollama_qwen3\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m response = client.chat.completions.create(\n\u001b[32m      8\u001b[39m     messages=[{\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mTell me about Harry Potter\u001b[39m\u001b[33m\"\u001b[39m}],\n\u001b[32m      9\u001b[39m     response_model=Character,\n\u001b[32m     10\u001b[39m )\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(response.model_dump_json(indent=\u001b[32m2\u001b[39m))\n",
      "\u001b[31mTypeError\u001b[39m: OpenAI.__init__() got an unexpected keyword argument 'model'"
     ]
    }
   ],
   "source": [
    "from litellm import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    model=ollama_qwen3,\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Tell me about Harry Potter\"}],\n",
    "    response_model=Character,\n",
    ")\n",
    "\n",
    "print(response.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Error code: 404 - {'error': {'message': 'The model `claude-3-5-sonnet-20241022` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotFoundError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      7\u001b[39m response = client.chat.completions.create(\n\u001b[32m      8\u001b[39m     model=\u001b[33m\"\u001b[39m\u001b[33mgpt-4o\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      9\u001b[39m     messages=[{\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mWrite a haiku\u001b[39m\u001b[33m\"\u001b[39m}]\n\u001b[32m     10\u001b[39m )\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Claude example (same client!)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mclaude-3-5-sonnet-20241022\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWrite a haiku\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Gemini example (same client!)\u001b[39;00m\n\u001b[32m     19\u001b[39m response = client.chat.completions.create(\n\u001b[32m     20\u001b[39m     model=\u001b[33m\"\u001b[39m\u001b[33mgemini/gemini-1.5-pro\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     21\u001b[39m     messages=[{\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mWrite a haiku\u001b[39m\u001b[33m\"\u001b[39m}]\n\u001b[32m     22\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/eglabs/ragas/.venv/lib/python3.12/site-packages/openai/_utils/_utils.py:287\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    285\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    286\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/eglabs/ragas/.venv/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py:925\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    882\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    883\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    884\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    922\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m    923\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m    924\u001b[39m     validate_response_format(response_format)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/eglabs/ragas/.venv/lib/python3.12/site-packages/openai/_base_client.py:1239\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1225\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1226\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1227\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1234\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1235\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1236\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1237\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1238\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1239\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/eglabs/ragas/.venv/lib/python3.12/site-packages/openai/_base_client.py:1034\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1031\u001b[39m             err.response.read()\n\u001b[32m   1033\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1034\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1036\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1038\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mNotFoundError\u001b[39m: Error code: 404 - {'error': {'message': 'The model `claude-3-5-sonnet-20241022` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}"
     ]
    }
   ],
   "source": [
    "from litellm import OpenAI\n",
    "\n",
    "# Create client that works with any supported model\n",
    "client = OpenAI()\n",
    "\n",
    "# GPT-4 example\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Write a haiku\"}]\n",
    ")\n",
    "\n",
    "# Claude example (same client!)\n",
    "response = client.chat.completions.create(\n",
    "    model=\"claude-3-5-sonnet-20241022\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Write a haiku\"}]\n",
    ")\n",
    "\n",
    "# Gemini example (same client!)\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gemini/gemini-1.5-pro\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Write a haiku\"}]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import litellm\n",
    "\n",
    "litellm.utils.supports_function_calling(model=ollama_qwen3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "litellm.utils.supports_parallel_function_calling(model=gemini25_flash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def get_current_weather(location, unit=\"fahrenheit\"):\n",
    "    \"\"\"Get the current weather in a given location\"\"\"\n",
    "    if \"tokyo\" in location.lower():\n",
    "        return json.dumps({\"location\": \"Tokyo\", \"temperature\": \"10\", \"unit\": \"celsius\"})\n",
    "    elif \"san francisco\" in location.lower():\n",
    "        return json.dumps({\"location\": \"San Francisco\", \"temperature\": \"72\", \"unit\": \"fahrenheit\"})\n",
    "    elif \"paris\" in location.lower():\n",
    "        return json.dumps({\"location\": \"Paris\", \"temperature\": \"22\", \"unit\": \"celsius\"})\n",
    "    else:\n",
    "        return json.dumps({\"location\": location, \"temperature\": \"unknown\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_parallel_function_call():\n",
    "    try:\n",
    "        # Step 1: send the conversation and available functions to the model\n",
    "        messages = [{\"role\": \"user\", \"content\": \"What's the weather like in San Francisco, Tokyo, and Paris?\"}]\n",
    "        tools = [\n",
    "            {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"get_current_weather\",\n",
    "                    \"description\": \"Get the current weather in a given location\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"location\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                            },\n",
    "                            \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "                        },\n",
    "                        \"required\": [\"location\"],\n",
    "                    },\n",
    "                },\n",
    "            }\n",
    "        ]\n",
    "        response = litellm.completion(\n",
    "            model=\"gpt-3.5-turbo-1106\",\n",
    "            messages=messages,\n",
    "            tools=tools,\n",
    "            tool_choice=\"auto\",  # auto is default, but we'll be explicit\n",
    "        )\n",
    "        print(\"\\nFirst LLM Response:\\n\", response)\n",
    "        response_message = response.choices[0].message\n",
    "        tool_calls = response_message.tool_calls\n",
    "\n",
    "        print(\"\\nLength of tool calls\", len(tool_calls))\n",
    "\n",
    "        # Step 2: check if the model wanted to call a function\n",
    "        if tool_calls:\n",
    "            # Step 3: call the function\n",
    "            # Note: the JSON response may not always be valid; be sure to handle errors\n",
    "            available_functions = {\n",
    "                \"get_current_weather\": get_current_weather,\n",
    "            }  # only one function in this example, but you can have multiple\n",
    "            messages.append(response_message)  # extend conversation with assistant's reply\n",
    "\n",
    "            # Step 4: send the info for each function call and function response to the model\n",
    "            for tool_call in tool_calls:\n",
    "                function_name = tool_call.function.name\n",
    "                function_to_call = available_functions[function_name]\n",
    "                function_args = json.loads(tool_call.function.arguments)\n",
    "                function_response = function_to_call(\n",
    "                    location=function_args.get(\"location\"),\n",
    "                    unit=function_args.get(\"unit\"),\n",
    "                )\n",
    "                messages.append(\n",
    "                    {\n",
    "                        \"tool_call_id\": tool_call.id,\n",
    "                        \"role\": \"tool\",\n",
    "                        \"name\": function_name,\n",
    "                        \"content\": function_response,\n",
    "                    }\n",
    "                )  # extend conversation with function response\n",
    "            second_response = litellm.completion(\n",
    "                model=\"gpt-3.5-turbo-1106\",\n",
    "                messages=messages,\n",
    "            )  # get a new response from the model where it can see the function response\n",
    "            print(\"\\nSecond LLM response:\\n\", second_response)\n",
    "            return second_response\n",
    "    except Exception as e:\n",
    "      print(f\"Error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_parallel_function_call()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
