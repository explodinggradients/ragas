{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "> A python list like object that contains your evaluation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jjmachan/workspace/eglabs/ragas/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# | export\n",
    "from abc import ABC, abstractmethod\n",
    "import os\n",
    "import typing as t\n",
    "import csv\n",
    "import uuid\n",
    "\n",
    "from fastcore.utils import patch\n",
    "import pandas as pd\n",
    "\n",
    "from ragas_experimental.model.pydantic_model import ExtendedPydanticBaseModel as BaseModel\n",
    "from ragas_experimental.utils import create_nano_id, async_to_sync, get_test_directory\n",
    "from ragas_experimental.backends.ragas_api_client import RagasApiClient\n",
    "from ragas_experimental.typing import SUPPORTED_BACKENDS\n",
    "import ragas_experimental.typing as rt\n",
    "from ragas_experimental.metric import MetricResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "BaseModelType = t.TypeVar(\"BaseModelType\", bound=BaseModel)\n",
    "\n",
    "class DatasetBackend(ABC):\n",
    "    \"\"\"Abstract base class for dataset backends.\n",
    "    \n",
    "    All dataset storage backends must implement these methods.\n",
    "    \"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def initialize(self, dataset): \n",
    "        \"\"\"Initialize the backend with dataset information\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_column_mapping(self, model): \n",
    "        \"\"\"Get mapping between model fields and backend columns\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def load_entries(self, model_class): \n",
    "        \"\"\"Load all entries from storage\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def append_entry(self, entry): \n",
    "        \"\"\"Add a new entry to storage and return its ID\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def update_entry(self, entry): \n",
    "        \"\"\"Update an existing entry in storage\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def delete_entry(self, entry_id): \n",
    "        \"\"\"Delete an entry from storage\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_entry_by_field(self, field_name: str, field_value: t.Any, model_class):\n",
    "        \"\"\"Get an entry by field value\"\"\"\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class RagasAppBackend(DatasetBackend):\n",
    "    \"\"\"Backend for storing datasets using the Ragas API.\"\"\"\n",
    "    \n",
    "    def __init__(self, ragas_api_client, project_id, dataset_id):\n",
    "        \"\"\"Initialize the RagasAppBackend.\n",
    "        \n",
    "        Args:\n",
    "            ragas_api_client: The RagasApiClient instance\n",
    "            project_id: The ID of the project\n",
    "            dataset_id: The ID of the dataset\n",
    "        \"\"\"\n",
    "        self.ragas_api_client = ragas_api_client\n",
    "        self.project_id = project_id\n",
    "        self.dataset_id = dataset_id\n",
    "        self.dataset = None\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"RagasAppBackend(project_id={self.project_id}, dataset_id={self.dataset_id})\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "        \n",
    "    def initialize(self, dataset):\n",
    "        \"\"\"Initialize the backend with the dataset instance.\"\"\"\n",
    "        self.dataset = dataset\n",
    "        \n",
    "    def get_column_mapping(self, model):\n",
    "        \"\"\"Get mapping between model fields and backend columns.\"\"\"\n",
    "        sync_func = async_to_sync(self.ragas_api_client.list_dataset_columns)\n",
    "        columns = sync_func(project_id=self.project_id, dataset_id=self.dataset_id)\n",
    "        column_id_map = {column[\"name\"]: column[\"id\"] for column in columns[\"items\"]}\n",
    "        \n",
    "        # Update the model's column mapping with the values from the API\n",
    "        column_mapping = {}\n",
    "        for field_name in model.__annotations__:\n",
    "            if field_name in column_id_map:\n",
    "                column_mapping[field_name] = column_id_map[field_name]\n",
    "        \n",
    "        return column_mapping\n",
    "    \n",
    "    def load_entries(self, model_class):\n",
    "        \"\"\"Load all entries from the API.\"\"\"\n",
    "        # Get all rows\n",
    "        sync_func = async_to_sync(self.ragas_api_client.list_dataset_rows)\n",
    "        response = sync_func(\n",
    "            project_id=self.project_id,\n",
    "            dataset_id=self.dataset_id\n",
    "        )\n",
    "        \n",
    "        # Get column mapping (ID -> name)\n",
    "        column_map = {v: k for k, v in model_class.__column_mapping__.items()}\n",
    "        \n",
    "        # Process rows\n",
    "        entries = []\n",
    "        for row in response.get(\"items\", []):\n",
    "            model_data = {}\n",
    "            row_id = row.get(\"id\")\n",
    "            \n",
    "            # Convert from API data format to model fields\n",
    "            for col_id, value in row.get(\"data\", {}).items():\n",
    "                if col_id in column_map:\n",
    "                    field_name = column_map[col_id]\n",
    "                    model_data[field_name] = value\n",
    "            \n",
    "            # Create model instance\n",
    "            entry = model_class(**model_data)\n",
    "            \n",
    "            # Store row ID for future operations\n",
    "            entry._row_id = row_id\n",
    "            \n",
    "            entries.append(entry)\n",
    "            \n",
    "        return entries\n",
    "    \n",
    "    def append_entry(self, entry):\n",
    "        \"\"\"Add a new entry to the API and return its ID.\"\"\"\n",
    "        import ragas_experimental.typing as rt\n",
    "        \n",
    "        # Get column mapping\n",
    "        column_id_map = entry.__class__.__column_mapping__\n",
    "        \n",
    "        # Create row data\n",
    "        row_dict_converted = rt.ModelConverter.instance_to_row(entry)\n",
    "        row_id = create_nano_id()\n",
    "        row_data = {}\n",
    "        \n",
    "        for column in row_dict_converted[\"data\"]:\n",
    "            if column[\"column_id\"] in column_id_map:\n",
    "                row_data[column_id_map[column[\"column_id\"]]] = column[\"data\"]\n",
    "        \n",
    "        # Create row in API\n",
    "        sync_func = async_to_sync(self.ragas_api_client.create_dataset_row)\n",
    "        response = sync_func(\n",
    "            project_id=self.project_id,\n",
    "            dataset_id=self.dataset_id,\n",
    "            id=row_id,\n",
    "            data=row_data,\n",
    "        )\n",
    "        \n",
    "        # Return the row ID\n",
    "        return response[\"id\"]\n",
    "    \n",
    "    def update_entry(self, entry):\n",
    "        \"\"\"Update an existing entry in the API.\"\"\"\n",
    "        import ragas_experimental.typing as rt\n",
    "        \n",
    "        # Get the row ID\n",
    "        row_id = None\n",
    "        if hasattr(entry, \"_row_id\") and entry._row_id:\n",
    "            row_id = entry._row_id\n",
    "        else:\n",
    "            raise ValueError(\"Cannot update: entry has no row ID\")\n",
    "        \n",
    "        # Get column mapping and prepare data\n",
    "        column_id_map = entry.__class__.__column_mapping__\n",
    "        row_dict = rt.ModelConverter.instance_to_row(entry)[\"data\"]\n",
    "        row_data = {}\n",
    "        \n",
    "        for column in row_dict:\n",
    "            if column[\"column_id\"] in column_id_map:\n",
    "                row_data[column_id_map[column[\"column_id\"]]] = column[\"data\"]\n",
    "        \n",
    "        # Update in API\n",
    "        sync_func = async_to_sync(self.ragas_api_client.update_dataset_row)\n",
    "        response = sync_func(\n",
    "            project_id=self.project_id,\n",
    "            dataset_id=self.dataset_id,\n",
    "            row_id=row_id,\n",
    "            data=row_data,\n",
    "        )\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def delete_entry(self, entry_id):\n",
    "        \"\"\"Delete an entry from the API.\"\"\"\n",
    "        # Delete the row\n",
    "        sync_func = async_to_sync(self.ragas_api_client.delete_dataset_row)\n",
    "        response = sync_func(\n",
    "            project_id=self.project_id,\n",
    "            dataset_id=self.dataset_id,\n",
    "            row_id=entry_id\n",
    "        )\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def get_entry_by_field(self, field_name, field_value, model_class):\n",
    "        \"\"\"Get an entry by field value.\"\"\"\n",
    "        # We don't have direct filtering in the API, so load all and filter\n",
    "        entries = self.load_entries(model_class)\n",
    "        \n",
    "        # Search for matching entry\n",
    "        for entry in entries:\n",
    "            if hasattr(entry, field_name) and getattr(entry, field_name) == field_value:\n",
    "                return entry\n",
    "                \n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class LocalBackend(DatasetBackend):\n",
    "    def __init__(self, local_root_dir, project_id, dataset_id, dataset_name, type: t.Literal[\"datasets\", \"experiments\"]):\n",
    "        \"\"\"Initialize the LocalBackend.\n",
    "        \n",
    "        Args:\n",
    "            local_root_dir: The root directory for all projects\n",
    "            project_id: The ID of the project\n",
    "            dataset_id: The ID of the dataset\n",
    "            dataset_name: The name of the dataset\n",
    "        \"\"\"\n",
    "        self.local_root_dir = local_root_dir\n",
    "        self.project_id = project_id\n",
    "        self.dataset_id = dataset_id\n",
    "        self.dataset_name = dataset_name\n",
    "        self.dataset = None\n",
    "        self.type = type\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"LocalBackend(local_root_dir={self.local_root_dir}, project_id={self.project_id}, dataset_id={self.dataset_id}, dataset_name={self.dataset_name})\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "        \n",
    "    def initialize(self, dataset):\n",
    "        \"\"\"Initialize the backend with the dataset instance.\"\"\"\n",
    "        self.dataset = dataset\n",
    "        \n",
    "        # Ensure CSV file exists\n",
    "        self._ensure_csv_exists()\n",
    "        \n",
    "    def _ensure_csv_exists(self):\n",
    "        \"\"\"Create the CSV file if it doesn't exist.\"\"\"\n",
    "        csv_path = self._get_csv_path()\n",
    "        \n",
    "        # Create directories if needed\n",
    "        os.makedirs(os.path.dirname(csv_path), exist_ok=True)\n",
    "        \n",
    "        # Create file with headers if it doesn't exist\n",
    "        if not os.path.exists(csv_path):\n",
    "            # Include _row_id in the headers\n",
    "            field_names = [\"_row_id\"] + list(self.dataset.model.__annotations__.keys())\n",
    "            \n",
    "            with open(csv_path, 'w', newline='') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(field_names)\n",
    "    \n",
    "    def _get_csv_path(self):\n",
    "        \"\"\"Get the path to the CSV file.\"\"\"\n",
    "        return os.path.join(\n",
    "            self.local_root_dir, \n",
    "            self.project_id, \n",
    "            self.type,\n",
    "            f\"{self.dataset_name}.csv\"\n",
    "        )\n",
    "        \n",
    "    def get_column_mapping(self, model) -> t.Dict:\n",
    "        \"\"\"Get mapping between model fields and CSV columns.\n",
    "        \n",
    "        For CSV, column names directly match field names.\n",
    "        \"\"\"\n",
    "        # Simple dictionary comprehension\n",
    "        return model.model_fields\n",
    "    \n",
    "    def load_entries(self, model_class):\n",
    "        \"\"\"Load all entries from the CSV file.\"\"\"\n",
    "        csv_path = self._get_csv_path()\n",
    "        \n",
    "        if not os.path.exists(csv_path):\n",
    "            return []\n",
    "            \n",
    "        entries = []\n",
    "        \n",
    "        with open(csv_path, 'r', newline='') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            \n",
    "            for row in reader:\n",
    "                try:\n",
    "                    # Extract row_id and remove from model data\n",
    "                    row_id = row.get(\"_row_id\", str(uuid.uuid4()))\n",
    "                    \n",
    "                    # Create a copy without _row_id for model instantiation\n",
    "                    model_data = {k: v for k, v in row.items() if k != \"_row_id\"}\n",
    "                    \n",
    "                    # Convert types as needed\n",
    "                    typed_row = {}\n",
    "                    for field, value in model_data.items():\n",
    "                        if field in model_class.model_fields:\n",
    "                            field_type = model_class.model_fields[field].annotation\n",
    "                            \n",
    "                            # Handle basic type conversions\n",
    "                            if field_type == int:\n",
    "                                typed_row[field] = int(value) if value else 0\n",
    "                            elif field_type == float:\n",
    "                                typed_row[field] = float(value) if value else 0.0\n",
    "                            elif field_type == bool:\n",
    "                                typed_row[field] = value.lower() in ('true', 't', 'yes', 'y', '1')\n",
    "                            else:\n",
    "                                typed_row[field] = value\n",
    "                    \n",
    "                    # Create model instance\n",
    "                    entry = model_class(**typed_row)\n",
    "                    \n",
    "                    # Set the row ID from CSV (or use UUID if not available)\n",
    "                    entry._row_id = row_id\n",
    "                    \n",
    "                    entries.append(entry)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading row from CSV: {e}\")\n",
    "        \n",
    "        return entries\n",
    "    \n",
    "    def append_entry(self, entry):\n",
    "        \"\"\"Add a new entry to the CSV file and return a generated ID.\"\"\"\n",
    "        csv_path = self._get_csv_path()\n",
    "        \n",
    "        # Read existing rows to avoid overwriting\n",
    "        existing_rows = []\n",
    "        if os.path.exists(csv_path) and os.path.getsize(csv_path) > 0:\n",
    "            with open(csv_path, 'r', newline='') as f:\n",
    "                reader = csv.DictReader(f)\n",
    "                existing_rows = list(reader)\n",
    "        \n",
    "        # Generate a row ID if needed\n",
    "        row_id = getattr(entry, \"_row_id\", None) or str(uuid.uuid4())\n",
    "        \n",
    "        # Get field names including row_id\n",
    "        field_names = [\"_row_id\"] + list(entry.model_fields.keys())\n",
    "        \n",
    "        # Convert entry to dict\n",
    "        entry_dict = entry.model_dump()\n",
    "        \n",
    "        # Add row_id to the dict\n",
    "        entry_dict[\"_row_id\"] = row_id\n",
    "        \n",
    "        # Write all rows back with the new entry\n",
    "        with open(csv_path, 'w', newline='') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=field_names)\n",
    "            writer.writeheader()\n",
    "            \n",
    "            # Write existing rows\n",
    "            for row in existing_rows:\n",
    "                writer.writerow(row)\n",
    "                \n",
    "            # Write new row\n",
    "            writer.writerow(entry_dict)\n",
    "        \n",
    "        # Return the row ID\n",
    "        return row_id\n",
    "    \n",
    "    def update_entry(self, entry):\n",
    "        \"\"\"Update an existing entry in the CSV file.\n",
    "        \n",
    "        Since CSV files don't support in-place updates, we need to\n",
    "        rewrite the entire file.\n",
    "        \"\"\"\n",
    "        # Create a copy of entries to modify\n",
    "        entries_to_save = list(self.dataset._entries)  # Make a copy\n",
    "        \n",
    "        # Find the entry to update\n",
    "        updated = False\n",
    "        for i, e in enumerate(entries_to_save):\n",
    "            if hasattr(e, \"_row_id\") and hasattr(entry, \"_row_id\") and e._row_id == entry._row_id:\n",
    "                # Update the entry in our copy\n",
    "                entries_to_save[i] = entry\n",
    "                updated = True\n",
    "                break\n",
    "        \n",
    "        # If entry wasn't found, just append it\n",
    "        if not updated and entries_to_save:\n",
    "            entries_to_save.append(entry)\n",
    "        \n",
    "        # Write all entries back to CSV\n",
    "        self._write_entries_to_csv(entries_to_save)\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def delete_entry(self, entry_id):\n",
    "        \"\"\"Delete an entry from the CSV file.\n",
    "        \n",
    "        This method should NOT modify self.dataset._entries directly.\n",
    "        Dataset.pop() handles that separately.\n",
    "        \"\"\"\n",
    "        # Create a copy of entries to modify, excluding the one to delete\n",
    "        entries_to_save = []\n",
    "        for e in self.dataset._entries:\n",
    "            if not (hasattr(e, \"_row_id\") and e._row_id == entry_id):\n",
    "                entries_to_save.append(e)\n",
    "        \n",
    "        # Write all entries back to CSV\n",
    "        self._write_entries_to_csv(entries_to_save)\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def _write_entries_to_csv(self, entries):\n",
    "        \"\"\"Write all entries to the CSV file.\"\"\"\n",
    "        csv_path = self._get_csv_path()\n",
    "        \n",
    "        if not entries:\n",
    "            # If no entries, just create an empty CSV with headers\n",
    "            field_names = [\"_row_id\"] + list(self.dataset.model.model_fields.keys())\n",
    "            with open(csv_path, 'w', newline='') as f:\n",
    "                writer = csv.DictWriter(f, fieldnames=field_names)\n",
    "                writer.writeheader()\n",
    "            return\n",
    "            \n",
    "        # Get field names including _row_id\n",
    "        field_names = [\"_row_id\"] + list(entries[0].__class__.model_fields.keys())\n",
    "        \n",
    "        # Write all entries\n",
    "        with open(csv_path, 'w', newline='') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=field_names)\n",
    "            writer.writeheader()\n",
    "            \n",
    "            for entry in entries:\n",
    "                # Create a dict with model data + row_id\n",
    "                entry_dict = entry.model_dump()\n",
    "                entry_dict[\"_row_id\"] = getattr(entry, \"_row_id\", str(uuid.uuid4()))\n",
    "                \n",
    "                writer.writerow(entry_dict)\n",
    "    \n",
    "    def get_entry_by_field(self, field_name, field_value, model_class):\n",
    "        \"\"\"Get an entry by field value.\"\"\"\n",
    "        entries = self.load_entries(model_class)\n",
    "        \n",
    "        for entry in entries:\n",
    "            if hasattr(entry, field_name) and getattr(entry, field_name) == field_value:\n",
    "                return entry\n",
    "                \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def create_dataset_backend(backend_type: SUPPORTED_BACKENDS, **kwargs):\n",
    "    \"\"\"Factory function to create the appropriate backend.\n",
    "    \n",
    "    Args:\n",
    "        backend_type: The type of backend to create (ragas_app or local)\n",
    "        **kwargs: Arguments specific to the backend\n",
    "        \n",
    "    Returns:\n",
    "        DatasetBackend: An instance of the requested backend\n",
    "    \"\"\"\n",
    "    backend_classes = {\n",
    "        \"ragas_app\": RagasAppBackend,\n",
    "        \"local\": LocalBackend,\n",
    "    }\n",
    "    \n",
    "    if backend_type not in backend_classes:\n",
    "        raise ValueError(f\"Unsupported backend: {backend_type}\")\n",
    "        \n",
    "    return backend_classes[backend_type](**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LocalBackend(local_root_dir=/var/folders/2y/02fp70k56p75ldrkgtx7z10r0000gn/T/ragas_test_dPmBLc9qLgdj, project_id=test_project, dataset_id=test_dataset, dataset_name=test_dataset)\n"
     ]
    }
   ],
   "source": [
    "temp_dir = get_test_directory()\n",
    "backend = create_dataset_backend(\"local\", local_root_dir=temp_dir, project_id=\"test_project\", dataset_id=\"test_dataset\", dataset_name=\"test_dataset\", type=\"dataset\")\n",
    "print(backend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class Dataset(t.Generic[BaseModelType]):\n",
    "    \"\"\"A list-like interface for managing dataset entries with backend synchronization.\n",
    "    \n",
    "    This class behaves like a Python list while synchronizing operations with the\n",
    "    chosen backend (Ragas API or local filesystem).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        model: t.Type[BaseModel],\n",
    "        project_id: str,\n",
    "        dataset_id: str,\n",
    "        datatable_type: t.Literal[\"datasets\", \"experiments\"],\n",
    "        ragas_api_client: t.Optional[RagasApiClient] = None,\n",
    "        backend: SUPPORTED_BACKENDS = \"local\",\n",
    "        local_root_dir: t.Optional[str] = None,\n",
    "    ):\n",
    "        \"\"\"Initialize a Dataset with the specified backend.\n",
    "        \n",
    "        Args:\n",
    "            name: The name of the dataset\n",
    "            model: The Pydantic model class for entries\n",
    "            project_id: The ID of the parent project\n",
    "            dataset_id: The ID of this dataset\n",
    "            ragas_api_client: Required for ragas_app backend\n",
    "            backend: The storage backend to use (ragas_app or local)\n",
    "            local_root_dir: Required for local backend\n",
    "        \"\"\"\n",
    "        # Store basic properties\n",
    "        self.name = name\n",
    "        self.model = model\n",
    "        self.project_id = project_id\n",
    "        self.dataset_id = dataset_id\n",
    "        self.backend_type = backend\n",
    "        self.datatable_type = datatable_type\n",
    "        self._entries: t.List[BaseModelType] = []\n",
    "\n",
    "        # Create the appropriate backend\n",
    "        backend_params = {}\n",
    "        if backend == \"ragas_app\":\n",
    "            if ragas_api_client is None:\n",
    "                raise ValueError(\"ragas_api_client is required for ragas_app backend\")\n",
    "            backend_params = {\n",
    "                \"ragas_api_client\": ragas_api_client,\n",
    "                \"project_id\": project_id,\n",
    "                \"dataset_id\": dataset_id\n",
    "            }\n",
    "        elif backend == \"local\":\n",
    "            if local_root_dir is None:\n",
    "                raise ValueError(\"local_root_dir is required for local backend\")\n",
    "            backend_params = {\n",
    "                \"local_root_dir\": local_root_dir,\n",
    "                \"project_id\": project_id,\n",
    "                \"dataset_id\": dataset_id,\n",
    "                \"dataset_name\": name,\n",
    "                \"type\": self.datatable_type\n",
    "            }\n",
    "            \n",
    "        self._backend = create_dataset_backend(backend, **backend_params)\n",
    "        \n",
    "        # Initialize the backend with this dataset\n",
    "        self._backend.initialize(self)\n",
    "        \n",
    "        # Initialize column mapping if it doesn't exist yet\n",
    "        if not hasattr(self.model, \"__column_mapping__\"):\n",
    "            self.model.__column_mapping__ = {}\n",
    "            \n",
    "        # Get column mappings from backend and update the model's mapping\n",
    "        column_mapping = self._backend.get_column_mapping(model)\n",
    "        \n",
    "        # Update the model's column mapping\n",
    "        for field_name, column_id in column_mapping.items():\n",
    "            self.model.__column_mapping__[field_name] = column_id\n",
    "\n",
    "    def __getitem__(\n",
    "        self, key: t.Union[int, slice]\n",
    "    ) -> t.Union[BaseModelType, \"Dataset[BaseModelType]\"]:\n",
    "        \"\"\"Get an entry by index or slice.\"\"\"\n",
    "        if isinstance(key, slice):\n",
    "            # Create a new dataset with the sliced entries\n",
    "            new_dataset = type(self)(\n",
    "                name=self.name,\n",
    "                model=self.model,\n",
    "                project_id=self.project_id,\n",
    "                dataset_id=self.dataset_id,\n",
    "                backend=self.backend_type,\n",
    "                datatable_type=self.datatable_type\n",
    "            )\n",
    "            # Copy the backend reference\n",
    "            new_dataset._backend = self._backend\n",
    "            # Set the entries to the sliced entries\n",
    "            new_dataset._entries = self._entries[key]\n",
    "            return new_dataset\n",
    "        else:\n",
    "            return self._entries[key]\n",
    "\n",
    "    def __setitem__(self, index: int, entry: BaseModelType) -> None:\n",
    "        \"\"\"Update an entry at the given index and sync to backend.\"\"\"\n",
    "        if not isinstance(entry, self.model):\n",
    "            raise TypeError(f\"Entry must be an instance of {self.model.__name__}\")\n",
    "\n",
    "        # Get existing entry to get its ID\n",
    "        existing = self._entries[index]\n",
    "        if hasattr(existing, \"_row_id\") and existing._row_id:\n",
    "            entry._row_id = existing._row_id\n",
    "        \n",
    "        # Update in backend\n",
    "        self._backend.update_entry(entry)\n",
    "        \n",
    "        # Update local cache\n",
    "        self._entries[index] = entry\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        \"\"\"String representation of the dataset.\"\"\"\n",
    "        return f\"Dataset(name='{self.name}', model={self.model.__name__}, len={len(self)})\"\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Get the number of entries in the dataset.\"\"\"\n",
    "        return len(self._entries)\n",
    "\n",
    "    def __iter__(self) -> t.Iterator[BaseModelType]:\n",
    "        \"\"\"Iterate over the entries in the dataset.\"\"\"\n",
    "        return iter(self._entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model\n",
    "class DatasetModel(BaseModel):\n",
    "    id: int\n",
    "    name: str\n",
    "    description: str\n",
    "\n",
    "class ExperimentModel(DatasetModel):\n",
    "    tags: t.Literal[\"tag1\", \"tag2\", \"tag3\"]\n",
    "    result: MetricResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetModel(id=0, name='test', description='test description')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_instance = DatasetModel(\n",
    "    id=0, \n",
    "    name=\"test\", \n",
    "    description=\"test description\", \n",
    ")\n",
    "dataset_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExperimentModel(id=0, name='test', description='test description', tags='tag1', result=0.5)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_instance = ExperimentModel(\n",
    "    **dataset_instance.model_dump(),\n",
    "    tags=\"tag1\",\n",
    "    result=MetricResult(result=0.5, reason=\"test reason\"),\n",
    ")\n",
    "experiment_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas_experimental import Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_root_dir = get_test_directory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Project.create(name=\"test_project\", backend=\"local\", root_dir=tmp_root_dir)\n",
    "dataset_with_dataset_model = p.create_dataset(name=\"dataset_with_dataset_model\", model=DatasetModel)\n",
    "dataset_with_experiment_model = p.create_dataset(name=\"dataset_with_experiment_model\", model=ExperimentModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExperimentModel(id=0, name='test', description='test description', tags='tag1', result=0.5)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_instance = ExperimentModel(\n",
    "    **dataset_instance.model_dump(),\n",
    "    tags=\"tag1\",\n",
    "    result=MetricResult(result=0.5, reason=\"test reason\"),\n",
    ")\n",
    "experiment_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "@patch\n",
    "def append(self: Dataset, entry: BaseModelType) -> None:\n",
    "    \"\"\"Add a new entry to the dataset and sync to backend.\n",
    "    \n",
    "    Args:\n",
    "        entry: The entry to add to the dataset\n",
    "    \"\"\"\n",
    "    if not isinstance(entry, self.model):\n",
    "        raise TypeError(f\"Entry must be an instance of {self.model.__name__}\")\n",
    "        \n",
    "    # Add to backend and get ID\n",
    "    row_id = self._backend.append_entry(entry)\n",
    "    \n",
    "    # Store the ID\n",
    "    entry._row_id = row_id\n",
    "    \n",
    "    # Add to local cache\n",
    "    self._entries.append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_with_dataset_model.append(dataset_instance)\n",
    "dataset_with_experiment_model.append(experiment_instance)\n",
    "len(dataset_with_dataset_model), len(dataset_with_experiment_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "test_eq(len(dataset_with_dataset_model), 1)\n",
    "test_eq(len(dataset_with_experiment_model), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "@patch\n",
    "def pop(self: Dataset, index: int = -1) -> BaseModelType:\n",
    "    \"\"\"Remove and return entry at index, sync deletion to backend.\n",
    "    \n",
    "    Args:\n",
    "        index: The index of the entry to remove (default: -1, the last entry)\n",
    "        \n",
    "    Returns:\n",
    "        The removed entry\n",
    "    \"\"\"\n",
    "    # Get the entry\n",
    "    entry = self._entries[index]\n",
    "    \n",
    "    # Get the row ID\n",
    "    row_id = getattr(entry, \"_row_id\", None)\n",
    "    if row_id is None:\n",
    "        raise ValueError(\"Entry has no row ID. This likely means it was not added or synced to the dataset.\")\n",
    "    \n",
    "    # Delete from backend\n",
    "    self._backend.delete_entry(row_id)\n",
    "    \n",
    "    # Remove from local cache\n",
    "    return self._entries.pop(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_with_dataset_model.pop()\n",
    "dataset_with_experiment_model.pop()\n",
    "len(dataset_with_dataset_model), len(dataset_with_experiment_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "test_eq(len(dataset_with_dataset_model), 0)\n",
    "test_eq(len(dataset_with_experiment_model), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now add some more entries\n",
    "for i in range(10):\n",
    "    dataset_with_dataset_model.append(dataset_instance)\n",
    "    dataset_with_experiment_model.append(experiment_instance)\n",
    "\n",
    "test_eq(len(dataset_with_dataset_model), 10)\n",
    "test_eq(len(dataset_with_experiment_model), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "@patch\n",
    "def load(self: Dataset) -> None:\n",
    "    \"\"\"Load all entries from the backend.\"\"\"\n",
    "    # Get entries from backend\n",
    "    self._entries = self._backend.load_entries(self.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_with_dataset_model.load()\n",
    "dataset_with_experiment_model.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "@patch\n",
    "def load_as_dicts(self: Dataset) -> t.List[t.Dict]:\n",
    "    \"\"\"Load all entries as dictionaries.\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries representing the entries\n",
    "    \"\"\"\n",
    "    # Make sure we have entries\n",
    "    if not self._entries:\n",
    "        self.load()\n",
    "    \n",
    "    # Convert to dictionaries\n",
    "    return [entry.model_dump() for entry in self._entries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 0, 'name': 'test', 'description': 'test description'},\n",
       " {'id': 0, 'name': 'test', 'description': 'test description'},\n",
       " {'id': 0, 'name': 'test', 'description': 'test description'},\n",
       " {'id': 0, 'name': 'test', 'description': 'test description'},\n",
       " {'id': 0, 'name': 'test', 'description': 'test description'},\n",
       " {'id': 0, 'name': 'test', 'description': 'test description'},\n",
       " {'id': 0, 'name': 'test', 'description': 'test description'},\n",
       " {'id': 0, 'name': 'test', 'description': 'test description'},\n",
       " {'id': 0, 'name': 'test', 'description': 'test description'},\n",
       " {'id': 0, 'name': 'test', 'description': 'test description'}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_with_dataset_model.load_as_dicts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 0,\n",
       "  'name': 'test',\n",
       "  'description': 'test description',\n",
       "  'tags': 'tag1',\n",
       "  'result': '0.5'},\n",
       " {'id': 0,\n",
       "  'name': 'test',\n",
       "  'description': 'test description',\n",
       "  'tags': 'tag1',\n",
       "  'result': '0.5'},\n",
       " {'id': 0,\n",
       "  'name': 'test',\n",
       "  'description': 'test description',\n",
       "  'tags': 'tag1',\n",
       "  'result': '0.5'},\n",
       " {'id': 0,\n",
       "  'name': 'test',\n",
       "  'description': 'test description',\n",
       "  'tags': 'tag1',\n",
       "  'result': '0.5'},\n",
       " {'id': 0,\n",
       "  'name': 'test',\n",
       "  'description': 'test description',\n",
       "  'tags': 'tag1',\n",
       "  'result': '0.5'},\n",
       " {'id': 0,\n",
       "  'name': 'test',\n",
       "  'description': 'test description',\n",
       "  'tags': 'tag1',\n",
       "  'result': '0.5'},\n",
       " {'id': 0,\n",
       "  'name': 'test',\n",
       "  'description': 'test description',\n",
       "  'tags': 'tag1',\n",
       "  'result': '0.5'},\n",
       " {'id': 0,\n",
       "  'name': 'test',\n",
       "  'description': 'test description',\n",
       "  'tags': 'tag1',\n",
       "  'result': '0.5'},\n",
       " {'id': 0,\n",
       "  'name': 'test',\n",
       "  'description': 'test description',\n",
       "  'tags': 'tag1',\n",
       "  'result': '0.5'},\n",
       " {'id': 0,\n",
       "  'name': 'test',\n",
       "  'description': 'test description',\n",
       "  'tags': 'tag1',\n",
       "  'result': '0.5'}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_with_experiment_model.load_as_dicts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "@patch\n",
    "def to_pandas(self: Dataset) -> \"pd.DataFrame\":\n",
    "    \"\"\"Convert dataset to pandas DataFrame.\"\"\"\n",
    "\n",
    "    # Make sure we have data\n",
    "    if not self._entries:\n",
    "        self.load()\n",
    "    \n",
    "    # Convert entries to dictionaries\n",
    "    data = [entry.model_dump() for entry in self._entries]\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>tags</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>test description</td>\n",
       "      <td>tag1</td>\n",
       "      <td>(0, ., 5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>test description</td>\n",
       "      <td>tag1</td>\n",
       "      <td>(0, ., 5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>test description</td>\n",
       "      <td>tag1</td>\n",
       "      <td>(0, ., 5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>test description</td>\n",
       "      <td>tag1</td>\n",
       "      <td>(0, ., 5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>test description</td>\n",
       "      <td>tag1</td>\n",
       "      <td>(0, ., 5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>test description</td>\n",
       "      <td>tag1</td>\n",
       "      <td>(0, ., 5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>test description</td>\n",
       "      <td>tag1</td>\n",
       "      <td>(0, ., 5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>test description</td>\n",
       "      <td>tag1</td>\n",
       "      <td>(0, ., 5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>test description</td>\n",
       "      <td>tag1</td>\n",
       "      <td>(0, ., 5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>test description</td>\n",
       "      <td>tag1</td>\n",
       "      <td>(0, ., 5)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  name       description  tags     result\n",
       "0   0  test  test description  tag1  (0, ., 5)\n",
       "1   0  test  test description  tag1  (0, ., 5)\n",
       "2   0  test  test description  tag1  (0, ., 5)\n",
       "3   0  test  test description  tag1  (0, ., 5)\n",
       "4   0  test  test description  tag1  (0, ., 5)\n",
       "5   0  test  test description  tag1  (0, ., 5)\n",
       "6   0  test  test description  tag1  (0, ., 5)\n",
       "7   0  test  test description  tag1  (0, ., 5)\n",
       "8   0  test  test description  tag1  (0, ., 5)\n",
       "9   0  test  test description  tag1  (0, ., 5)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_with_experiment_model.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "@patch\n",
    "def save(self: Dataset, item: BaseModelType) -> None:\n",
    "    \"\"\"Save changes to an item to the backend.\n",
    "    \n",
    "    Args:\n",
    "        item: The item to save\n",
    "    \"\"\"\n",
    "    if not isinstance(item, self.model):\n",
    "        raise TypeError(f\"Item must be an instance of {self.model.__name__}\")\n",
    "    \n",
    "    # Check if the item has a row ID\n",
    "    if not hasattr(item, \"_row_id\") or not item._row_id:\n",
    "        # Try to find it in our entries by matching\n",
    "        for i, entry in enumerate(self._entries):\n",
    "            if id(entry) == id(item):  # Check if it's the same object\n",
    "                if hasattr(entry, \"_row_id\") and entry._row_id:\n",
    "                    item._row_id = entry._row_id\n",
    "                    break\n",
    "    \n",
    "    if not hasattr(item, \"_row_id\") or not item._row_id:\n",
    "        raise ValueError(\"Cannot save: item is not from this dataset or was not properly synced\")\n",
    "    \n",
    "    # Update in backend\n",
    "    self._backend.update_entry(item)\n",
    "    \n",
    "    # Update in local cache if needed\n",
    "    self._update_local_entry(item)\n",
    "    \n",
    "@patch\n",
    "def _update_local_entry(self: Dataset, item: BaseModelType) -> None:\n",
    "    \"\"\"Update an entry in the local cache.\n",
    "    \n",
    "    Args:\n",
    "        item: The item to update\n",
    "    \"\"\"\n",
    "    for i, entry in enumerate(self._entries):\n",
    "        if hasattr(entry, \"_row_id\") and hasattr(item, \"_row_id\") and entry._row_id == item._row_id:\n",
    "            # If it's not the same object, update our copy\n",
    "            if id(entry) != id(item):\n",
    "                self._entries[i] = item\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExperimentModel(id=0, name='test', description='test description', tags='tag1', result='0.5')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = dataset_with_experiment_model[0]\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'updated name'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.name = \"updated name\"\n",
    "dataset_with_experiment_model.save(d)\n",
    "dataset_with_experiment_model[0].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 0,\n",
       "  'name': 'updated name',\n",
       "  'description': 'test description',\n",
       "  'tags': 'tag1',\n",
       "  'result': '0.5'},\n",
       " {'id': 0,\n",
       "  'name': 'test',\n",
       "  'description': 'test description',\n",
       "  'tags': 'tag1',\n",
       "  'result': '0.5'},\n",
       " {'id': 0,\n",
       "  'name': 'test',\n",
       "  'description': 'test description',\n",
       "  'tags': 'tag1',\n",
       "  'result': '0.5'},\n",
       " {'id': 0,\n",
       "  'name': 'test',\n",
       "  'description': 'test description',\n",
       "  'tags': 'tag1',\n",
       "  'result': '0.5'},\n",
       " {'id': 0,\n",
       "  'name': 'test',\n",
       "  'description': 'test description',\n",
       "  'tags': 'tag1',\n",
       "  'result': '0.5'},\n",
       " {'id': 0,\n",
       "  'name': 'test',\n",
       "  'description': 'test description',\n",
       "  'tags': 'tag1',\n",
       "  'result': '0.5'},\n",
       " {'id': 0,\n",
       "  'name': 'test',\n",
       "  'description': 'test description',\n",
       "  'tags': 'tag1',\n",
       "  'result': '0.5'},\n",
       " {'id': 0,\n",
       "  'name': 'test',\n",
       "  'description': 'test description',\n",
       "  'tags': 'tag1',\n",
       "  'result': '0.5'},\n",
       " {'id': 0,\n",
       "  'name': 'test',\n",
       "  'description': 'test description',\n",
       "  'tags': 'tag1',\n",
       "  'result': '0.5'},\n",
       " {'id': 0,\n",
       "  'name': 'test',\n",
       "  'description': 'test description',\n",
       "  'tags': 'tag1',\n",
       "  'result': '0.5'}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_with_experiment_model.load_as_dicts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "@patch\n",
    "def get(self: Dataset, field_value: t.Any, field_name: str = \"_row_id\") -> t.Optional[BaseModelType]:\n",
    "    \"\"\"Get an entry by field value.\n",
    "    \n",
    "    Args:\n",
    "        field_value: The value to match\n",
    "        field_name: The field to match against (default: \"_row_id\")\n",
    "        \n",
    "    Returns:\n",
    "        The matching model instance or None if not found\n",
    "    \"\"\"\n",
    "    # Check if we need to load entries\n",
    "    if not self._entries:\n",
    "        self.load()\n",
    "    \n",
    "    # Search in local entries first\n",
    "    for entry in self._entries:\n",
    "        if hasattr(entry, field_name) and getattr(entry, field_name) == field_value:\n",
    "            return entry\n",
    "    \n",
    "    # If not found, try to get from backend\n",
    "    if field_name == \"_row_id\":\n",
    "        # Special case for row IDs\n",
    "        for entry in self._entries:\n",
    "            if hasattr(entry, \"_row_id\") and entry._row_id == field_value:\n",
    "                return entry\n",
    "    else:\n",
    "        # Use backend to search\n",
    "        return self._backend.get_entry_by_field(field_name, field_value, self.model)\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'36eeed65-2105-4900-b9bb-bb42ddc35820'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d._row_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExperimentModel(id=0, name='updated name', description='test description', tags='tag1', result='0.5')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_instance = dataset_with_experiment_model.get(d._row_id)\n",
    "dataset_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "@patch\n",
    "def to_pandas(self: Dataset) -> \"pd.DataFrame\":\n",
    "    \"\"\"Convert dataset to pandas DataFrame.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing all entries\n",
    "    \"\"\"\n",
    "    # Make sure we have data\n",
    "    if not self._entries:\n",
    "        self.load()\n",
    "    \n",
    "    # Convert entries to dictionaries\n",
    "    data = [entry.model_dump() for entry in self._entries]\n",
    "    return pd.DataFrame(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
