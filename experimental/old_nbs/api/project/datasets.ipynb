{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b89e33f6",
   "metadata": {},
   "source": [
    "# Dataset Management\n",
    "\n",
    "> Methods to create and manage datasets within projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ea028c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp project.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6842baad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83f0eee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jjmachan/workspace/eglabs/ragas/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# | export\n",
    "import typing as t\n",
    "import os\n",
    "import asyncio\n",
    "import tempfile\n",
    "\n",
    "from fastcore.utils import patch\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from ragas_experimental.project.core import Project\n",
    "from ragas_experimental.typing import SUPPORTED_BACKENDS\n",
    "from ragas_experimental.backends.factory import RagasApiClientFactory\n",
    "from ragas_experimental.backends.ragas_api_client import RagasApiClient\n",
    "import ragas_experimental.typing as rt\n",
    "from ragas_experimental.utils import async_to_sync, create_nano_id\n",
    "from ragas_experimental.dataset import Dataset\n",
    "from ragas_experimental.utils import get_test_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "381d6909",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "async def create_dataset_columns(project_id, dataset_id, columns, create_dataset_column_func):\n",
    "    tasks = []\n",
    "    for column in columns:\n",
    "        tasks.append(create_dataset_column_func(\n",
    "            project_id=project_id,\n",
    "            dataset_id=dataset_id,\n",
    "            id=create_nano_id(),\n",
    "            name=column[\"name\"],\n",
    "            type=column[\"type\"],\n",
    "            settings=column[\"settings\"],\n",
    "        ))\n",
    "    return await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0a1a475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def get_dataset_from_ragas_app(\n",
    "    self: Project, \n",
    "    name: str, \n",
    "    model: t.Type[BaseModel]\n",
    ") -> Dataset:\n",
    "    \"\"\"Create a dataset in the Ragas App backend.\"\"\"\n",
    "    # create the dataset\n",
    "    sync_version = async_to_sync(self._ragas_api_client.create_dataset)\n",
    "    dataset_info = sync_version(\n",
    "        project_id=self.project_id,\n",
    "        name=name if name is not None else model.__name__,\n",
    "    )\n",
    "\n",
    "    # create the columns for the dataset\n",
    "    column_types = rt.ModelConverter.model_to_columns(model)\n",
    "    sync_version = async_to_sync(create_dataset_columns)\n",
    "    sync_version(\n",
    "        project_id=self.project_id,\n",
    "        dataset_id=dataset_info[\"id\"],\n",
    "        columns=column_types,\n",
    "        create_dataset_column_func=self._ragas_api_client.create_dataset_column,\n",
    "    )\n",
    "        \n",
    "    # Return a new Dataset instance\n",
    "    return Dataset(\n",
    "        name=name if name is not None else model.__name__,\n",
    "        model=model,\n",
    "        datatable_type=\"datasets\",\n",
    "        project_id=self.project_id,\n",
    "        dataset_id=dataset_info[\"id\"],\n",
    "        ragas_api_client=self._ragas_api_client,\n",
    "        backend=\"ragas_app\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aed39788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def get_dataset_from_local(\n",
    "    self: Project,\n",
    "    name: str,\n",
    "    model: t.Type[BaseModel]\n",
    ") -> Dataset:\n",
    "    \"\"\"Create a dataset in the local filesystem backend.\n",
    "    \n",
    "    Args:\n",
    "        name: Name of the dataset\n",
    "        model: Pydantic model defining the structure\n",
    "        \n",
    "    Returns:\n",
    "        Dataset: A new dataset configured to use the local backend\n",
    "    \"\"\"\n",
    "    # Use a UUID as the dataset ID\n",
    "    dataset_id = create_nano_id()\n",
    "    \n",
    "    # Return a new Dataset instance with local backend\n",
    "    return Dataset(\n",
    "        name=name if name is not None else model.__name__,\n",
    "        model=model,\n",
    "        datatable_type=\"datasets\",\n",
    "        project_id=self.project_id,\n",
    "        dataset_id=dataset_id,\n",
    "        backend=\"local\",\n",
    "        local_root_dir=os.path.dirname(self._root_dir)  # Root dir for all projects\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae500be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "@patch\n",
    "def create_dataset(\n",
    "    self: Project, \n",
    "    model: t.Type[BaseModel], \n",
    "    name: t.Optional[str] = None,\n",
    "    backend: t.Optional[SUPPORTED_BACKENDS] = None\n",
    ") -> Dataset:\n",
    "    \"\"\"Create a new dataset.\n",
    "\n",
    "    Args:\n",
    "        model: Model class defining the dataset structure\n",
    "        name: Name of the dataset (defaults to model name if not provided)\n",
    "        backend: The backend to use (defaults to project's backend if not specified)\n",
    "\n",
    "    Returns:\n",
    "        Dataset: A new dataset object for managing entries\n",
    "    \"\"\"\n",
    "    # If name is not provided, use the model name\n",
    "    if name is None:\n",
    "        name = model.__name__\n",
    "        \n",
    "    # If backend is not specified, use the project's backend\n",
    "    if backend is None:\n",
    "        backend = self.backend\n",
    "\n",
    "    # Create dataset using the appropriate backend\n",
    "    if backend == \"local\":\n",
    "        return get_dataset_from_local(self, name, model)\n",
    "    elif backend == \"ragas_app\":\n",
    "        return get_dataset_from_ragas_app(self, name, model)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported backend: {backend}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c166d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file exists: True\n",
      "CSV content:\n",
      "_row_id,id,name\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    "from pydantic import BaseModel\n",
    "from ragas_experimental import Project\n",
    "\n",
    "# Create a test directory\n",
    "with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    # Create a project\n",
    "    project = Project.create(\n",
    "        name=\"test_project\",\n",
    "        description=\"Test project\",\n",
    "        backend=\"local\",\n",
    "        root_dir=temp_dir\n",
    "    )\n",
    "    \n",
    "    # Define a test model\n",
    "    class TestModel(BaseModel):\n",
    "        id: int\n",
    "        name: str\n",
    "    \n",
    "    # Create a dataset\n",
    "    dataset = project.create_dataset(\n",
    "        model=TestModel,\n",
    "        name=\"test_dataset\"\n",
    "    )\n",
    "    \n",
    "    # Check if CSV file exists\n",
    "    csv_path = os.path.join(temp_dir, \"test_project\", \"datasets\", \"test_dataset.csv\")\n",
    "    print(f\"CSV file exists: {os.path.exists(csv_path)}\")\n",
    "    \n",
    "    # Read CSV content\n",
    "    if os.path.exists(csv_path):\n",
    "        with open(csv_path, 'r') as f:\n",
    "            print(\"CSV content:\")\n",
    "            print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0776c897",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas_experimental.metric import MetricResult\n",
    "from ragas_experimental import Project\n",
    "from ragas_experimental.utils import get_test_directory\n",
    "\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d851ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_root_dir = get_test_directory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34a5adfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model\n",
    "class DatasetModel(BaseModel):\n",
    "    id: int\n",
    "    name: str\n",
    "    description: str\n",
    "\n",
    "class ExperimentModel(DatasetModel):\n",
    "    tags: t.Literal[\"tag1\", \"tag2\", \"tag3\"]\n",
    "    result: MetricResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a50e3d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Project.create(name=\"test_project\", backend=\"local\", root_dir=tmp_root_dir)\n",
    "dataset_with_dataset_model = p.create_dataset(name=\"dataset_with_dataset_model\", model=DatasetModel)\n",
    "dataset_with_experiment_model = p.create_dataset(name=\"dataset_with_experiment_model\", model=ExperimentModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16972bbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dataset_with_dataset_model.csv', 'dataset_with_experiment_model.csv']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(os.path.join(tmp_root_dir, \"test_project\", \"datasets\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03b0be74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LocalBackend(local_root_dir=/var/folders/2y/02fp70k56p75ldrkgtx7z10r0000gn/T/ragas_test_dcqQZIcawxpX, project_id=test_project, dataset_id=nckelCD21MKD, dataset_name=dataset_with_dataset_model)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_with_dataset_model._backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b19996ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(os.path.exists(\n",
    "    os.path.join(tmp_root_dir, \"test_project\", \"datasets\", f'{dataset_with_dataset_model.name}.csv')\n",
    "), True)\n",
    "\n",
    "test_eq(os.path.exists(\n",
    "    os.path.join(tmp_root_dir, \"test_project\", \"datasets\", f'{dataset_with_experiment_model.name}.csv')\n",
    "), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d402bb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "@patch\n",
    "def get_dataset_by_id(\n",
    "    self: Project, \n",
    "    dataset_id: str, \n",
    "    model: t.Type[BaseModel],\n",
    "    backend: t.Optional[SUPPORTED_BACKENDS] = None\n",
    ") -> Dataset:\n",
    "    \"\"\"Get an existing dataset by ID.\n",
    "    \n",
    "    Args:\n",
    "        dataset_id: The ID of the dataset to retrieve\n",
    "        model: The model class to use for the dataset entries\n",
    "        backend: The backend to use (defaults to project's backend)\n",
    "        \n",
    "    Returns:\n",
    "        Dataset: The retrieved dataset\n",
    "    \"\"\"\n",
    "    # If backend is not specified, use the project's backend\n",
    "    if backend is None:\n",
    "        backend = self.backend\n",
    "        \n",
    "    if backend == \"ragas_app\":\n",
    "        # Search for database with given ID\n",
    "        sync_version = async_to_sync(self._ragas_api_client.get_dataset)\n",
    "        dataset_info = sync_version(\n",
    "            project_id=self.project_id,\n",
    "            dataset_id=dataset_id\n",
    "        )\n",
    "\n",
    "        # For now, return Dataset without model type\n",
    "        return Dataset(\n",
    "            name=dataset_info[\"name\"],\n",
    "            model=model,\n",
    "            datatable_type=\"datasets\",\n",
    "            project_id=self.project_id,\n",
    "            dataset_id=dataset_id,\n",
    "            ragas_api_client=self._ragas_api_client,\n",
    "            backend=\"ragas_app\"\n",
    "        )\n",
    "    elif backend == \"local\":\n",
    "        # For local backend, this is not a typical operation since we use names\n",
    "        # We could maintain a mapping of IDs to names, but for now just raise an error\n",
    "        raise NotImplementedError(\n",
    "            \"get_dataset_by_id is not implemented for local backend. \"\n",
    "            \"Use get_dataset with the dataset name instead.\"\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported backend: {backend}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53688362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "@patch\n",
    "def get_dataset(\n",
    "    self: Project, \n",
    "    dataset_name: str, \n",
    "    model: t.Type[BaseModel],\n",
    "    backend: t.Optional[SUPPORTED_BACKENDS] = None\n",
    ") -> Dataset:\n",
    "    \"\"\"Get an existing dataset by name.\n",
    "    \n",
    "    Args:\n",
    "        dataset_name: The name of the dataset to retrieve\n",
    "        model: The model class to use for the dataset entries\n",
    "        backend: The backend to use (defaults to project's backend if not specified)\n",
    "        \n",
    "    Returns:\n",
    "        Dataset: The retrieved dataset\n",
    "    \"\"\"\n",
    "    # If backend is not specified, use the project's backend\n",
    "    if backend is None:\n",
    "        backend = self.backend\n",
    "        \n",
    "    if backend == \"ragas_app\":\n",
    "        # Search for dataset with given name\n",
    "        sync_version = async_to_sync(self._ragas_api_client.get_dataset_by_name)\n",
    "        dataset_info = sync_version(\n",
    "            project_id=self.project_id,\n",
    "            dataset_name=dataset_name\n",
    "        )\n",
    "\n",
    "        # Return Dataset instance\n",
    "        return Dataset(\n",
    "            name=dataset_info[\"name\"],\n",
    "            model=model,\n",
    "            datatable_type=\"datasets\",\n",
    "            project_id=self.project_id,\n",
    "            dataset_id=dataset_info[\"id\"],\n",
    "            ragas_api_client=self._ragas_api_client,\n",
    "            backend=\"ragas_app\"\n",
    "        )\n",
    "    elif backend == \"local\":\n",
    "        # Check if the dataset file exists\n",
    "        dataset_path = self.get_dataset_path(dataset_name)\n",
    "        if not os.path.exists(dataset_path):\n",
    "            raise ValueError(f\"Dataset '{dataset_name}' does not exist\")\n",
    "            \n",
    "        # Create dataset instance with a random ID\n",
    "        dataset_id = create_nano_id()\n",
    "        \n",
    "        # Return Dataset instance\n",
    "        return Dataset(\n",
    "            name=dataset_name,\n",
    "            model=model,\n",
    "            datatable_type=\"datasets\",\n",
    "            project_id=self.project_id,\n",
    "            dataset_id=dataset_id,\n",
    "            backend=\"local\",\n",
    "            local_root_dir=os.path.dirname(self._root_dir)  # Root dir for all projects\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported backend: {backend}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07d859b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "@patch\n",
    "def list_dataset_names(\n",
    "    self: Project,\n",
    "    backend: t.Optional[SUPPORTED_BACKENDS] = None\n",
    ") -> t.List[str]:\n",
    "    \"\"\"List all datasets in the project.\n",
    "    \n",
    "    Args:\n",
    "        backend: The backend to use (defaults to project's backend)\n",
    "        \n",
    "    Returns:\n",
    "        List[str]: Names of all datasets in the project\n",
    "    \"\"\"\n",
    "    # If backend is not specified, use the project's backend\n",
    "    if backend is None:\n",
    "        backend = self.backend\n",
    "        \n",
    "    if backend == \"ragas_app\":\n",
    "        # Get all datasets from API\n",
    "        sync_version = async_to_sync(self._ragas_api_client.list_datasets)\n",
    "        datasets = sync_version(project_id=self.project_id)\n",
    "        return [dataset[\"name\"] for dataset in datasets]\n",
    "    elif backend == \"local\":\n",
    "        # Get all CSV files in the datasets directory\n",
    "        datasets_dir = os.path.join(self._root_dir, \"datasets\")\n",
    "        if not os.path.exists(datasets_dir):\n",
    "            return []\n",
    "            \n",
    "        return [\n",
    "            os.path.splitext(f)[0] for f in os.listdir(datasets_dir)\n",
    "            if f.endswith('.csv')\n",
    "        ]\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported backend: {backend}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93ed3cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a test model for demonstration\n",
    "class TestModel(BaseModel):\n",
    "    id: int\n",
    "    name: str\n",
    "    description: str\n",
    "    tags: t.Literal[\"tag1\", \"tag2\", \"tag3\"]\n",
    "    tags_color_coded: t.Annotated[t.Literal[\"red\", \"green\", \"blue\"], rt.Select(colors=[\"red\", \"green\", \"blue\"])]\n",
    "    url: t.Annotated[str, rt.Url()] = \"https://www.google.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "55f47f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_row_id,id,name,description,score\n",
      "bd7d69f5-8836-4e79-a191-b466dc9c4818,0,Test Item 0,Description for item 0,0.0\n",
      "70d04a30-9697-4f41-a7d0-62e7c733ccdd,1,Test Item 1,Description for item 1,0.5\n",
      "a9ca1712-51cb-4220-a050-6d1df060d2d1,2,Test Item 2,Description for item 2,1.0\n",
      "\n",
      "Retrieved dataset: Dataset(name='test_dataset_comprehensive', model=LocalTestModel, len=3)\n",
      "Updated entry: id=1 name='Updated Name' description='Description for item 1' score=9.9\n",
      "\n",
      "DataFrame (first 2 rows):\n",
      "   id          name             description  score\n",
      "0   0   Test Item 0  Description for item 0    0.0\n",
      "1   1  Updated Name  Description for item 1    9.9\n"
     ]
    }
   ],
   "source": [
    "# Use a persistent test directory \n",
    "test_dir = get_test_directory()\n",
    "\n",
    "# Create a new project with local backend\n",
    "local_project = Project.create(\n",
    "    name=\"test_local_project_comprehensive\",\n",
    "    description=\"A test project using local backend\",\n",
    "    backend=\"local\",\n",
    "    root_dir=test_dir\n",
    ")\n",
    "\n",
    "# Define a test model\n",
    "class LocalTestModel(BaseModel):\n",
    "    id: int\n",
    "    name: str\n",
    "    description: str\n",
    "    score: float\n",
    "\n",
    "# Create a dataset with local backend\n",
    "local_dataset = local_project.create_dataset(\n",
    "    model=LocalTestModel,\n",
    "    name=\"test_dataset_comprehensive\"\n",
    ")\n",
    "\n",
    "# Add some entries\n",
    "for i in range(3):\n",
    "    entry = LocalTestModel(\n",
    "        id=i,\n",
    "        name=f\"Test Item {i}\",\n",
    "        description=f\"Description for item {i}\",\n",
    "        score=i * 0.5\n",
    "    )\n",
    "    local_dataset.append(entry)\n",
    "\n",
    "# Check the dataset\n",
    "#print(f\"Dataset after adding entries: {local_dataset}\")\n",
    "test_eq(len(local_dataset), 3)\n",
    "\n",
    "# Get the dataset path\n",
    "dataset_path = local_project.get_dataset_path(local_dataset.name)\n",
    "#print(f\"Dataset file path: {dataset_path}\")\n",
    "test_eq(\n",
    "    os.path.join(test_dir, local_project.name, 'datasets', f'{local_dataset.name}.csv'),\n",
    "    dataset_path\n",
    ")\n",
    "\n",
    "# open and print raw csv file\n",
    "with open(os.path.join(test_dir, local_project.name, 'datasets', f'{local_dataset.name}.csv')) as f:\n",
    "    print(f.read())\n",
    "\n",
    "# Get the dataset by name\n",
    "retrieved_dataset = local_project.get_dataset(\n",
    "    dataset_name=\"test_dataset_comprehensive\",\n",
    "    model=LocalTestModel\n",
    ")\n",
    "        \n",
    "# Load entries\n",
    "retrieved_dataset.load()\n",
    "print(f\"Retrieved dataset: {retrieved_dataset}\")\n",
    "\n",
    "# Modify an entry\n",
    "entry = retrieved_dataset[1]  \n",
    "entry.name = \"Updated Name\"\n",
    "entry.score = 9.9\n",
    "retrieved_dataset.save(entry)\n",
    "\n",
    "# Load again to verify changes\n",
    "retrieved_dataset.load()\n",
    "print(f\"Updated entry: {retrieved_dataset[1]}\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = retrieved_dataset.to_pandas()\n",
    "print(\"\\nDataFrame (first 2 rows):\")\n",
    "print(df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a55b1028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport os\\nfrom pydantic import BaseModel\\n\\n# Set environment variables for API access\\nRAGAS_APP_TOKEN = \"your-api-key\"\\nRAGAS_API_BASE_URL = \"https://api.dev.app.ragas.io\"\\nos.environ[\"RAGAS_APP_TOKEN\"] = RAGAS_APP_TOKEN\\nos.environ[\"RAGAS_API_BASE_URL\"] = RAGAS_API_BASE_URL\\n\\n# Get a project from the Ragas API\\nragas_app_project = Project.get(\\n    name=\"Your Project Name\",\\n    backend=\"ragas_app\"\\n)\\n\\n# Define a test model\\nclass ApiTestModel(BaseModel):\\n    id: int\\n    name: str\\n    description: str\\n    score: float\\n\\n# Create a dataset with ragas_app backend\\napi_dataset = ragas_app_project.create_dataset(\\n    model=ApiTestModel,\\n    name=\"api_test_dataset\",\\n    backend=\"ragas_app\"\\n)\\n\\n# Add some entries\\nfor i in range(3):\\n    entry = ApiTestModel(\\n        id=i,\\n        name=f\"API Test Item {i}\",\\n        description=f\"Description for API item {i}\",\\n        score=i * 1.1\\n    )\\n    api_dataset.append(entry)\\n\\n# List all datasets in the project\\ndataset_names = ragas_app_project.list_dataset_names(backend=\"ragas_app\")\\nprint(f\"Datasets in project: {dataset_names}\")\\n\\n# Get the dataset by name\\nretrieved_dataset = ragas_app_project.get_dataset(\\n    dataset_name=\"api_test_dataset\",\\n    model=ApiTestModel,\\n    backend=\"ragas_app\"\\n)\\n\\n# Load entries\\nretrieved_dataset.load()\\nprint(f\"Retrieved dataset: {retrieved_dataset}\")\\n\\n# View as DataFrame\\ndf = retrieved_dataset.to_pandas()\\nprint(\"\\nDataFrame:\")\\nprint(df)\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of using ragas_app backend (commented out since it requires API access)\n",
    "'''\n",
    "import os\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# Set environment variables for API access\n",
    "RAGAS_APP_TOKEN = \"your-api-key\"\n",
    "RAGAS_API_BASE_URL = \"https://api.dev.app.ragas.io\"\n",
    "os.environ[\"RAGAS_APP_TOKEN\"] = RAGAS_APP_TOKEN\n",
    "os.environ[\"RAGAS_API_BASE_URL\"] = RAGAS_API_BASE_URL\n",
    "\n",
    "# Get a project from the Ragas API\n",
    "ragas_app_project = Project.get(\n",
    "    name=\"Your Project Name\",\n",
    "    backend=\"ragas_app\"\n",
    ")\n",
    "\n",
    "# Define a test model\n",
    "class ApiTestModel(BaseModel):\n",
    "    id: int\n",
    "    name: str\n",
    "    description: str\n",
    "    score: float\n",
    "\n",
    "# Create a dataset with ragas_app backend\n",
    "api_dataset = ragas_app_project.create_dataset(\n",
    "    model=ApiTestModel,\n",
    "    name=\"api_test_dataset\",\n",
    "    backend=\"ragas_app\"\n",
    ")\n",
    "\n",
    "# Add some entries\n",
    "for i in range(3):\n",
    "    entry = ApiTestModel(\n",
    "        id=i,\n",
    "        name=f\"API Test Item {i}\",\n",
    "        description=f\"Description for API item {i}\",\n",
    "        score=i * 1.1\n",
    "    )\n",
    "    api_dataset.append(entry)\n",
    "\n",
    "# List all datasets in the project\n",
    "dataset_names = ragas_app_project.list_dataset_names(backend=\"ragas_app\")\n",
    "print(f\"Datasets in project: {dataset_names}\")\n",
    "\n",
    "# Get the dataset by name\n",
    "retrieved_dataset = ragas_app_project.get_dataset(\n",
    "    dataset_name=\"api_test_dataset\",\n",
    "    model=ApiTestModel,\n",
    "    backend=\"ragas_app\"\n",
    ")\n",
    "\n",
    "# Load entries\n",
    "retrieved_dataset.load()\n",
    "print(f\"Retrieved dataset: {retrieved_dataset}\")\n",
    "\n",
    "# View as DataFrame\n",
    "df = retrieved_dataset.to_pandas()\n",
    "print(\"\\nDataFrame:\")\n",
    "print(df)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2f761688",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_dataset_class_for_local_backend():\n",
    "    \"\"\"Updates the Dataset class to support local backend.\n",
    "    \n",
    "    This is called when the module is imported to patch the Dataset class\n",
    "    with methods that enable local backend support.\n",
    "    \"\"\"\n",
    "    from ragas_experimental.dataset import Dataset\n",
    "    import csv\n",
    "    import os\n",
    "    import uuid\n",
    "    \n",
    "    # Add backend parameter to Dataset.__init__\n",
    "    original_init = Dataset.__init__\n",
    "    \n",
    "    def new_init(\n",
    "        self,\n",
    "        name: str,\n",
    "        model: t.Type[BaseModel],\n",
    "        project_id: str,\n",
    "        dataset_id: str,\n",
    "        ragas_api_client=None,\n",
    "        backend: t.Literal[\"ragas_app\", \"local\"] = \"ragas_app\",\n",
    "        local_root_dir: t.Optional[str] = None,\n",
    "    ):\n",
    "        self.backend = backend\n",
    "        self.local_root_dir = local_root_dir\n",
    "        \n",
    "        if backend == \"local\":\n",
    "            if local_root_dir is None:\n",
    "                raise ValueError(\"local_root_dir is required for local backend\")\n",
    "                \n",
    "            # Set basic properties\n",
    "            self.name = name\n",
    "            self.model = model\n",
    "            self.project_id = project_id\n",
    "            self.dataset_id = dataset_id\n",
    "            self._ragas_api_client = None\n",
    "            self._entries = []\n",
    "            \n",
    "            # Setup column mapping\n",
    "            if not hasattr(self.model, \"__column_mapping__\"):\n",
    "                self.model.__column_mapping__ = {}\n",
    "                \n",
    "            # For local backend, columns map directly to field names\n",
    "            for field_name in model.__annotations__:\n",
    "                self.model.__column_mapping__[field_name] = field_name\n",
    "                \n",
    "            # Load entries from CSV if it exists\n",
    "            self._load_from_csv()\n",
    "        else:\n",
    "            # Call original init for ragas_app backend\n",
    "            original_init(self, name, model, project_id, dataset_id, ragas_api_client)\n",
    "    \n",
    "    # Add method to load from CSV\n",
    "    def _load_from_csv(self):\n",
    "        \"\"\"Load dataset entries from CSV file.\"\"\"\n",
    "        if self.backend != \"local\":\n",
    "            return\n",
    "            \n",
    "        # Construct CSV path\n",
    "        project_dir = os.path.join(self.local_root_dir, self.project_id)\n",
    "        csv_path = os.path.join(project_dir, \"datasets\", f\"{self.name}.csv\")\n",
    "        \n",
    "        if not os.path.exists(csv_path):\n",
    "            return\n",
    "            \n",
    "        # Read CSV\n",
    "        with open(csv_path, 'r', newline='') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            \n",
    "            # Clear existing entries\n",
    "            self._entries.clear()\n",
    "            \n",
    "            # Process rows\n",
    "            for row in reader:\n",
    "                try:\n",
    "                    # Convert types as needed based on model annotations\n",
    "                    typed_row = {}\n",
    "                    for field, value in row.items():\n",
    "                        if field in self.model.__annotations__:\n",
    "                            field_type = self.model.__annotations__[field]\n",
    "                            \n",
    "                            # Handle basic type conversions\n",
    "                            if field_type == int:\n",
    "                                typed_row[field] = int(value) if value else 0\n",
    "                            elif field_type == float:\n",
    "                                typed_row[field] = float(value) if value else 0.0\n",
    "                            elif field_type == bool:\n",
    "                                typed_row[field] = value.lower() in ('true', 't', 'yes', 'y', '1')\n",
    "                            else:\n",
    "                                typed_row[field] = value\n",
    "                    \n",
    "                    # Create model instance\n",
    "                    entry = self.model(**typed_row)\n",
    "                    \n",
    "                    # Add row_id for tracking changes\n",
    "                    entry._row_id = str(uuid.uuid4())\n",
    "                    \n",
    "                    self._entries.append(entry)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading row: {e}\")\n",
    "    \n",
    "    # Add method to save to CSV\n",
    "    def _save_to_csv(self):\n",
    "        \"\"\"Save all entries to CSV file.\"\"\"\n",
    "        if self.backend != \"local\":\n",
    "            return\n",
    "            \n",
    "        # Construct CSV path\n",
    "        project_dir = os.path.join(self.local_root_dir, self.project_id)\n",
    "        csv_path = os.path.join(project_dir, \"datasets\", f\"{self.name}.csv\")\n",
    "        \n",
    "        # Ensure directory exists\n",
    "        os.makedirs(os.path.dirname(csv_path), exist_ok=True)\n",
    "        \n",
    "        # Get field names from model\n",
    "        field_names = list(self.model.__annotations__.keys())\n",
    "        \n",
    "        # Write to CSV\n",
    "        with open(csv_path, 'w', newline='') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=field_names)\n",
    "            writer.writeheader()\n",
    "            \n",
    "            for entry in self._entries:\n",
    "                # Convert model instance to dict and write row\n",
    "                writer.writerow(entry.model_dump())\n",
    "    \n",
    "    # Patch the original methods to support local backend\n",
    "    \n",
    "    # Patch append\n",
    "    original_append = Dataset.append\n",
    "    \n",
    "    def new_append(self, entry):\n",
    "        if self.backend == \"local\":\n",
    "            if not isinstance(entry, self.model):\n",
    "                raise TypeError(f\"Entry must be an instance of {self.model.__name__}\")\n",
    "                \n",
    "            # Add row_id for tracking changes\n",
    "            entry._row_id = str(uuid.uuid4())\n",
    "            \n",
    "            # Add to in-memory entries\n",
    "            self._entries.append(entry)\n",
    "            \n",
    "            # Save to CSV\n",
    "            self._save_to_csv()\n",
    "        else:\n",
    "            original_append(self, entry)\n",
    "    \n",
    "    # Patch pop\n",
    "    original_pop = Dataset.pop\n",
    "    \n",
    "    def new_pop(self, index=-1):\n",
    "        if self.backend == \"local\":\n",
    "            # Remove from in-memory entries\n",
    "            entry = self._entries.pop(index)\n",
    "            \n",
    "            # Save to CSV\n",
    "            self._save_to_csv()\n",
    "            \n",
    "            return entry\n",
    "        else:\n",
    "            return original_pop(self, index)\n",
    "    \n",
    "    # Patch load\n",
    "    original_load = Dataset.load\n",
    "    \n",
    "    def new_load(self):\n",
    "        if self.backend == \"local\":\n",
    "            self._load_from_csv()\n",
    "        else:\n",
    "            original_load(self)\n",
    "    \n",
    "    # Patch save\n",
    "    original_save = Dataset.save\n",
    "    \n",
    "    def new_save(self, item):\n",
    "        if self.backend == \"local\":\n",
    "            if not isinstance(item, self.model):\n",
    "                raise TypeError(f\"Item must be an instance of {self.model.__name__}\")\n",
    "                \n",
    "            # Find the item in our entries\n",
    "            found = False\n",
    "            for i, entry in enumerate(self._entries):\n",
    "                if hasattr(entry, \"_row_id\") and hasattr(item, \"_row_id\") and entry._row_id == item._row_id:\n",
    "                    # Update the entry\n",
    "                    self._entries[i] = item\n",
    "                    found = True\n",
    "                    break\n",
    "                    \n",
    "            if not found:\n",
    "                # If we didn't find it, add it\n",
    "                if not hasattr(item, \"_row_id\"):\n",
    "                    item._row_id = str(uuid.uuid4())\n",
    "                self._entries.append(item)\n",
    "                \n",
    "            # Save to CSV\n",
    "            self._save_to_csv()\n",
    "        else:\n",
    "            original_save(self, item)\n",
    "    \n",
    "    # Apply all patches\n",
    "    Dataset.__init__ = new_init\n",
    "    Dataset._load_from_csv = _load_from_csv\n",
    "    Dataset._save_to_csv = _save_to_csv\n",
    "    Dataset.append = new_append\n",
    "    Dataset.pop = new_pop\n",
    "    Dataset.load = new_load\n",
    "    Dataset.save = new_save\n",
    "    \n",
    "    return Dataset\n",
    "\n",
    "# Update the Dataset class\n",
    "updated_dataset_class = update_dataset_class_for_local_backend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "23a6eabf",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "update_dataset_class_for_local_backend.<locals>.new_init() got an unexpected keyword argument 'datatable_type'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m     score: \u001b[38;5;28mfloat\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Create a dataset with local backend\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m local_dataset = \u001b[43mlocal_project\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLocalTestModel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtest_dataset\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlocal\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     28\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Add some entries to the dataset\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m5\u001b[39m):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mcreate_dataset\u001b[39m\u001b[34m(self, model, name, backend)\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Create dataset using the appropriate backend\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m backend == \u001b[33m\"\u001b[39m\u001b[33mlocal\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_dataset_from_local\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m backend == \u001b[33m\"\u001b[39m\u001b[33mragas_app\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     31\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m get_dataset_from_ragas_app(\u001b[38;5;28mself\u001b[39m, name, model)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mget_dataset_from_local\u001b[39m\u001b[34m(self, name, model)\u001b[39m\n\u001b[32m     17\u001b[39m dataset_id = create_nano_id()\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Return a new Dataset instance with local backend\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdatatable_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdatasets\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproject_id\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mproject_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlocal\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_root_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdirname\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_root_dir\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Root dir for all projects\u001b[39;49;00m\n\u001b[32m     28\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: update_dataset_class_for_local_backend.<locals>.new_init() got an unexpected keyword argument 'datatable_type'"
     ]
    }
   ],
   "source": [
    "# Example of using the local backend Dataset operations\n",
    "import tempfile\n",
    "import os\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# Create a temporary directory for demonstration\n",
    "with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    # Create a new project with local backend\n",
    "    local_project = Project.create(\n",
    "        name=\"test_local_project\",\n",
    "        description=\"A test project using local backend\",\n",
    "        backend=\"local\",\n",
    "        root_dir=temp_dir\n",
    "    )\n",
    "    \n",
    "    # Define a test model\n",
    "    class LocalTestModel(BaseModel):\n",
    "        id: int\n",
    "        name: str\n",
    "        description: str\n",
    "        score: float\n",
    "    \n",
    "    # Create a dataset with local backend\n",
    "    local_dataset = local_project.create_dataset(\n",
    "        model=LocalTestModel,\n",
    "        name=\"test_dataset\",\n",
    "        backend=\"local\"\n",
    "    )\n",
    "    \n",
    "    # Add some entries to the dataset\n",
    "    for i in range(5):\n",
    "        entry = LocalTestModel(\n",
    "            id=i,\n",
    "            name=f\"Test Item {i}\",\n",
    "            description=f\"Description for item {i}\",\n",
    "            score=i * 0.1\n",
    "        )\n",
    "        local_dataset.append(entry)\n",
    "    \n",
    "    # Print the dataset contents\n",
    "    print(f\"Dataset after adding entries: {local_dataset}\")\n",
    "    \n",
    "    # Check the CSV file\n",
    "    dataset_path = local_project.get_dataset_path(\"test_dataset\")\n",
    "    print(f\"Dataset file path: {dataset_path}\")\n",
    "    with open(dataset_path, 'r') as f:\n",
    "        csv_content = f.read()\n",
    "    print(f\"CSV content:\\n{csv_content}\")\n",
    "    \n",
    "    # Modify an entry\n",
    "    entry = local_dataset[2]\n",
    "    entry.name = \"Updated Name\"\n",
    "    entry.score = 9.9\n",
    "    local_dataset.save(entry)\n",
    "    \n",
    "    # Load the dataset again\n",
    "    local_dataset.load()\n",
    "    \n",
    "    # Print updated entry\n",
    "    print(f\"Updated entry: {local_dataset[2]}\")\n",
    "    \n",
    "    # Convert to pandas DataFrame\n",
    "    df = local_dataset.to_pandas()\n",
    "    print(\"\\nDataFrame:\")\n",
    "    print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
