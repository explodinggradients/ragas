{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "215f57b4",
   "metadata": {},
   "source": [
    "# MetricResult\n",
    "> MetricResult object to store the result of a metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164726f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp metric.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc3080c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import typing as t\n",
    "\n",
    "from fastcore.utils import patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1c801a-6568-4ba4-8bbe-30bf154174fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MetricResult:\n",
    "    \"\"\"Class to hold the result of a metric evaluation.\n",
    "    \n",
    "    This class behaves like its underlying result value but still provides access\n",
    "    to additional metadata like reasoning.\n",
    "    \n",
    "    Works with:\n",
    "    - DiscreteMetrics (string results)\n",
    "    - NumericMetrics (float/int results)\n",
    "    - RankingMetrics (list results)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, result: t.Any, reason: t.Optional[str] = None, traces: t.Optional[t.Dict[str, t.Any]] = None):\n",
    "        if traces is not None:\n",
    "            invalid_keys = [key for key in traces.keys() if key not in {\"input\", \"output\"}]\n",
    "            if invalid_keys:\n",
    "                raise ValueError(f\"Invalid keys in traces: {invalid_keys}. Allowed keys are 'input' and 'output'.\")\n",
    "        self._result = result\n",
    "        self.reason = reason\n",
    "        self.traces = traces\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return repr(self._result)\n",
    "    \n",
    "    # Access to underlying result\n",
    "    @property\n",
    "    def result(self):\n",
    "        \"\"\"Get the raw result value.\"\"\"\n",
    "        return self._result\n",
    "    \n",
    "    \n",
    "    # String conversion - works for all types\n",
    "    def __str__(self):\n",
    "        return str(self._result)\n",
    "    \n",
    "    # Container-like behaviors for list results (RankingMetric)\n",
    "    def __getitem__(self, key):\n",
    "        if not hasattr(self._result, \"__getitem__\"):\n",
    "            raise TypeError(f\"{type(self._result).__name__} object is not subscriptable\")\n",
    "        return self._result[key]\n",
    "    \n",
    "    def __iter__(self):\n",
    "        if not hasattr(self._result, \"__iter__\"):\n",
    "            raise TypeError(f\"{type(self._result).__name__} object is not iterable\")\n",
    "        return iter(self._result)\n",
    "    \n",
    "    def __len__(self):\n",
    "        if not hasattr(self._result, \"__len__\"):\n",
    "            raise TypeError(f\"{type(self._result).__name__} has no len()\")\n",
    "        return len(self._result)\n",
    "    \n",
    "    # Numeric operations for numeric results (NumericMetric)\n",
    "    def __float__(self):\n",
    "        if isinstance(self._result, (int, float)):\n",
    "            return float(self._result)\n",
    "        raise TypeError(f\"Cannot convert {type(self._result).__name__} to float\")\n",
    "    \n",
    "    def __int__(self):\n",
    "        if isinstance(self._result, (int, float)):\n",
    "            return int(self._result)\n",
    "        raise TypeError(f\"Cannot convert {type(self._result).__name__} to int\")\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        if not isinstance(self._result, (int, float)):\n",
    "            raise TypeError(f\"Cannot add {type(self._result).__name__} objects\")\n",
    "        if isinstance(other, MetricResult):\n",
    "            return self._result + other._result\n",
    "        return self._result + other\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        if not isinstance(self._result, (int, float)):\n",
    "            raise TypeError(f\"Cannot add {type(self._result).__name__} objects\")\n",
    "        return other + self._result\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        if not isinstance(self._result, (int, float)):\n",
    "            raise TypeError(f\"Cannot subtract {type(self._result).__name__} objects\")\n",
    "        if isinstance(other, MetricResult):\n",
    "            return self._result - other._result\n",
    "        return self._result - other\n",
    "    \n",
    "    def __rsub__(self, other):\n",
    "        if not isinstance(self._result, (int, float)):\n",
    "            raise TypeError(f\"Cannot subtract {type(self._result).__name__} objects\")\n",
    "        return other - self._result\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        if not isinstance(self._result, (int, float)):\n",
    "            raise TypeError(f\"Cannot multiply {type(self._result).__name__} objects\")\n",
    "        if isinstance(other, MetricResult):\n",
    "            return self._result * other._result\n",
    "        return self._result * other\n",
    "    \n",
    "    def __rmul__(self, other):\n",
    "        if not isinstance(self._result, (int, float)):\n",
    "            raise TypeError(f\"Cannot multiply {type(self._result).__name__} objects\")\n",
    "        return other * self._result\n",
    "    \n",
    "    def __truediv__(self, other):\n",
    "        if not isinstance(self._result, (int, float)):\n",
    "            raise TypeError(f\"Cannot divide {type(self._result).__name__} objects\")\n",
    "        if isinstance(other, MetricResult):\n",
    "            return self._result / other._result\n",
    "        return self._result / other\n",
    "    \n",
    "    def __rtruediv__(self, other):\n",
    "        if not isinstance(self._result, (int, float)):\n",
    "            raise TypeError(f\"Cannot divide {type(self._result).__name__} objects\")\n",
    "        return other / self._result\n",
    "    \n",
    "    # Comparison operations - work for all types with same-type comparisons\n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, MetricResult):\n",
    "            return self._result == other._result\n",
    "        return self._result == other\n",
    "    \n",
    "    def __lt__(self, other):\n",
    "        if isinstance(other, MetricResult):\n",
    "            return self._result < other._result\n",
    "        return self._result < other\n",
    "    \n",
    "    def __le__(self, other):\n",
    "        if isinstance(other, MetricResult):\n",
    "            return self._result <= other._result\n",
    "        return self._result <= other\n",
    "    \n",
    "    def __gt__(self, other):\n",
    "        if isinstance(other, MetricResult):\n",
    "            return self._result > other._result\n",
    "        return self._result > other\n",
    "    \n",
    "    def __ge__(self, other):\n",
    "        if isinstance(other, MetricResult):\n",
    "            return self._result >= other._result\n",
    "        return self._result >= other\n",
    "    \n",
    "    # Method forwarding for type-specific behaviors\n",
    "    def __getattr__(self, name):\n",
    "        \"\"\"Forward attribute access to the result object if it has that attribute.\n",
    "        \n",
    "        This allows calling string methods on discrete results, \n",
    "        numeric methods on numeric results, and list methods on ranking results.\n",
    "        \"\"\"\n",
    "        if hasattr(self._result, name):\n",
    "            attr = getattr(self._result, name)\n",
    "            if callable(attr):\n",
    "                # If it's a method, wrap it to return MetricResult when appropriate\n",
    "                def wrapper(*args, **kwargs):\n",
    "                    result = attr(*args, **kwargs)\n",
    "                    # If the result is of the same type as self._result, wrap it\n",
    "                    if isinstance(result, type(self._result)):\n",
    "                        return MetricResult(result=result, reason=self.reason)\n",
    "                    return result\n",
    "                return wrapper\n",
    "            return attr\n",
    "        raise AttributeError(f\"{type(self).__name__} has no attribute '{name}'\")\n",
    "    \n",
    "    # JSON/dict serialization\n",
    "    def to_dict(self):\n",
    "        \"\"\"Convert the result to a dictionary.\"\"\"\n",
    "        return {\n",
    "            \"result\": self._result,\n",
    "            \"reason\": self.reason\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490cdd2f",
   "metadata": {},
   "source": [
    "### Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24589401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n",
      "This is a test\n",
      "8.0\n",
      "LOW\n",
      "[2, 3]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "metric_result = MetricResult(result=42, reason=\"This is a test\")\n",
    "print(metric_result)\n",
    "print(metric_result.reason)\n",
    "\n",
    "### Example with Numeric Operations\n",
    "num_result1 = MetricResult(result=5.0)\n",
    "num_result2 = MetricResult(result=3.0)\n",
    "print(num_result1 + num_result2)  # 8.0\n",
    "\n",
    "\n",
    "### Example with String Operations\n",
    "str_result = MetricResult(result=\"low\")\n",
    "print(str_result.upper())  # \"LOW\"\n",
    "\n",
    "## Example with List Operations\n",
    "list_result = MetricResult(result=[1, 2, 3])\n",
    "print(list_result[1:])  # 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ce7a1d",
   "metadata": {},
   "source": [
    "now lets make it `Pydantic` compatible also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8fb818",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from pydantic_core import core_schema\n",
    "from pydantic import GetCoreSchemaHandler, ValidationInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c288c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@patch(cls_method=True)\n",
    "def validate(cls: MetricResult, value: t.Any, info: ValidationInfo):\n",
    "    \"\"\"Provide compatibility with older Pydantic versions.\"\"\"\n",
    "    if isinstance(value, MetricResult):\n",
    "        return value\n",
    "    return MetricResult(result=value)\n",
    "\n",
    "@patch\n",
    "def __json__(self: MetricResult):\n",
    "    \"\"\"Return data for JSON serialization.\n",
    "    \n",
    "    This method is used by json.dumps and other JSON serializers \n",
    "    to convert MetricResult to a JSON-compatible format.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"result\": self._result,\n",
    "        \"reason\": self.reason,\n",
    "    }\n",
    "\n",
    "@patch(cls_method=True)\n",
    "def __get_pydantic_core_schema__(\n",
    "    cls: MetricResult, \n",
    "    _source_type: t.Any, \n",
    "    _handler: GetCoreSchemaHandler\n",
    ") -> core_schema.CoreSchema:\n",
    "    \"\"\"Generate a Pydantic core schema for MetricResult.\n",
    "    \n",
    "    This custom schema handles different serialization behaviors:\n",
    "    - For model_dump(): Returns the original MetricResult instance\n",
    "    - For model_dump_json(): Converts to a JSON-compatible dict using __json__\n",
    "    \"\"\"\n",
    "    def serializer_function(instance, info):\n",
    "        \"\"\"Handle different serialization modes for MetricResult.\"\"\"\n",
    "        # For JSON serialization (model_dump_json), use __json__ method\n",
    "        if getattr(info, 'mode', None) == 'json':\n",
    "            return instance.__json__()\n",
    "        # For Python serialization (model_dump), return the instance itself\n",
    "        return instance\n",
    "    \n",
    "    return core_schema.union_schema([\n",
    "        # First schema: handles validation of MetricResult instances\n",
    "        core_schema.is_instance_schema(MetricResult),\n",
    "        \n",
    "        # Second schema: handles validation of other values and conversion to MetricResult\n",
    "        core_schema.chain_schema([\n",
    "            core_schema.any_schema(),\n",
    "            core_schema.no_info_plain_validator_function(\n",
    "                lambda value: MetricResult(result=value) if not isinstance(value, MetricResult) else value\n",
    "            ),\n",
    "        ]),\n",
    "    ], serialization=core_schema.plain_serializer_function_ser_schema(\n",
    "        serializer_function,\n",
    "        info_arg=True  # Explicitly specify that we're using the info argument\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49739a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class TestModel(BaseModel):\n",
    "    response: str\n",
    "    grade: MetricResult\n",
    "    faithfulness: MetricResult\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac6b955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TestModel(response='test', grade=1, faithfulness=1)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = TestModel(response=\"test\", grade=MetricResult(result=1, reason=\"test\"), faithfulness=MetricResult(result=1, reason=\"test\"))\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffe750f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'response': 'test', 'grade': 1, 'faithfulness': 1}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc2a1ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"response\":\"test\",\"grade\":{\"result\":1,\"reason\":\"test\"},\"faithfulness\":{\"result\":1,\"reason\":\"test\"}}'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.model_dump_json()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
