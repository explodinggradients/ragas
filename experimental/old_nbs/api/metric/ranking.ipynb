{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RankingMetric\n",
    "> Base class for ranking metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp metric.ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/random/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "\n",
    "import typing as t\n",
    "from dataclasses import dataclass\n",
    "from pydantic import BaseModel, Field\n",
    "from ragas_experimental.metric import Metric, MetricResult\n",
    "from ragas_experimental.metric.decorator import create_metric_decorator\n",
    "\n",
    "@dataclass\n",
    "class RankingMetric(Metric):\n",
    "    num_ranks: int\n",
    "    \n",
    "    def _get_response_model(self, with_reasoning: bool) -> t.Type[BaseModel]:\n",
    "        \"\"\"Get or create a response model based on reasoning parameter.\"\"\"\n",
    "        \n",
    "        if with_reasoning in self._response_models:\n",
    "            return self._response_models[with_reasoning]\n",
    "        \n",
    "        # Store values needed for validation\n",
    "        num_ranks = self.num_ranks\n",
    "        \n",
    "        # Create explicit model classes instead of using create_model\n",
    "        if with_reasoning:\n",
    "            # Model with result and reason\n",
    "            class ResponseModelWithReason(BaseModel):\n",
    "                result: t.List[int] = Field(...)\n",
    "                reason: str = Field(...)\n",
    "                \n",
    "                def model_post_init(self, __context):\n",
    "                    expected = set(range(num_ranks))\n",
    "                    if set(self.result) != expected:\n",
    "                        raise ValueError(\n",
    "                            f\"'result' must contain exactly the numbers {sorted(expected)} without repetition.\"\n",
    "                        )\n",
    "            \n",
    "            self._response_models[with_reasoning] = ResponseModelWithReason\n",
    "            return ResponseModelWithReason\n",
    "        else:\n",
    "            # Model with just result\n",
    "            class ResponseModel(BaseModel):\n",
    "                result: t.List[int] = Field(...)\n",
    "                \n",
    "                def model_post_init(self, __context):\n",
    "                    expected = set(range(num_ranks))\n",
    "                    if set(self.result) != expected:\n",
    "                        raise ValueError(\n",
    "                            f\"'result' must contain exactly the numbers {sorted(expected)} without repetition.\"\n",
    "                        )\n",
    "            \n",
    "            self._response_models[with_reasoning] = ResponseModel\n",
    "            return ResponseModel\n",
    "\n",
    "    def _ensemble(self, results: t.List[MetricResult]) -> MetricResult:\n",
    "        if len(results) == 1:\n",
    "            return results[0]\n",
    "\n",
    "        n_items = self.num_ranks  # Use the class attribute instead of len(results)\n",
    "        borda_scores = [0] * n_items\n",
    "\n",
    "        for result in results:\n",
    "            for position_idx, item_idx in enumerate(result.result):\n",
    "                borda_scores[item_idx] += (n_items - position_idx)  # Fixed the formula\n",
    "\n",
    "        indexed_scores = [(score, i) for i, score in enumerate(borda_scores)]    \n",
    "        indexed_scores.sort(key=lambda x: (-x[0], x[1]))    \n",
    "        final_ranking = [pos for _, pos in indexed_scores]\n",
    "\n",
    "        if any(r.reason for r in results):\n",
    "            reason = \"Ensemble ranking based on multiple evaluations.\\n\" + '\\n'.join([r.reason for r in results if r.reason])\n",
    "        else:\n",
    "            reason = None\n",
    "        \n",
    "        return MetricResult(result=final_ranking, reason=reason)\n",
    "    \n",
    "\n",
    "ranking_metric = create_metric_decorator(RankingMetric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1, 0]\n",
      "Ensemble ranking based on multiple evaluations.\n",
      "The ranking is based on the length and detail of each response. 'the longest and most detailed answer.' is the most comprehensive, followed by 'a bit more detailed.', and 'short answer.' is the briefest.\n",
      "The ranking is based on the length and detail of each response. The response 'the longest and most detailed answer.' is ranked highest (2) because it is the most detailed, followed by 'a bit more detailed.' (1), and finally 'short answer.' (0) as it is the least detailed.\n",
      "The responses are ranked based on the level of detail and length. 'short answer.' is the least detailed, 'a bit more detailed.' provides more information, and 'the longest and most detailed answer.' offers the most comprehensive explanation.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#| eval: false\n",
    "\n",
    "from ragas_experimental.llm import ragas_llm\n",
    "from openai import OpenAI\n",
    "\n",
    "llm = ragas_llm(provider=\"openai\",model=\"gpt-4o\",client=OpenAI())\n",
    "\n",
    "my_ranking_metric = RankingMetric(\n",
    "    name='response_ranking',\n",
    "    llm=llm,  # Your language model instance\n",
    "    prompt=\"Rank the following responses:\\n{candidates}\",\n",
    "    num_ranks=3,\n",
    ")\n",
    "\n",
    "# To score a single input (ranking candidate responses)\n",
    "result = my_ranking_metric.score(candidates=[\n",
    "    \"short answer.\",\n",
    "    \"a bit more detailed.\",\n",
    "    \"the longest and most detailed answer.\"\n",
    "],n=3)\n",
    "print(result)   # Might output something like: [1, 0, 2]\n",
    "print(result.reason)  # Provides the reasoning behind the ranking\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom ranking metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 2]\n",
      "Ranked based on response clarity and detail.\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "\n",
    "from ragas_experimental.metric import MetricResult\n",
    "\n",
    "@ranking_metric(\n",
    "    llm=llm,  # Your language model instance\n",
    "    prompt=\"Rank the following responses:\\n{candidates}\",\n",
    "    name='new_ranking_metric',\n",
    "    num_ranks=3\n",
    ")\n",
    "def my_ranking_metric(llm, prompt, **kwargs):\n",
    "    # Your custom logic that calls the LLM and returns a tuple of (ranking, reason)\n",
    "    # For example, process the prompt (formatted with candidates) and produce a ranking.\n",
    "    ranking = [1, 0, 2]  # Dummy ranking: second candidate is best, then first, then third.\n",
    "    reason = \"Ranked based on response clarity and detail.\"\n",
    "    return MetricResult(result=ranking, reason=reason)\n",
    "\n",
    "# Using the decorator-based ranking metric:\n",
    "result = my_ranking_metric.score(candidates=[\n",
    "    \"Response A: short answer.\",\n",
    "    \"Response B: a bit more detailed.\",\n",
    "    \"Response C: the longest and most detailed answer.\"\n",
    "])\n",
    "print(result)   # E.g., [1, 0, 2]\n",
    "print(result.reason)  # E.g., \"Ranked based on response clarity and detail.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
