{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp metric.decorator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decorator Factory\n",
    "> decorator factory for creating custom metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import typing as t\n",
    "import inspect\n",
    "import asyncio\n",
    "from dataclasses import dataclass\n",
    "from ragas_experimental.metric import MetricResult\n",
    "from ragas_experimental.llm import RagasLLM\n",
    "from ragas_experimental.prompt.base import Prompt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_metric_decorator(metric_class):\n",
    "    \"\"\"\n",
    "    Factory function that creates decorator factories for different metric types.\n",
    "    \n",
    "    Args:\n",
    "        metric_class: The metric class to use (DiscreteMetrics, NumericMetrics, etc.)\n",
    "        \n",
    "    Returns:\n",
    "        A decorator factory function for the specified metric type\n",
    "    \"\"\"\n",
    "    def decorator_factory(llm:RagasLLM, prompt: t.Union[str, Prompt], name: t.Optional[str] = None, **metric_params):\n",
    "        \"\"\"\n",
    "        Creates a decorator that wraps a function into a metric instance.\n",
    "        \n",
    "        Args:\n",
    "            llm: The language model instance to use\n",
    "            prompt: The prompt template\n",
    "            name: Optional name for the metric (defaults to function name)\n",
    "            **metric_params: Additional parameters specific to the metric type\n",
    "                (values for DiscreteMetrics, range for NumericMetrics, etc.)\n",
    "        \n",
    "        Returns:\n",
    "            A decorator function\n",
    "        \"\"\"\n",
    "        def decorator(func):\n",
    "            # Get metric name and check if function is async\n",
    "            metric_name = name or func.__name__\n",
    "            is_async = inspect.iscoroutinefunction(func)\n",
    "            \n",
    "            #TODO: Move to dataclass type implementation\n",
    "            @dataclass\n",
    "            class CustomMetric(metric_class):\n",
    "                \n",
    "                def _run_sync_in_async(self, func, *args, **kwargs):\n",
    "                    \"\"\"Run a synchronous function in an async context.\"\"\"\n",
    "                    # For sync functions, just run them normally\n",
    "                    return func(*args, **kwargs)\n",
    "                \n",
    "                def _execute_metric(self, is_async_execution, reasoning, **kwargs):\n",
    "                    \"\"\"Execute the metric function with proper async handling.\"\"\"\n",
    "                    try:\n",
    "                        if is_async:\n",
    "                            # Async function implementation\n",
    "                            if is_async_execution:\n",
    "                                # In async context, await the function directly\n",
    "                                result = func(self.llm, self.prompt, **kwargs)\n",
    "                            else:\n",
    "                                # In sync context, run the async function in an event loop\n",
    "                                try:\n",
    "                                    loop = asyncio.get_event_loop()\n",
    "                                except RuntimeError:\n",
    "                                    loop = asyncio.new_event_loop()\n",
    "                                    asyncio.set_event_loop(loop)\n",
    "                                result = loop.run_until_complete(func(self.llm, self.prompt, **kwargs))\n",
    "                        else:\n",
    "                            # Sync function implementation\n",
    "                            result = func(self.llm, self.prompt, **kwargs)\n",
    "                        \n",
    "                        return result\n",
    "                    except Exception as e:\n",
    "                        # Handle errors gracefully\n",
    "                        error_msg = f\"Error executing metric {self.name}: {str(e)}\"\n",
    "                        return MetricResult(result=None, reason=error_msg)\n",
    "                \n",
    "                def score(self, reasoning: bool = True, n: int = 1, **kwargs):\n",
    "                    \"\"\"Synchronous scoring method.\"\"\"\n",
    "                    return self._execute_metric(is_async_execution=False, reasoning=reasoning, **kwargs)\n",
    "                \n",
    "                async def ascore(self, reasoning: bool = True, n: int = 1, **kwargs):\n",
    "                    \"\"\"Asynchronous scoring method.\"\"\"\n",
    "                    if is_async:\n",
    "                        # For async functions, await the result\n",
    "                        result = await func(self.llm, self.prompt, **kwargs)\n",
    "                        return self._extract_result(result, reasoning)\n",
    "                    else:\n",
    "                        # For sync functions, run normally\n",
    "                        result = self._run_sync_in_async(func, self.llm, self.prompt, **kwargs)\n",
    "                        return result\n",
    "            \n",
    "            # Create the metric instance with all parameters\n",
    "            metric_instance = CustomMetric(\n",
    "                name=metric_name,\n",
    "                prompt=prompt,\n",
    "                llm=llm,\n",
    "                **metric_params\n",
    "            )\n",
    "            \n",
    "            # Preserve metadata\n",
    "            metric_instance.__name__ = metric_name\n",
    "            metric_instance.__doc__ = func.__doc__\n",
    "            \n",
    "            return metric_instance\n",
    "        \n",
    "        return decorator\n",
    "    \n",
    "    return decorator_factory\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "low\n",
      "The context or details of the user's response ('my response') are not provided, making it impossible to evaluate its helpfulness accurately.\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "\n",
    "\n",
    "from ragas_experimental.metric import DiscreteMetric, MetricResult\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from ragas_experimental.llm import ragas_llm\n",
    "from openai import OpenAI\n",
    "\n",
    "llm = ragas_llm(provider=\"openai\",model=\"gpt-4o\",client=OpenAI())\n",
    "\n",
    "discrete_metric = create_metric_decorator(DiscreteMetric)\n",
    "\n",
    "@discrete_metric(llm=llm,\n",
    "    prompt=\"Evaluate if given answer is helpful\\n\\n{response}\",\n",
    "    name='new_metric',values=[\"low\",\"med\",\"high\"])\n",
    "def my_metric(llm,prompt,**kwargs):\n",
    "\n",
    "        class response_model(BaseModel):\n",
    "             output: t.List[bool]\n",
    "             reason: str\n",
    "        \n",
    "        response = llm.generate(prompt.format(**kwargs),response_model=response_model)\n",
    "        total = sum(response.output)\n",
    "        if total < 1:\n",
    "            score = 'low'\n",
    "        else:\n",
    "            score = 'high'\n",
    "        return MetricResult(result=score, reason=response.reason)\n",
    "\n",
    "result = my_metric.score(response='my response') # result\n",
    "print(result)\n",
    "print(result.reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
