{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb5be51b",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34105067-eed8-4f09-913f-7043ff77cb3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ragas/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from ragas.testset import TestsetGenerator\n",
    "\n",
    "import os\n",
    "import openai\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39b1990c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import download_loader, SimpleDirectoryReader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "321dbebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7457125d",
   "metadata": {},
   "source": [
    "## Assessing current state of test date generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ce32d303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/shahules/belar/experimental\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1bc5970e-0873-4e06-9b68-3dda31b1d478",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ArxivReader = download_loader(\"ArxivReader\")\n",
    "\n",
    "loader = ArxivReader() #uses simpledirectory reader under the hood, hence need modification to laod pages properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "46684465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents = loader.load_data(\"retrieval augmented generation AND large language model\",max_results=20,papers_dir='./arxiv-papers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62b52b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = SimpleDirectoryReader(\"./arxiv-papers/\",num_files_limit=10)\n",
    "documents = reader.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc3d9cf",
   "metadata": {},
   "source": [
    "## SimpleDirectoryReader?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aab30508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b28f535a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(documents[14].get_metadata_str())\n",
    "# print(\"CONTENT\")\n",
    "# print(documents[14].get_content())\n",
    "# print(len(documents[14].get_content().split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4139e70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 5.,  3.,  6., 13., 28., 21., 39., 10.,  5.,  1.]),\n",
       " array([  10. ,  113.1,  216.2,  319.3,  422.4,  525.5,  628.6,  731.7,\n",
       "         834.8,  937.9, 1041. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist([len(doc.get_content().split()) for doc in documents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df051faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dist =  {\n",
    "    \"simple\": 0.4,\n",
    "    \"reasoning\": 0.3,\n",
    "    \"multi_context\": 0.0,\n",
    "    \"conditional\": 0.3,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f9c0891",
   "metadata": {},
   "outputs": [],
   "source": [
    "testsetgenerator = TestsetGenerator.from_default(testset_distribution=test_dist,chunk_size=712)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "553ac808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/shahules/belar/experimental\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "442cd5a6-6856-4488-8dae-be7195586986",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                            | 0/25 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 816\n",
      "seed question What is the effect of retrieval on the use of trivial heuristics by human annotators in counterfactual edits?\n",
      "{'reason': \"The specific study or context for 'retrieval', 'trivial heuristics', 'human annotators', and 'counterfactual edits' is not provided.\", 'verdict': 'No'}\n",
      "rewritten question What are the qualitative differences between counterfactual edits made by humans when they are shown retrieved data vs. when they are not?\n",
      "Len of text chunks 750\n",
      "seed question What is the purpose of the GPT-3 Editor module?\n",
      "{'reason': 'The question is clear without additional context', 'verdict': 'Yes'}\n",
      "{'reason': 'The question is clear without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  4%|â–ˆâ–ˆ                                                  | 1/25 [00:37<15:01, 37.55s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 779\n",
      "seed question What is the purpose of using native dialog prompts in the experimental setup?\n",
      "{'reason': 'The specific experiment or study being referred to is not mentioned in the question.', 'verdict': 'No'}\n",
      "rewritten question What is the purpose of using native dialog prompts in the experimental setup of the study?\n",
      "Len of text chunks 704\n",
      "Len of text chunks 0\n",
      "Len of text chunks 1075\n",
      "seed question What is the purpose of using reflection tokens in SELF-RAG?\n",
      "{'reason': 'The question is clear without additional context', 'verdict': 'Yes'}\n",
      "{'reason': 'The question is clear without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 12%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                             | 3/25 [01:25<10:04, 27.47s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 767\n",
      "seed question What are the impacts of different retrievers on top of LLaMA2-70B?\n",
      "{'reason': \"The specific context or field of study related to 'LLaMA2-70B' and 'retrievers' is not mentioned in the question.\", 'verdict': 'No'}\n",
      "rewritten question What are the impacts of Dragon, Contriever, and OpenAI embeddings on top of LLaMA2-70B?\n",
      "Len of text chunks 1007\n",
      "seed question What is the purpose of the CORE framework in generating counterfactual perturbations?\n",
      "{'reason': 'The question is clear without additional context', 'verdict': 'Yes'}\n",
      "{'reason': 'The question is clear without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                       | 6/25 [02:15<06:37, 20.92s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 926\n",
      "seed question What is the methodology used in the pipeline framework called Rewrite-Retrieve-Read?\n",
      "{'reason': 'The question is clear without additional context', 'verdict': 'Yes'}\n",
      "{'reason': 'The specific pipeline framework or study being referred to is not mentioned in the question.', 'verdict': 'No'}\n",
      "Len of text chunks 582\n",
      "seed question What is the maximum token length for the 13B model during training?\n",
      "{'reason': 'The specific model or system being referred to is not mentioned in the question.', 'verdict': 'No'}\n",
      "rewritten question What is the maximum token length for the 13B model during training in the Mdata creation procedure?\n",
      "Len of text chunks 1058\n",
      "seed question What is the proposed framework for retrieval augmentation in this paper?\n",
      "{'reason': 'The specific paper being referred to is not mentioned in the question.', 'verdict': 'No'}\n",
      "rewritten question What is the proposed framework for retrieval augmentation in the Rewrite-Retrieve-Read paper?\n",
      "Len of text chunks 537\n",
      "seed question What is the average score of LLaMA2-70B-32k-ret in the QM, QASP, NQA, QLTY, MSQ, HQA, and MFQA datasets?\n",
      "{'reason': 'The specific context or source of these acronyms and the LLaMA2-70B-32k-ret is not provided.', 'verdict': 'No'}\n",
      "rewritten question What is the average score of LLaMA2-70B-32k-ret in the QM, QASP, NQA, QLTY, MSQ, HQA, and MFQA datasets in the given context?\n",
      "Len of text chunks 996\n",
      "seed question What is the noise level for label correctness in the generated data for IMDb and MNLI in CORE?\n",
      "{'reason': 'The specific paper or study that generated the data for IMDb and MNLI in CORE is not mentioned in the question.', 'verdict': 'No'}\n",
      "rewritten question What is the noise level for label correctness in the generated data for IMDb and MNLI in CORE?\n",
      "Len of text chunks 914\n",
      "seed question What is the purpose of RAG-assisted Representative Vector Summarization (RVS)?\n",
      "{'reason': 'The question is clear without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 10/25 [03:34<05:03, 20.26s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 675\n",
      "seed question What is the purpose of the query rewriting step in the Rewrite-Retrieve-Read pipeline?\n",
      "{'reason': 'The question is clear without additional context', 'verdict': 'Yes'}\n",
      "{'reason': 'The question is clear without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                    | 15/25 [04:09<02:17, 13.79s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 700\n",
      "seed question What is the accuracy of the reward model in predicting GPT-4 judgments?\n",
      "{'reason': 'The specific reward model being referred to is not mentioned in the question.', 'verdict': 'No'}\n",
      "rewritten question What is the accuracy of the reward model in predicting GPT-4 judgments in the study mentioned in the context?\n",
      "Len of text chunks 722\n",
      "seed question What are the F1 bands for the large models in the cluster with golden evidence?\n",
      "{'reason': 'The specific models and cluster being referred to are not mentioned in the question.', 'verdict': 'No'}\n",
      "rewritten question What are the F1 bands for the large models in the cluster with golden evidence?\n",
      "Len of text chunks 957\n",
      "seed question What is the average document length for the QASP dataset?\n",
      "{'reason': 'The question is clear without additional context', 'verdict': 'Yes'}\n",
      "{'reason': 'The specific table being referred to is not mentioned in the question.', 'verdict': 'No'}\n",
      "Len of text chunks 1004\n",
      "seed question What are the experimental settings for training and inference in the SELF-RAG model?\n",
      "{'reason': 'The specific paper or source that discusses the SELF-RAG model is not mentioned in the question.', 'verdict': 'No'}\n",
      "rewritten question What are the experimental settings for training and inference in the SELF-RAG model described in the context?\n",
      "Len of text chunks 900\n",
      "Len of text chunks 646\n",
      "seed question What is the proposed framework in the paper?\n",
      "{'reason': 'The specific paper being referred to is not mentioned in the question.', 'verdict': 'No'}\n",
      "rewritten question What is the proposed framework in the RETRIEVAL-GENERATION SYNERGY AUGMENTED LARGE LANGUAGE MODELS paper?\n",
      "Len of text chunks 740\n",
      "seed question What is the purpose of including tokenized Wikipedia articles in the task-specific corpus for MNLI?\n",
      "{'reason': 'The question is clear without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š        | 21/25 [05:21<00:51, 12.92s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 517\n",
      "seed question What are the two solutions being studied in this work and how do they compare in terms of performance and computation?\n",
      "{'reason': 'The specific work being referred to is not mentioned in the question.', 'verdict': 'No'}\n",
      "rewritten question What are the two solutions being studied in the work \"Retrieval Meets Long Context Large Language Models\" and how do they compare in terms of performance and computation?\n",
      "Len of text chunks 684\n",
      "Len of text chunks 708\n",
      "seed question What is the purpose of the query rewriting step in the Rewrite-Retrieve-Read pipeline?\n",
      "{'reason': 'The question is clear without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "28it [05:45,  8.85s/it]                                                                 \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 937\n",
      "seed question What is the hit ratio for the LLM rewriter on AmbigNQ?\n",
      "{'reason': 'The question is clear without additional context', 'verdict': 'Yes'}\n",
      "{'reason': \"The specific retrieval method and 'AmbigNQ' are not clearly defined or commonly known.\", 'verdict': 'No'}\n",
      "Len of text chunks 558\n",
      "seed question What is the purpose of using native dialog prompts for response generation?\n",
      "{'reason': 'The question is clear without additional context', 'verdict': 'Yes'}\n",
      "{'reason': \"The specific context or study regarding 'native dialog prompts' and 'PaLM's response generation' is not provided in the question.\", 'verdict': 'No'}\n",
      "Len of text chunks 682\n",
      "Len of text chunks 723\n",
      "Len of text chunks 898\n",
      "seed question What is the average QM score for the LLaMA2-70B model with a sequence length of 32k?\n",
      "{'reason': 'The specific context or field of the LLaMA2-70B model is not mentioned in the question.', 'verdict': 'No'}\n",
      "rewritten question What is the average QM score for the LLaMA2-70B model with a sequence length of 32k in the study?\n",
      "Len of text chunks 934\n",
      "seed question What are the four types of reflection tokens used in SELF-RAG?\n",
      "{'reason': 'The question is clear without additional context', 'verdict': 'Yes'}\n",
      "{'reason': \"The specific context or meaning of 'SELF-RAG' and 'reflection token' is not provided in the question.\", 'verdict': 'No'}\n",
      "Len of text chunks 769\n",
      "seed question What is the evaluation metric used for multiple-choice QA in the experiments?\n",
      "{'reason': 'The specific experiments being referred to are not mentioned in the question.', 'verdict': 'No'}\n",
      "rewritten question What is the evaluation metric used for multiple-choice QA in the experiments conducted in the paper?\n",
      "Len of text chunks 605\n",
      "Len of text chunks 537\n",
      "seed question What is the personalized accuracy for News Categorization in LaMP-2 for ð‘˜=1?\n",
      "{'reason': \"The specific context or source of 'LaMP-2' and 'personalized accuracy' is not provided in the question.\", 'verdict': 'No'}\n",
      "rewritten question What is the personalized accuracy for News Categorization in LaMP-2 for ð‘˜=1 in the paper 'Integrating Summarization and Retrieval for Enhanced Personalization via Large Language Models Conferenceâ€™17, July 2017, Washington, DC, USA'?\n",
      "Len of text chunks 506\n",
      "Len of text chunks 1091\n",
      "Len of text chunks 821\n",
      "seed question What is the purpose of RETA-LLM?\n",
      "{'reason': 'The specific meaning or function of RETA-LLM is not provided in the question.', 'verdict': 'No'}\n",
      "rewritten question What is the purpose of RETA-LLM in the paper?\n",
      "Len of text chunks 943\n",
      "Len of text chunks 842\n",
      "seed question What is the prompt schema called for query augmentation in large-scale retrieval?\n",
      "{'reason': 'The specific context or system being referred to is not mentioned in the question.', 'verdict': 'No'}\n",
      "rewritten question What is the prompt schema called for query augmentation in large-scale retrieval in the paper?\n",
      "Len of text chunks 668\n",
      "seed question What is the Personalized Accuracy of the GPT-3.5 Summ. model for Task LaMP-1 with k=1?\n",
      "{'reason': 'The question is clear without additional context', 'verdict': 'Yes'}\n",
      "{'reason': 'The question is clear without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "36it [08:30, 13.69s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 392\n",
      "Len of text chunks 918\n",
      "seed question What datasets were used in the study?\n",
      "{'reason': 'The specific study being referred to is not mentioned in the question.', 'verdict': 'No'}\n",
      "rewritten question What datasets were used for the zero-shot evaluations in the study on integrating long context information for generative QA or summarization tasks via retrieval or LLM's own self-attention mechanism?\n",
      "Len of text chunks 504\n",
      "seed question What is the purpose of RAG-assisted Representative Vector Summarization (RVS)?\n",
      "{'reason': 'The question is clear without additional context', 'verdict': 'Yes'}\n",
      "{'reason': 'The question is clear without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "45it [09:17, 10.31s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 520\n",
      "Len of text chunks 551\n",
      "seed question What are the impacts of different retrievers on the performance of LLaMA2-70B?\n",
      "{'reason': \"The specific context or field of 'LLaMA2-70B' and 'retrievers' is not mentioned in the question.\", 'verdict': 'No'}\n",
      "rewritten question What are the impacts of different retrievers on the performance of LLaMA2-70B?\n",
      "Len of text chunks 742\n",
      "seed question What is the purpose of the crowd-sourcing study mentioned in Section 5.3?\n",
      "{'reason': 'The specific study or document containing Section 5.3 is not mentioned in the question.', 'verdict': 'No'}\n",
      "rewritten question What is the purpose of the crowd-sourcing study mentioned in Section 5.3 of the paper?\n",
      "Len of text chunks 882\n",
      "seed question What is the MAP value for LameR in the TREC Deep Learning 2019 dataset?\n",
      "{'reason': 'The question is clear without additional context', 'verdict': 'Yes'}\n",
      "{'reason': 'The question is clear without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "55it [10:06,  8.23s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 485\n",
      "Len of text chunks 878\n",
      "Len of text chunks 729\n",
      "Len of text chunks 731\n",
      "seed question What is the nDCG@10 achieved by LameR on DL19?\n",
      "{'reason': \"The specific context or meaning of 'nDCG@10', 'LameR', and 'DL19' is not provided in the question.\", 'verdict': 'No'}\n",
      "rewritten question What is the nDCG@10 achieved by LameR on DL19?\n",
      "Len of text chunks 1056\n",
      "Len of text chunks 805\n",
      "seed question What tasks are the CORE counterfactuals generated for?\n",
      "{'reason': \"The specific context or subject related to 'CORE counterfactuals' is not mentioned in the question.\", 'verdict': 'No'}\n",
      "rewritten question What tasks are the CORE counterfactuals generated for in the study?\n",
      "Len of text chunks 939\n",
      "seed question What are the demerits of using a self-supervised retriever in combination with a versatile LLM in the retrieval pipeline?\n",
      "{'reason': 'The question is clear without additional context', 'verdict': 'Yes'}\n",
      "{'reason': \"The specific pilot experiments and the terms 'self-supervised retriever' and 'versatile LLM' are not explained in the question.\", 'verdict': 'No'}\n",
      "Len of text chunks 1108\n",
      "Len of text chunks 663\n",
      "seed question How does query rewriting impact performance in open-domain QA?\n",
      "{'reason': 'The question is clear without additional context', 'verdict': 'Yes'}\n",
      "{'reason': 'The question is clear without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "66it [11:31,  8.05s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 1075\n",
      "seed question What are the four types of reflection tokens used in SELF-RAG?\n",
      "{'reason': 'The question is clear without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "78it [11:56,  5.83s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 717\n",
      "Len of text chunks 622\n",
      "Len of text chunks 950\n",
      "seed question What is the maximum new token length set for closed-set tasks in the ALCE paper?\n",
      "{'reason': 'The specific paper being referred to is not mentioned in the question.', 'verdict': 'No'}\n",
      "rewritten question What is the maximum new token length set for closed-set tasks in the ALCE paper?\n",
      "Len of text chunks 735\n",
      "seed question What is the sensibleness prompt alignment in Table 1?\n",
      "{'reason': 'The specific table or context is not mentioned in the question.', 'verdict': 'No'}\n",
      "rewritten question What is the sensibleness prompt alignment in Table 1 of the paper?\n",
      "Len of text chunks 863\n",
      "Len of text chunks 413\n",
      "Len of text chunks 729\n",
      "Len of text chunks 754\n",
      "Len of text chunks 602\n",
      "seed question What is the difference between global tradeoff and local tradeoff in the context of model re-ranking?\n",
      "{'reason': \"The terms 'global tradeoff' and 'local tradeoff' in model re-ranking are not universally understood and may require specific context or definition.\", 'verdict': 'No'}\n",
      "rewritten question What is the difference between global tradeoff and local tradeoff in the context of model re-ranking?\n",
      "Len of text chunks 1001\n",
      "seed question What is the methodology of the Rewrite-Retrieve-Read pipeline?\n",
      "{'reason': 'The question is clear without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "91it [13:08,  5.72s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 894\n",
      "seed question What is the impact of CF-DPR counterfactuals on encouraging humans to make diverse counterfactual perturbations to text?\n",
      "{'reason': \"The specific context or study involving 'CF-DPR counterfactuals' is not provided in the question.\", 'verdict': 'No'}\n",
      "rewritten question What is the impact of CF-DPR counterfactuals on encouraging humans to make diverse counterfactual perturbations to text in the CORE framework?\n",
      "Len of text chunks 809\n",
      "Len of text chunks 293\n",
      "seed question What is the purpose of the CORE framework?\n",
      "{'reason': 'The question is clear without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "105it [13:37,  4.41s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 685\n",
      "Len of text chunks 662\n",
      "seed question What are some potential constraints that could result in local tradeoffs in the context of model decoding strategies?\n",
      "{'reason': 'The question is clear without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "120it [13:58,  3.35s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 431\n",
      "Len of text chunks 539\n",
      "seed question What is the purpose of re-ranking or input-level ensembling in the experiments?\n",
      "{'reason': 'The specific experiments being referred to are not mentioned in the question.', 'verdict': 'No'}\n",
      "rewritten question What is the purpose of re-ranking or input-level ensembling in the experiments described in the context?\n",
      "Len of text chunks 0\n",
      "Len of text chunks 818\n",
      "seed question What are the optional modules provided by RETA-LLM?\n",
      "{'reason': 'The specific meaning or function of RETA-LLM is not provided in the question.', 'verdict': 'No'}\n",
      "rewritten question What are the optional modules provided by RETA-LLM?\n",
      "Len of text chunks 622\n",
      "Len of text chunks 752\n",
      "Len of text chunks 673\n",
      "seed question What is the score improvement for LLaMA2-70B in HotpotQA when the sequence length increases from 4k to 16k?\n",
      "{'reason': 'The specific context or study where LLaMA2-70B was used in HotpotQA is not mentioned in the question.', 'verdict': 'No'}\n",
      "rewritten question What is the score improvement for LLaMA2-70B in HotpotQA when the sequence length increases from 4k to 16k?\n",
      "Len of text chunks 769\n",
      "seed question What is the sentiment of the review for the movie \"Ã„ Bug's Life\"?\n",
      "{'reason': 'The specific review being referred to is not mentioned in the question.', 'verdict': 'No'}\n",
      "rewritten question What is the sentiment of the review for the movie \"Ã„ Bug's Life\"?\n",
      "Len of text chunks 1057\n",
      "seed question What is the accuracy of the PaLM model for Sensibleness prompt alignment in the pilot data?\n",
      "{'reason': \"The specific context or source of the 'PaLM model' and 'Sensibleness prompt alignment' is not mentioned in the question.\", 'verdict': 'No'}\n",
      "rewritten question What is the accuracy of the PaLM model for Sensibleness prompt alignment in the pilot data?\n",
      "Len of text chunks 910\n",
      "Len of text chunks 824\n",
      "seed question What are the four distinct clusters in Figure 1 and what types of evidence and dialog history are present in each cluster?\n",
      "{'reason': 'The specific Figure 1 being referred to is not mentioned in the question.', 'verdict': 'No'}\n",
      "rewritten question What are the four distinct clusters in Figure 1 and what types of evidence and dialog history are present in each cluster in the full grid plot?\n",
      "Len of text chunks 886\n",
      "seed question What dataset is used for the experiments in this study?\n",
      "{'reason': 'The specific study being referred to is not mentioned in the question.', 'verdict': 'No'}\n",
      "rewritten question What dataset is used for the experiments in the QReCC dataset?\n",
      "Len of text chunks 729\n",
      "seed question What is the purpose of the CF-DPR framework in the given context?\n",
      "{'reason': 'The specific context for the CF-DPR framework is not provided in the question.', 'verdict': 'No'}\n",
      "rewritten question What is the purpose of the CF-DPR framework in the Counterfactual Retrieval Editing paper?\n",
      "Len of text chunks 335\n",
      "seed question What is the proposed framework for obtaining effective documents in large language models?\n",
      "{'reason': 'The specific paper or study proposing a framework is not mentioned in the question.', 'verdict': 'No'}\n",
      "rewritten question What is the proposed framework for obtaining effective documents in the paper \"RETRIEVAL-GENERATION SYNERGY AUGMENTED LARGE LANGUAGE MODELS\"?\n",
      "Len of text chunks 526\n",
      "Len of text chunks 505\n",
      "seed question What is the impact of limited context size on the auto-metrics for examples with \"non-evidence\" and \"golden\" evidence?\n",
      "{'reason': \"The specific study or paper that discusses 'non-evidence' and 'golden' evidence in relation to context size and auto-metrics is not mentioned.\", 'verdict': 'No'}\n",
      "rewritten question What is the impact of limited context size on the auto-metrics for examples with \"non-evidence\" and \"golden\" evidence in the given context?\n",
      "Len of text chunks 622\n",
      "seed question Which model outperforms all non-proprietary models in citation precision and recall on the ASQA task?\n",
      "{'reason': 'The specific model being referred to is not mentioned in the question.', 'verdict': 'No'}\n",
      "rewritten question Which model outperforms all non-proprietary models in citation precision and recall on the ASQA task according to the table?\n",
      "Len of text chunks 798\n",
      "seed question What is the context length of the Vicuna model used for generating summaries in the experiments?\n",
      "{'reason': 'The specific experiments being referred to are not mentioned in the question.', 'verdict': 'No'}\n",
      "rewritten question What is the context length of the Vicuna model used for generating summaries in the experiments conducted in the paper 'Exploring Zero-Shot Learning in Neural Networks'?\n",
      "Len of text chunks 886\n",
      "seed question What is the nDCG@10 score for LameR with GPT-4 in the DL20 dataset?\n",
      "{'reason': \"The specific context or field related to the terms 'nDCG@10', 'LameR', 'GPT-4', and 'DL20 dataset' is not provided.\", 'verdict': 'No'}\n",
      "rewritten question What is the nDCG@10 score for LameR with GPT-4 in the DL20 dataset?\n",
      "Len of text chunks 876\n",
      "seed question What is the hit ratio for the LLM rewriter in the AmbigNQ dataset?\n",
      "{'reason': \"The specific context or study involving the 'LLM rewriter' and 'AmbigNQ dataset' is not mentioned in the question.\", 'verdict': 'No'}\n",
      "rewritten question What is the hit ratio for the LLM rewriter in the AmbigNQ dataset?\n",
      "Len of text chunks 527\n",
      "Len of text chunks 1085\n",
      "Len of text chunks 1015\n",
      "seed question What is the effect of presence or absence of evidence and/or dialog history on the value of the tradeoff?\n",
      "{'reason': 'The specific context or study related to the tradeoff is not mentioned in the question.', 'verdict': 'No'}\n",
      "rewritten question What is the effect of presence or absence of evidence and/or dialog history on the value of the tradeoff in the study?\n",
      "Len of text chunks 777\n",
      "seed question What is the sentiment of the movie review in the IMDb dataset?\n",
      "{'reason': 'The specific movie review being referred to is not mentioned in the question.', 'verdict': 'No'}\n",
      "rewritten question What is the sentiment of the movie review in the IMDb dataset in the paper 'Exploring Zero-Shot Learning in Neural Networks'?\n",
      "Len of text chunks 743\n",
      "seed question What is the performance improvement of LameR SimLM compared to Q2D SimLM on DL19 and DL20?\n",
      "{'reason': 'The specific context or field of these acronyms (LameR SimLM, Q2D SimLM, DL19, DL20) is not provided.', 'verdict': 'No'}\n",
      "rewritten question What is the performance improvement of LameR SimLM compared to Q2D SimLM on DL19 and DL20 according to the paper 'Exploring Zero-Shot Learning in Neural Networks'?\n",
      "Len of text chunks 601\n",
      "seed question What methods are used in the workflow implemented in docGPT for AI in Medicine?\n",
      "{'reason': 'The specific workflow or methods used by docGPT for AI in Medicine are not common knowledge and would require additional context or explanation.', 'verdict': 'No'}\n",
      "rewritten question What methods are used in the workflow implemented in docGPT for AI in Medicine?\n",
      "Len of text chunks 340\n",
      "seed question What is the focus of the study mentioned in the given context?\n",
      "{'reason': 'The specific study or context is not mentioned in the question.', 'verdict': 'No'}\n",
      "rewritten question What is the focus of the study mentioned in the RETRIEVAL MEETS LONG CONTEXT LARGE LANGUAGE MODELS paper?\n",
      "Len of text chunks 577\n",
      "seed question What is the relationship between fluency and attribution in LLMs prompted with retrieved evidence in knowledge-heavy dialog settings?\n",
      "{'reason': 'The terms used in the question are specific to a certain field of study and may not be understood without additional context or explanation.', 'verdict': 'No'}\n",
      "rewritten question What is the relationship between fluency and attribution in LLMs prompted with retrieved evidence in knowledge-heavy dialog settings according to the paper 'Characterizing Attribution and Fluency Tradeoffs for Retrieval-Augmented Large Language Models'?\n",
      "Len of text chunks 531\n",
      "Len of text chunks 509\n",
      "Len of text chunks 621\n",
      "Len of text chunks 465\n",
      "Len of text chunks 990\n",
      "seed question What is the date of birth of Emilie HeghArntzen's mother?\n",
      "{'reason': \"The specific person's mother's date of birth is not commonly known information and would require additional context or research.\", 'verdict': 'No'}\n",
      "rewritten question What is the date of birth of Emilie HeghArntzen's mother according to the information provided in the context?\n",
      "Len of text chunks 769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed question What is the answer recall of generated documents at iteration 3 for the HotpotQA dataset with ITRG (refresh)?\n",
      "{'reason': 'The question is clear without additional context', 'verdict': 'Yes'}\n",
      "{'reason': 'The specific study or experiment being referred to is not mentioned in the question.', 'verdict': 'No'}\n",
      "Len of text chunks 927\n",
      "seed question What are the optional modules provided by RETA-LLM?\n",
      "{'reason': 'The specific context or definition of RETA-LLM is not provided in the question.', 'verdict': 'No'}\n",
      "rewritten question What are the optional modules provided by RETA-LLM in the RETA-LLM toolkit?\n",
      "Len of text chunks 615\n",
      "seed question What is the implementation process for the rewriter in the frozen pipeline?\n",
      "{'reason': \"The specific 'rewriter' and 'frozen pipeline' being referred to are not mentioned in the question.\", 'verdict': 'No'}\n",
      "rewritten question What is the implementation process for the rewriter in the frozen pipeline in the context of the expectation of the reward?\n",
      "Len of text chunks 746\n",
      "Len of text chunks 954\n",
      "seed question What is the agreement percentage between the critic model and GPT-4-based predictions for reflection token categories?\n",
      "{'reason': 'The specific study or experiment involving the critic model and GPT-4-based predictions is not mentioned in the question.', 'verdict': 'No'}\n",
      "rewritten question What is the agreement percentage between the critic model and GPT-4-based predictions for reflection token categories in the paper?\n",
      "Len of text chunks 0\n",
      "Len of text chunks 997\n",
      "seed question What is the number of generated answers in the proposed retrieval method?\n",
      "{'reason': 'The specific retrieval method being referred to is not mentioned in the question.', 'verdict': 'No'}\n",
      "rewritten question What is the number of generated answers in the proposed retrieval method described in the context?\n",
      "Len of text chunks 951\n",
      "seed question What is the purpose of Self-Reflective Retrieval-Augmented Generation (SELF-RAG) in improving an LM's generation quality and factuality?\n",
      "{'reason': 'The question is clear without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "136it [19:22,  9.28s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 783\n",
      "Len of text chunks 769\n",
      "seed question What is the premise and hypothesis in the MNLI task?\n",
      "{'reason': 'The question is clear without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "153it [20:05,  6.92s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 737\n",
      "Len of text chunks 916\n",
      "seed question What are the evaluation metrics for LaMP-4Task?\n",
      "{'reason': \"The specific context or field of 'LaMP-4Task' is not mentioned in the question.\", 'verdict': 'No'}\n",
      "rewritten question What are the evaluation metrics for LaMP-4Task in the 'Integrating Summarization and Retrieval for Enhanced Personalization via Large Language Models Conferenceâ€™17' paper?\n",
      "Len of text chunks 695\n",
      "seed question What is the workflow implemented in docGPT for summarization?\n",
      "{'reason': 'The specific workflow or process being referred to is not clear in the question.', 'verdict': 'No'}\n",
      "rewritten question What is the workflow implemented in docGPT for summarization in the RVS program?\n",
      "Len of text chunks 623\n",
      "seed question What is the impact of human annotators being primed with retrieval on the use of trivial heuristics in counterfactual edits?\n",
      "{'reason': 'The question is clear without additional context', 'verdict': 'Yes'}\n",
      "{'reason': 'The question is clear without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "171it [20:59,  5.57s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 0\n",
      "Len of text chunks 883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sequence item 0: expected str instance, ReadTimeoutError found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed question What is the goal of the proposed method of integrating summarization and retrieval in the context of language model personalization?\n",
      "{'reason': 'The specific method or paper being referred to is not mentioned in the question.', 'verdict': 'No'}\n",
      "rewritten question What is the goal of the proposed method of integrating summarization and retrieval in the context of language model personalization in the Conference'17 paper?\n",
      "Len of text chunks 1028\n",
      "seed question What is the purpose of the ablation studies conducted on the three datasets?\n",
      "{'reason': 'The specific ablation studies and datasets being referred to are not mentioned in the question.', 'verdict': 'No'}\n",
      "rewritten question What is the purpose of the ablation studies conducted on the three datasets in the context of the paper 'Exploring Zero-Shot Learning in Neural Networks'?\n",
      "Len of text chunks 400\n",
      "seed question What is the purpose of RAG-assisted Representative Vector Summarization (RVS)?\n",
      "{'reason': 'The question is clear without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "190it [21:31,  4.24s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 1034\n",
      "seed question What dataset is used for the experiments in this work?\n",
      "{'reason': 'The specific work being referred to is not mentioned in the question.', 'verdict': 'No'}\n",
      "rewritten question What dataset is used for the experiments in the QReCC dataset?\n",
      "Len of text chunks 727\n",
      "Len of text chunks 929\n",
      "seed question What are the two types of computer memory?\n",
      "{'reason': 'The question is clear without additional context', 'verdict': 'Yes'}\n",
      "{'reason': 'The question is clear without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "210it [22:03,  3.34s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 688\n",
      "Len of text chunks 1017\n",
      "seed question What is the effect of changing the weighting term for ISSUP on the citation precision of the models?\n",
      "{'reason': \"The specific models and the term 'ISSUP' are not explained in the question.\", 'verdict': 'No'}\n",
      "rewritten question What is the effect of changing the weighting term for ISSUP on the citation precision of the models in the Preprint paper?\n",
      "Len of text chunks 1028\n",
      "seed question What is the purpose of the SELF-RAG framework?\n",
      "{'reason': 'The question is clear without additional context', 'verdict': 'Yes'}\n",
      "{'reason': \"The terms 'SELF-RAG' and 'LLM' are not explained or defined in the question.\", 'verdict': 'No'}\n",
      "Len of text chunks 932\n",
      "seed question What is the impact of CF-DPR counterfactuals on encouraging humans to make diverse counterfactual perturbations to text?\n",
      "{'reason': \"The specific context or study involving 'CF-DPR counterfactuals' is not mentioned in the question.\", 'verdict': 'No'}\n",
      "rewritten question What is the impact of CF-DPR counterfactuals on encouraging humans to make diverse counterfactual perturbations to text in the CORE framework?\n",
      "Len of text chunks 615\n",
      "seed question What is the purpose of the CORE framework?\n",
      "{'reason': 'The question is clear without additional context', 'verdict': 'Yes'}\n",
      "{'reason': 'The question is clear without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "231it [23:15,  3.37s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 904\n",
      "seed question What is the nDCG@10 score for LameR GPT-4 in DL20?\n",
      "{'reason': \"The specific context or meaning of 'DL20' and 'LameR GPT-4' is not provided in the question.\", 'verdict': 'No'}\n",
      "rewritten question What is the nDCG@10 score for LameR GPT-4 in DL20?\n",
      "Len of text chunks 615\n",
      "seed question What is the role of the President of India according to India's constitution?\n",
      "{'reason': 'The question is clear without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "253it [23:34,  2.54s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 899\n",
      "seed question What are some potential constraints that could result in local tradeoff in the context of model re-ranking?\n",
      "{'reason': 'The specific model or context for re-ranking is not mentioned in the question.', 'verdict': 'No'}\n",
      "rewritten question What are some potential constraints that could result in local tradeoff in the context of model re-ranking in the study?\n",
      "Len of text chunks 331\n",
      "seed question What is the focus of this work on query rewriting for retrieval-augmented large language models?\n",
      "{'reason': 'The specific work being referred to is not mentioned in the question.', 'verdict': 'No'}\n",
      "rewritten question What is the focus of the work on query rewriting for retrieval-augmented large language models in the paper \"Query Rewriting for Retrieval-Augmented Large Language Models\"?\n",
      "Len of text chunks 786\n",
      "seed question What are the references mentioned in the document?\n",
      "{'reason': 'The specific document being referred to is not mentioned in the question.', 'verdict': 'No'}\n",
      "rewritten question What are the references mentioned in the document \"Exploring Zero-Shot Learning in Neural Networks\"?\n",
      "Len of text chunks 963\n",
      "seed question What is the agreement percentage between the authors' assessments and GPT-4 predictions for usefulness?\n",
      "{'reason': \"The specific study or context in which the authors' assessments and GPT-4 predictions were made is not mentioned in the question.\", 'verdict': 'No'}\n",
      "rewritten question What is the agreement percentage between the authors' assessments and GPT-4 predictions for usefulness in the Preprint?\n",
      "Len of text chunks 0\n",
      "Len of text chunks 733\n",
      "Len of text chunks 291\n",
      "Len of text chunks 378\n",
      "seed question How does LLaMA2-70B-32k with retrieval compare to GPT-3.5-turbo-16k and Davinci-003 in terms of average accuracy over seven datasets?\n",
      "{'reason': \"The specific context or field of these terms is not clear. They seem to refer to specific models or systems, but without further context, it's unclear what they are.\", 'verdict': 'No'}\n",
      "rewritten question How does LLaMA2-70B-32k with retrieval compare to GPT-3.5-turbo-16k and Davinci-003 in terms of average accuracy over seven datasets in the study 'Exploring Zero-Shot Learning in Neural Networks'?\n",
      "Len of text chunks 646\n",
      "seed question What is the best performance among non-proprietary models for the ASQA task?\n",
      "{'reason': \"The specific ASQA task or the criteria for 'best performance' is not mentioned in the question.\", 'verdict': 'No'}\n",
      "rewritten question What is the best performance among non-proprietary models for the ASQA task in the paper?\n",
      "Len of text chunks 701\n",
      "Len of text chunks 1055\n",
      "Len of text chunks 881\n",
      "seed question What is the core idea behind the BM25 method for large-scale retrieval?\n",
      "{'reason': 'The question is clear without additional context', 'verdict': 'Yes'}\n",
      "{'reason': 'The specific method or system being referred to is not mentioned in the question.', 'verdict': 'No'}\n",
      "Len of text chunks 636\n",
      "seed question What is the exact match score of ITRG (refresh) on the 2WikiMultiHopQA dataset in the 5-shot setting?\n",
      "{'reason': 'The question is clear without additional context', 'verdict': 'Yes'}\n",
      "{'reason': 'The specific match or competition being referred to is not mentioned in the question.', 'verdict': 'No'}\n",
      "Len of text chunks 614\n",
      "seed question Which model outperforms all non-proprietary models in citation precision and recall on the ASQA task?\n",
      "{'reason': 'The specific model being referred to is not mentioned in the question.', 'verdict': 'No'}\n",
      "rewritten question Which model outperforms all non-proprietary models in citation precision and recall on the ASQA task according to the results reported in the paper?\n",
      "Len of text chunks 273\n",
      "seed question Which model enjoys a larger benefit of incorporating context through retrieval, GPT-43B or LLaMA2-70B?\n",
      "{'reason': 'The specific context or study where these models were compared is not provided.', 'verdict': 'No'}\n",
      "rewritten question Which model enjoys a larger benefit of incorporating context through retrieval, GPT-43B or LLaMA2-70B?\n",
      "Len of text chunks 471\n",
      "seed question Can we observe systematic local tradeoffs for different prompt structures within the same-color experiments in Figure 3?\n",
      "{'reason': 'The specific study or paper that contains Figure 3 is not mentioned in the question.', 'verdict': 'No'}\n",
      "rewritten question Can we observe systematic local tradeoffs for different prompt structures within the same-color experiments in Figure 3 of the study?\n",
      "Len of text chunks 374\n",
      "Len of text chunks 856\n",
      "seed question What is the date of birth of Emilie HeghArntzen's mother?\n",
      "{'reason': \"The specific person's mother's date of birth is not commonly known information and would require specific context or knowledge.\", 'verdict': 'No'}\n",
      "rewritten question What is the date of birth of Emilie HeghArntzen's mother according to the information provided in the context?\n",
      "Len of text chunks 735\n",
      "Len of text chunks 218\n",
      "Len of text chunks 834\n",
      "seed question What are the contributions of the study on retrieval-augmentation and long context extension of LLMs?\n",
      "{'reason': 'The specific study being referred to is not mentioned in the question.', 'verdict': 'No'}\n",
      "rewritten question What are the contributions of the study on retrieval-augmentation and long context extension of LLMs in the paper \"RETRIEVAL MEETS LONG CONTEXT LARGE LANGUAGE MODELS\" by Peng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu, Zihan Liu, Sandeep Subramanian, Evelina Bakhturina, Mohammad Shoeybi, and Bryan Catanzaro?\n",
      "Len of text chunks 0\n",
      "Len of text chunks 398\n",
      "seed question What is the purpose of the t-SNE visualization in AI for Medicine?\n",
      "{'reason': 'The specific context or application of t-SNE visualization in AI for Medicine is not clear.', 'verdict': 'No'}\n",
      "rewritten question What is the purpose of the t-SNE visualization in the AI for Medicine paper?\n",
      "Len of text chunks 833\n",
      "seed question What are the contributions of this work in the study of long context language models?\n",
      "{'reason': 'The specific work being referred to is not mentioned in the question.', 'verdict': 'No'}\n",
      "rewritten question What are the contributions of this work in the study of long context language models in the paper 'Exploring Zero-Shot Learning in Neural Networks'?\n",
      "Len of text chunks 751\n",
      "Len of text chunks 943\n",
      "seed question What is the top 1 accuracy achieved by the CF-DPR model for the MNLI task?\n",
      "{'reason': 'The question is clear without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "276it [26:36,  4.34s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 998\n",
      "seed question What are the baselines used in the experiments?\n",
      "{'reason': 'The specific experiments being referred to are not mentioned in the question.', 'verdict': 'No'}\n",
      "rewritten question What are the baselines used in the experiments conducted in the Preprint?\n",
      "Len of text chunks 910\n",
      "Len of text chunks 927\n",
      "seed question What is the accuracy of reward predictions for the base LM Llama2-7B in terms of ISUSE?\n",
      "{'reason': \"The terms 'base LM Llama2-7B' and 'ISUSE' are not commonly known or understood without specific context or explanation.\", 'verdict': 'No'}\n",
      "rewritten question What is the accuracy of reward predictions for the base LM Llama2-7B in terms of ISUSE in the preprint?\n",
      "Len of text chunks 505\n",
      "seed question What is the impact of limited context size on the number of books loaned in January?\n",
      "{'reason': \"The question is not clear without additional context. It's not clear how 'limited context size' relates to 'number of books loaned in January'.\", 'verdict': 'No'}\n",
      "rewritten question What is the impact of limited context size on the number of books loaned in January?\n",
      "Len of text chunks 700\n",
      "Len of text chunks 533\n",
      "Len of text chunks 723\n",
      "seed question What is the purpose of conducting a pilot human evaluation in the context described?\n",
      "{'reason': 'The specific context for conducting a pilot human evaluation is not mentioned in the question.', 'verdict': 'No'}\n",
      "rewritten question What is the purpose of conducting a pilot human evaluation in the context described in the Meena paper?\n",
      "Len of text chunks 835\n",
      "seed question What are the main points discussed in the section \"7.6 Discussion\"?\n",
      "{'reason': \"The specific text or document that contains the section '7.6 Discussion' is not mentioned in the question.\", 'verdict': 'No'}\n",
      "rewritten question What are the main points discussed in the section \"7.6 Discussion\" of the paper?\n",
      "Len of text chunks 960\n",
      "seed question What is the purpose of RETA-LLM?\n",
      "{'reason': 'The specific meaning or function of RETA-LLM is not provided in the question.', 'verdict': 'No'}\n",
      "rewritten question What is the purpose of RETA-LLM in the paper?\n",
      "Len of text chunks 917\n",
      "Len of text chunks 1089\n",
      "seed question What is the problem formulation in LaMP?\n",
      "{'reason': \"The specific context or meaning of 'LaMP' is not provided in the question.\", 'verdict': 'No'}\n",
      "rewritten question What is the problem formulation in LaMP?\n",
      "Len of text chunks 539\n",
      "seed question What is the purpose of the extractive summarization step in the retrieval workflow of docGPT?\n",
      "{'reason': 'The question is clear without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_size = 25\n",
    "testset = testsetgenerator.generate(documents, test_size=test_size)\n",
    "test_df = testset.to_pandas()\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d58feb8-8604-4854-9d1d-7aa27495f0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv(\"arxiv_retrieval_v1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a03897e",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "- Conditional question evol is working\n",
    "- reasoning/multi context are not working as expected\n",
    "- Almost all questions are closed endeded \n",
    "- Almost all questions start with \"What\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453e52f7",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "144547a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.file.markdown_reader import MarkdownReader\n",
    "from llama_index.schema import Document\n",
    "from typing import List, Dict, Optional\n",
    "from pathlib import Path\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1e30ee88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RagasMdReader(MarkdownReader):\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_file_metadata(path):\n",
    "        \n",
    "        return {\"filename\":os.path.basename(path),\n",
    "                \"dirname\":os.path.dirname(path)}\n",
    "        \n",
    "    \n",
    "    def get_local_metadata(self, text):\n",
    "        \n",
    "#         hyperlinks = {}\n",
    "#         pattern = r\"\\[(.*?)\\]\\((?!https)(.*?)\\)\"\n",
    "#         for k,v in re.findall(pattern, text):\n",
    "#             if 'http' not in v and len(k.split())<=3 and v.split('/')[-1].startswith('#'):\n",
    "#                 hyperlinks[k] = v\n",
    "#         return hyperlinks\n",
    "        return_dict = {}\n",
    "        pattern = r'---\\s*title:\\s*(.+?)(?:\\s*description:\\s*\"(.*?)\")?\\s*---'\n",
    "        match = re.findall(pattern, text)\n",
    "        if match:\n",
    "            title,desc = match[0]\n",
    "            return_dict['title'] = title\n",
    "            return_dict['description'] = desc if desc else None\n",
    "            \n",
    "        return return_dict\n",
    "\n",
    "        \n",
    "    \n",
    "        \n",
    "    \n",
    "    def load_data(\n",
    "        self, file: Path, extra_info: Optional[Dict] = None\n",
    "    ) -> List[Document]:\n",
    "        \"\"\"Parse file into string.\"\"\"\n",
    "        \n",
    "        with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "        if self._remove_hyperlinks:\n",
    "            content = self.remove_hyperlinks(content)\n",
    "        if self._remove_images:\n",
    "            content = self.remove_images(content)\n",
    "            \n",
    "        local_metadata = self.get_local_metadata(content)\n",
    "\n",
    "        extra_info = dict(extra_info,**local_metadata) if local_metadata else {}\n",
    "        return [Document(text=text,metadata=extra_info)]\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f9c01902",
   "metadata": {},
   "outputs": [],
   "source": [
    "md_reader = RagasMdReader(remove_hyperlinks=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bca9d1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm2(prompt, **kwargs):\n",
    "    response = client.chat.completions.create(\n",
    "        model=kwargs.get(\"model\", \"gpt-3.5-turbo\"),\n",
    "        messages=[{\"role\": \"system\", \"content\": prompt}],\n",
    "        temperature=kwargs.get(\"temperature\", 0),\n",
    "        top_p=kwargs.get(\"top_p\", 1),\n",
    "        frequency_penalty=kwargs.get(\"frequency_penalty\", 0.0),\n",
    "        presence_penalty=kwargs.get(\"presence_penalty\", 0.0),\n",
    "        max_tokens=kwargs.get(\"max_tokens\", 500),\n",
    "        n=kwargs.get(\"n\", 1),\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "feec19c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.prompts import SEED_QUESTION, EVOLUTION_ELIMINATION, REWRITE_QUESTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "95d00bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = [\n",
    "    \"/Users/shahules/Myprojects/rag-experiments/gitlab-handbook/data/handbook/leadership/\",\n",
    "    \"/Users/shahules/Myprojects/rag-experiments/gitlab-handbook/data/handbook/company/\",\n",
    "]\n",
    "documents = []\n",
    "for dir_path in dirs:\n",
    "    loader = SimpleDirectoryReader(dir_path, \n",
    "                                   recursive=True,\n",
    "                                  file_extractor={\".md\":md_reader},\n",
    "                                file_metadata=RagasMdReader.get_file_metadata)\n",
    "    \n",
    "    documents.extend(loader.load_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "64e715b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Understanding SOCIAL STYLES\n"
     ]
    }
   ],
   "source": [
    "print(documents[14].metadata['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "15e60505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Sample text\n",
    "text = \"\"\"\n",
    "---\n",
    "title: \"Ask Me Anything\"\n",
    "description: \"Learn and ask questions at GitLab's Ask Me Anything (AMA) meetings\"\n",
    "---\n",
    "\"\"\"\n",
    "\n",
    "# Regular expression to capture title and description\n",
    "pattern_optional_description = '---\\s*title:\\s*(.+?)(?:\\s*description:\\s*\"(.*?)\")?\\s*---'\n",
    "\n",
    "# Finding matches\n",
    "matches = re.match(pattern_optional_description, text)\n",
    "\n",
    "matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "08037476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4cc57d02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{}]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = re.compile('---\\s*title:\\s*(.+?)(?:\\s*description:\\s*\"(.*?)\")?\\s*---')\n",
    "[m.groupdict() for m in pattern.finditer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "58b4cdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "---\n",
    "title: \"Book clubs\"\n",
    "---\n",
    "\n",
    "From time to time, we run internal book clubs on a book from one of our resource lists. All are welcome! However,\n",
    "each club has a suggested audience to indicate roles to which the content is tailored.\n",
    "\n",
    "- [Leadership]({{< ref \"_index.md#books\" >}})\n",
    "- [Development](https://about.gitlab.com/handbook/engineering/development/#books)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3a953420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern_optional_description = r'---\\s*title:\\s*\"(.+?)\"\\s*---'\n",
    "re.findall(pattern,text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "64f4ad42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "168"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b94cfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4bc085c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testset = testsetgenerator.generate(documents, test_size=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26f40412",
   "metadata": {},
   "outputs": [],
   "source": [
    "testset.to_pandas().to_csv(\"gitlab_communication_company_v2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4ae48ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "92931c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1=\"How should I use my notification settings in Slack?\"\n",
    "q2=\"What's the best way to manage Slack notification settings for efficient communication and minimal disruptions?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1052549c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = EVOLUTION_ELIMINATION.prompt.template.format(question1=q1, question2=q2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37bedda",
   "metadata": {},
   "source": [
    "## Create multi context\n",
    "- find similar docs using metadata \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a0773b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from llama_index.indices.query.embedding_utils import get_top_k_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "bac7c92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "41792a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = [doc.metadata.get('title').strip('\"') for doc in documents if doc.metadata]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "165254ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.embed_documents(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "064ef79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "19d68a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, indices = get_top_k_embeddings(embeddings[k],embeddings,similarity_cutoff=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "ac6d8408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seach query :Top Cross-Functional Initiatives\n",
      " Results: [['Top Cross-Functional Initiatives', 'Building High Performing Teams', 'Product Career Development Framework Working Group', 'Leadership', 'Single Codebase Working Group']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Seach query :{titles[k]}\\n Results: [{[titles[i] for i in indices[:5]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4c9938",
   "metadata": {},
   "source": [
    "## Merge documents based on meta-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0695effe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import HumanMessagePromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2f28fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "REWRITE_QUESTION = HumanMessagePromptTemplate.from_template(\"\"\"\n",
    "Rewrite the given question so that it can be answered without context.\n",
    "\n",
    "Question: When was he born?\n",
    "Context : Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time. He was born on 14 March 1879.\n",
    "Rewritten question: When was Albert Einstein born? \n",
    "\n",
    "Context: A clothes iron (also flatiron, smoothing iron, or simply iron) is a small appliance that, when heated, is used to press clothes to remove wrinkles and unwanted creases. Domestic irons generally range in operating temperature from between 121 Â°C (250 Â°F) to 182 Â°C (360 Â°F). It is named for the metal (iron) of which the device was historically made, and the use of it is generally called ironing, the final step in the process of laundering clothes.\n",
    "Question: What is temperate range of the device?\n",
    "Rewritten question: What is temperate range of clothes iron?\n",
    "\n",
    "\n",
    "Question:{question}\n",
    "Context: {context}\n",
    "Rewritten question:\n",
    "\"\"\")\n",
    "\n",
    "QUERY =  HumanMessagePromptTemplate.from_template(\"\"\"\n",
    "Rewrite the question using given context so that it can be read and answered without any extra information.\n",
    "\n",
    "Question:{question}\n",
    "Context: {context}\n",
    "\"\"\")\n",
    "\n",
    "text = \"\"\"\n",
    "The Mona Lisa, painted by Leonardo da Vinci in the early 16th century, is one of the most famous and valuable paintings in the world.\n",
    "Known for its enigmatic expression and innovative use of sfumato, the painting has become a symbol of Renaissance art.\n",
    "The Mona Lisa is displayed in the Louvre Museum in Paris and attracts millions of visitors annually.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5fc666f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content(indices):\n",
    "    \n",
    "    return '\\n'.join([documents[idx].get_content() for idx in indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d871744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "455779c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm2(SEED_QUESTION.format(context=get_content([49,50])).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3baa48ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the purpose of the GPT-3 Editor in the CORE framework?'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = response.choices[0].message.content\n",
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2839379b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rewrite the question using given context so that it can be read and answered without any extra information.\n",
      "\n",
      "Question:What is the impact of training data size on the model's performance?\n",
      "Context: Preprint.\n",
      "Table 2: Overall experiment results on six tasks. Bold numbers indicate the best performance among\n",
      "non-proprietary models, and gray-colored bold text indicates the best proprietary model when\n",
      "they outperforms all non-proprietary models.âˆ—indicates concurrent or recent results reported by\n",
      "concurrent work. â€“ indicates numbers that are not reported by the original papers or are not applicable.\n",
      "Models are sorted based on scale. FS, em, rg, mau, prec, rec denote FactScore (factuality); str-em,\n",
      "rouge (correctness); MAUVE (fluency); citation precision and recall, respectively.\n",
      "Short-form Closed-set Long-form generations (with citations)\n",
      "PopQA TQA Pub ARC Bio ASQA\n",
      "LM (acc) (acc) (acc) (acc) (FS) (em) (rg) (mau) (pre) (rec)\n",
      "LMs with proprietary data\n",
      "Llama2-c 13B 20.0 59.3 49.4 38.4 55.9 22.4 29.6 28.6 â€“ â€“\n",
      "Ret-Llama2-c 13B 51.8 59.8 52.1 37.9 79.9 32.8 34.8 43.8 19.8 36.1\n",
      "ChatGPT 29.3 74.3 70.1 75.3 71.8 35.3 36.2 68.8 â€“ â€“\n",
      "Ret-ChatGPT 50.8 65.7 54.7 75.3 â€“ 40.7 39.9 79.7 65.1 76.6\n",
      "Perplexity.ai â€“ â€“ â€“ â€“ 71.2 â€“ â€“ â€“ â€“ â€“\n",
      "Baselines without retrieval\n",
      "Llama2 7B 14.7 30.5 34.2 21.8 44.5 7.9 15.3 19.0 â€“ â€“\n",
      "Alpaca 7B 23.6 54.5 49.8 45.0 45.8 18.8 29.4 61.7 â€“ â€“\n",
      "Llama2 13B 14.7 38.5 29.4 29.4 53.4 7.2 12.4 16.0 â€“ â€“\n",
      "Alpaca 13B 24.4 61.3 55.5 54.9 50.2 22.9 32.0 70.6 â€“ â€“\n",
      "CoVE 65B* â€“ â€“ â€“ â€“ 71.2 â€“ â€“ â€“ â€“ â€“\n",
      "Baselines with retrieval\n",
      "Toolformer* 6B â€“ 48.8 â€“ â€“ â€“ â€“ â€“ â€“ â€“ â€“\n",
      "Llama2 7B 38.2 42.5 30.0 48.0 78.0 15.2 22.1 32.0 2.9 4.0\n",
      "Alpaca 7B 46.7 64.1 40.2 48.0 76.6 30.9 33.3 57.9 5.5 7.2\n",
      "Llama2-FT 7B 48.7 57.3 64.3 65.8 78.2 31.0 35.8 51.2 5.0 7.5\n",
      "SAIL* 7B â€“ â€“ 69.2 48.4 â€“ â€“ â€“ â€“ â€“ â€“\n",
      "Llama2 13B 45.7 47.0 30.2 26.0 77.5 16.3 20.5 24.7 2.3 3.6\n",
      "Alpaca 13B 46.1 66.9 51.1 57.6 77.7 34.8 36.7 56.6 2.0 3.8\n",
      "Our SELF-RAG 7B 54.9 66.4 72.4 67.3 81.2 30.0 35.7 74.3 66.9 67.8\n",
      "Our SELF-RAG 13B 55.8 69.3 74.5 73.1 80.2 31.7 37.0 71.6 70.3 71.3\n",
      "5 R ESULTS AND ANALYSIS\n",
      "5.1 M AINRESULTS\n",
      "Comparison against baselines without retrieval. Table 2 (top) presents the baselines without\n",
      "retrieval. Our SELF-RAG(bottom two rows) demonstrates a substantial performance advantage\n",
      "over supervised fine-tuned LLMs in all tasks and even outperforms ChatGPT in PubHealth, PopQA,\n",
      "biography generations, and ASQA (Rouge and MAUVE). Our approach also significantly outperforms\n",
      "a concurrent method that employs sophisticated prompt engineering; specifically, on the bio generation\n",
      "task, our 7B and 13B models outperform the concurrent CoVE (Dhuliawala et al., 2023), which\n",
      "iteratively prompts Llama2 65Bto refine output.\n",
      "Comparison against baselines with retrieval. As shown in Tables 2 (bottom), our SELF-RAGalso\n",
      "outperforms existing RAG in many tasks, obtaining the best performance among non-proprietary\n",
      "LM-based models on all tasks. While our method outperforms other baselines, on PopQA or Bio,\n",
      "powerful instruction-tuned LMs with retrieval (e.g., LLama2-chat, Alpaca) show large gains from\n",
      "their non-retrieval baselines. However, we found that these baselines provide limited solutions for\n",
      "tasks where we cannot simply copy or extract sub-strings of retrieved passages. On PubHealth\n",
      "and ARC-Challenge, baselines with retrieval do not improve performance notably from their no-\n",
      "retrieval counterparts. We also observe that most baselines with retrieval struggle to improve citation\n",
      "accuracy. On ASQA, our model shows significantly higher citation precision and recall than all\n",
      "models except ChatGPT. Gao et al. (2023) found that ChatGPT consistently exhibits superior efficacy\n",
      "in this particular task, surpassing smaller LMs. Our SELF-RAGbridges this performance gap, even\n",
      "outperforming ChatGPT in citation precision, which measures whether the model-generated claim is\n",
      "fully supported by cited evidence. We also found that on the metrics for factual precision, SELF-RAG\n",
      "7B occasionally outperforms our 13B due to the tendency of smaller SELF-RAGto often generate\n",
      "8\n",
      "Preprint.\n",
      "PQA Med AS\n",
      "(acc) (acc) (em)\n",
      "SELF-RAG(50k) 45.5 73.5 32.1\n",
      "Training\n",
      "No Retriever R 43.6 67.8 31.0\n",
      "No Critic C 42.6 72.0 18.1\n",
      "Test\n",
      "No retrieval 24.7 73.0 â€“\n",
      "Hard constraints 28.3 72.6 â€“\n",
      "Retrieve top1 41.8 73.1 28.6\n",
      "Remove ISSUP 44.1 73.2 30.6\n",
      "(a) Ablation\n",
      "1 270.070.5Precision\n",
      "1 2\n",
      "Weight for IsSupport9095Mauve\n",
      " (b) Customization\n",
      "0.0 0.2 0.4 0.60.980.990.991.00Accuracy\n",
      "PubHealth\n",
      "0.0 0.2 0.4 0.6\n",
      "Retrieval Threshold0.60.81.0AccuracyPopQA0.00.51.0\n",
      "Frequency\n",
      "0.250.500.751.00\n",
      "Frequency\n",
      " (c) Retrieval\n",
      "Figure 3: Analysis on SELF-RAG:(a)Ablation studies for key components of SELF-RAGtraining\n",
      "and inference based on our 7B model. (b) Effects of soft weights on ASQA citation precision and\n",
      "Mauve (fluency). (c) Retrieval frequency andnormalized accuracy on PubHealth and PopQA.\n",
      "precisely grounded yet shorter outputs. Llama2-FT 7B, which is the baseline LM trained on the same\n",
      "instruction-output pairs as SELF-RAGwithout retrieval or self-reflection and is retrieval-augmented\n",
      "at test time only, lags behind S ELF-RAG. This result indicates S ELF-RAGgains are not solely from\n",
      "training data and demonstrate the effectiveness of S ELF-RAGframework.\n",
      "5.2 A NALYSIS\n",
      "Ablation studies. We conduct a set of ablations of our framework to identify which factors play\n",
      "key roles. We evaluate two model variants trained differently than our model: No Retriever trains an\n",
      "LM using the standard instruction-following method given instruction-output pairs, without retrieved\n",
      "passages; No Critic trains an LM trained with input-output pairs that are always augmented with the\n",
      "top one retrieved document without reflection tokens. This is similar to SAIL (Luo et al., 2023), and\n",
      "we use our instruction-output data instead of using the Alpaca dataset (Dubois et al., 2023), as in\n",
      "SAIL. We also conduct ablation on our inference-time algorithm, including No retrieval disables\n",
      "retrieval during inference; Hard constraints indicates the model performance that retrieves when\n",
      "Retrieve =Yes instead of using the adaptive threshold; Retrieve top 1 always retrieves and uses the\n",
      "top one document only, similar to standard RAG approaches; Remove ISSUPindicates the model\n",
      "performance that removes ISSUPscore only during critique-guided beam search in Eq. 4. In this\n",
      "ablation experiment, we use a training instance size of 50k for a more efficient exploration of training\n",
      "variations. Later in this section, we conduct an analysis of the effect of training data size. We conduct\n",
      "the ablation studies on three datasets, PopQA, PubHealth, and ASQA. On ASQA, we evaluate models\n",
      "on sampled 150 instances and exclude ablations involving adaptive or no retrieval processes.\n",
      "We show in Table 3a the ablation results. The top part of the table shows results for training ablations,\n",
      "and the bottom part is for inference ablations. We see that all components play important roles. We\n",
      "also observe a large performance gap between SELF-RAGand No Retriever or Critic baselines across\n",
      "tasks, indicating that training an LM with those models largely contributes to the performance gain of\n",
      "SELF-RAG. Using the top passages regardless of their relevance (Retrieve top 1) as in conventional\n",
      "RAG approaches causes a large drop in PopQA and ASQA, and removing ISSUPduring the beam\n",
      "search results hurts performance on ASQA. This demonstrates the effectiveness of SELF-RAGâ€™s\n",
      "capabilities of carefully selecting generations based fine-grained multiple criterion, instead of naively\n",
      "using all of the top passages from the retrieval model or solely depending on relevance scores.\n",
      "Effects of inference-time customization. One key benefit of our proposed framework is that it\n",
      "enables us to control how much each critique type affects the final generation sampling. We analyze\n",
      "the effects of different parameter weights on the top of our 7B model during inference time on\n",
      "ASQA, where multiple evaluation aspects are considered. Figure 3b shows the effects of changing\n",
      "the weighting term for ISSUP, which criticizes how supported the output is by the text passage. As\n",
      "the figure shows, increasing the weight leads to positive effects on the modelsâ€™ citation precision\n",
      "since this puts more emphasis on whether model generation is supported by the evidence. On the\n",
      "9\n",
      "Preprint.\n",
      "0 50 100 150\n",
      "Num of training (k)3540455055Perfomance\n",
      "(a) PopQA\n",
      "0 100\n",
      "Num of training (k)717273\n",
      " (b) PubHealth\n",
      "0 100\n",
      "Num of training (k)4060\n",
      " (c) ASQA (prec)Pop Bio.\n",
      "S & P 92.5 70.0\n",
      "ISREL 95.0 90.0\n",
      "ISSUP 90.0 85.0\n",
      "(d) Human evaluation on PopQA\n",
      "and Bio generation.\n",
      "Figure 4: Training scale and Human analysis: (a) (b) (c) Training scale analysis shows the effect\n",
      "of the training data scale on PopQA, PubHealth and ASQA (citation precision), respectively. (d)\n",
      "Human analysis on S ELF-RAGoutputs as well as reflection tokens.\n",
      "contrary, a larger weight results in lower MAUVE scores: when generation gets longer and more\n",
      "fluent, there are often more claims that are not fully supported by citations, consistent with findings\n",
      "by Liu et al. (2023a). Our framework lets practitioners choose and customize modelsâ€™ behaviors at\n",
      "test time by adjusting such parameters without requiring additional training.\n",
      "Efficiency and accuracy trade-off. Using our framework, practitioners can adjust how often retrieval\n",
      "occurs using the token probability of reward tokens. We evaluate how this adaptive threshold affects\n",
      "overall accuracy and frequency of retrieval, and we evaluate the performance with varying numbers\n",
      "of threshold Î´(larger Î´results in less retrieval) on PubHealth and PopQA. Figure 3c shows that\n",
      "the modelâ€™s retrieval frequencies dramatically change on both datasets. as Î´varies. On one hand,\n",
      "performance deterioration by retrieving less is smaller on PubHealth but larger in PopQA.\n",
      "Effects of training data size. We conduct an analysis of how the data scale affects the modelâ€™s\n",
      "performance. In particular, we randomly sample 5k, 10k, 20k, and 50k instances from our original\n",
      "150k training instances, and fine-tune four SELF-RAG 7Bvariants on those subsets. Then, we compare\n",
      "the model performance on PopQA, PubHealth, and ASQA (citation precision) with our final SELF-\n",
      "RAGtrained on the full 150k instances. We also evaluate Figures 4a, 4b and 4c shows the modelsâ€™\n",
      "performance trained on different amount of data. Across all datasets, increasing data size often shows\n",
      "upward trajectories and the improvements are significantly larger in PopQA and ASQA, while we do\n",
      "not observed such significant improvements on Llama2-FT 7Bwhen increasing the training data from\n",
      "50k to 150k. These results also indicate that further expanding the training data of SELF-RAGmay\n",
      "lead to further improvements, although in this work we limit our training data size to 150k.\n",
      "Human evaluations. We conduct small human evaluations on SELF-RAGoutputs, as well as the\n",
      "reliability of predicted reflection tokens. In particular, we sampled 50 samples from PopQA and Bio\n",
      "results. Following Menick et al. (2022), human annotators evaluate S&P , which indicates whether\n",
      "the model output is plausible (i.e., the output is a reasonable and on-topic response to the question\n",
      "as if it were occurring in a conversation) and supported (i.e., the provided evidence is sufficient to\n",
      "verify the validity of the answer). For S&P, we do not consider the instances where SELF-RAG\n",
      "predicts irrelevant orno support . We then ask our annotators whether the model-predicted\n",
      "reflection tokens about ISRELand ISSUPmatch their inspections (e.g., whether the fully supported\n",
      "output is supported by the cited evidence). Human annotators find SELF-RAGanswers are often\n",
      "plausible and supported by relevant passages with higher S&P scores on short-form PopQA, which is\n",
      "consistent with Menick et al. (2022). Human annotators also find ISRELand ISSUPreflection token\n",
      "predictions are mostly aligned with their assessments. Appendix Table 6 shows several annotated\n",
      "examples and explanations on assessments.\n",
      "6 C ONCLUSION\n",
      "This work introduces SELF-RAG, a new framework to enhance the quality and factuality of LLMs\n",
      "through retrieval on demand and self-reflection. SELF-RAGtrains an LM to learn to retrieve, generate,\n",
      "and critique text passages and its own generation by predicting the next tokens from its original\n",
      "vocabulary as well as newly added special tokens, called reflection tokens. SELF-RAGfurther enables\n",
      "the tailoring of LM behaviors at test time by leveraging reflection tokens. Our holistic evaluations on\n",
      "six tasks using multiple metrics demonstrate that SELF-RAGsignificantly outperforms LLMs with\n",
      "more parameters or with conventional retrieval-augmented generation approaches.\n",
      "10\n",
      "\n",
      "Rewritten question: How does the size of the training data affect the performance of the SELF-RAG model according to the experiment results?\n"
     ]
    }
   ],
   "source": [
    "print(QUERY.format(question=q, context=get_content([12,13,14])).content)\n",
    "print('Rewritten question: How does the size of the training data affect the performance of the SELF-RAG model according to the experiment results?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fd1f148b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1909"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(get_content([12,13,14]).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3bad307a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stimuli BMS [41] Pan et al. [29] Salicon [15] ML-Net [9] Ours Ground Truth\n",
      "Figure 3: Qualitative performance comparison .\n",
      "SateliteJumbledSocial\n",
      "OutdoorNaturalArt\n",
      "InvertedFractalObjectCartoon\n",
      "OutdoorManMadeIndoorActionPatternNoisy\n",
      "Affective\n",
      "BlackWhiteRandom\n",
      "LowResolutionLineDrawingSketchSketchLineDrawingLowResolutionRandomBlackWhiteAffectiveNoisyPatternActionIndoorOutdoorManMadeCartoonObjectFractalInvertedArtOutdoorNaturalSocialJumbledSatelite\n",
      "0.00.20.40.60.81.0\n",
      "Figure 4: Classiï¬cation confusion matrix .\n",
      "bias. Location bias is an important component of human\n",
      "visual attention, and so should be included in any model.\n",
      "Table 2 shows the results of the 5-fold cross validation\n",
      "tests on the CAT2000 dataset. We achieve the best perfor-\n",
      "mance of the tested models. Table 3 shows the performance\n",
      "on the held out test images. Our model achieves the second\n",
      "best performance behind the DeepFix model [22]. Com-\n",
      "pared with DeepFix our network is shallower with less pa-\n",
      "rameters. The DeepFix model is not publicly available to\n",
      "test on the cross-validation data. Figure 3 shows example\n",
      "saliency maps generated by the tested models.\n",
      "Table 4 shows the results on the Salicon test set. Under\n",
      "this dataset our model does not have a performance advan-CC SAUC AUC Judd\n",
      "Our Model 0.730 0.771 0.861\n",
      "ML-Net [9] 0.7430 0.7680 0.8660\n",
      "Deep Convnet [29] 0.6220 0.7240 0.8580\n",
      "Shallow Convnet [29] 0.5957 0.6698 0.8364\n",
      "Rare 2012 Improved [31] 0.5108 0.6644 0.8148\n",
      "Baseline: BMS [41] 0.4268 0.6935 0.7899\n",
      "Baseline: GBVS [12] 0.4212 0.6303 0.7899\n",
      "Baseline: Itti [16] 0.2046 0.6101 0.6669\n",
      "Table 4: Salicon test set results.\n",
      "tage over ML-Net. This is largely because the dataset does\n",
      "not have diverse categories like the CAT-2000 dataset. Nev-\n",
      "ertheless, the performance of our model is roughly equiva-\n",
      "lent to the ML-net model.\n",
      "Comparison with averaging ensemble Given that the\n",
      "mixture model can be seen as an ensemble, it is important\n",
      "to compare the performance with a vanilla ensemble. We\n",
      "generate an ensemble of 5 networks where each network is\n",
      "trained on a random 80% of the training split. The saliency\n",
      "maps from each ensemble member are averaged to produce\n",
      "the ï¬nal saliency map. Our model still achieves superior\n",
      "performance compared with the ensemble (Table 5). Addi-\n",
      "tionally, our model is signiï¬cantly faster and requires less\n",
      "storage space due to the weight sharing.\n"
     ]
    }
   ],
   "source": [
    "print(get_content([14]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8464da3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.node_parser.simple import SimpleNodeParser\n",
    "node_parser = SimpleNodeParser.from_defaults(\n",
    "            chunk_size=1000, chunk_overlap=0, include_metadata=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ec05e172",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = node_parser.get_nodes_from_documents(\n",
    "            documents=documents\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a6cd93d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'page_label': '2',\n",
       " 'file_name': '00891b5a3dbbe4dae6b06432c3f335f8.pdf',\n",
       " 'file_path': 'arxiv-papers/00891b5a3dbbe4dae6b06432c3f335f8.pdf',\n",
       " 'creation_date': '2023-11-14',\n",
       " 'last_modified_date': '2023-11-14',\n",
       " 'last_accessed_date': '2023-11-14'}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes[1].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "03315c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1,2,3,4,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0c61a824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c5cb58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragas",
   "language": "python",
   "name": "ragas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
