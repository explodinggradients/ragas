{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d341594d",
   "metadata": {},
   "source": [
    "## Logs\n",
    "- Factuality NLI\n",
    "    - Without CoT\n",
    "    - With CoT ( WIN)  \n",
    "    - WikiQA \n",
    "        - generated non factual answer for measuring factuality agreement.\n",
    "        - Kendall Score = 0.7\n",
    "    - HotPotQA\n",
    "        - Accuracy = 0.75 \n",
    "    - Possible Improvements \n",
    "        - improve statement generation\n",
    "\n",
    "- Relevance scores\n",
    "    - QGen method\n",
    "        - models tried : t5-base / gptneo-125M\n",
    "        - WikiQA\n",
    "            - Kendall score = 0.65\n",
    "            - observations : finetune model on prompt/answer pairs to improve performance.\n",
    "    - Cross-encoder method\n",
    "        - models tried : distilbert \n",
    "        - WikiQA\n",
    "            - kendall score = 0.63\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bfb2480",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/alerts/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "import os\n",
    "import openai\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy.stats import kendalltau, spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4168502",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/Users/shahules/belar/src/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9adac051",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_KEY = json.load(open(\"/Users/shahules/openai-key.json\"))[\"jj\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21e09881",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c2a602",
   "metadata": {},
   "source": [
    "## OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bce4c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = OPENAI_KEY\n",
    "\n",
    "\n",
    "def llm(prompt, **kwargs):\n",
    "    response = openai.Completion.create(\n",
    "        model=kwargs.get(\"model\", \"text-davinci-003\"),\n",
    "        prompt=prompt,\n",
    "        temperature=kwargs.get(\"temperature\", 0),\n",
    "        top_p=kwargs.get(\"top_p\", 1),\n",
    "        frequency_penalty=kwargs.get(\"frequency_penalty\", 0.0),\n",
    "        presence_penalty=kwargs.get(\"presence_penalty\", 0.0),\n",
    "        max_tokens=kwargs.get(\"max_tokens\", 500),\n",
    "        logprobs=kwargs.get(\"logprobs\", 1),\n",
    "        n=kwargs.get(\"n\", 1),\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d9b4e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_logger(data, filename=\"nli_check\"):\n",
    "    output = json.load(open(filename + \".json\"))\n",
    "    output.append(data)\n",
    "    with open(filename + \".json\", \"w\") as file:\n",
    "        json.dump(output, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50add06b",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9f4280e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/Users/shahules/.cache/huggingface/datasets/explodinggradients___parquet/explodinggradients--ragas-wikiqa-5b5116e5cb909aca/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100%|████████████████████████████████████████████████████| 1/1 [00:00<00:00, 242.78it/s]\n"
     ]
    }
   ],
   "source": [
    "wikiqa_ragas = load_dataset(\"explodinggradients/ragas-wikiqa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e0148e",
   "metadata": {},
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eca20daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corr(target, prediction):\n",
    "    return [kendalltau(x, y).correlation for x, y in zip(target, predictions)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5563146",
   "metadata": {},
   "source": [
    "## QA-QG paradigm\n",
    "- Generate question and answer pair from `generated answer`.\n",
    "- Given `context`, ask these questions\n",
    "- Verify answer correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3e35532",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question_generation = \"\"\"Given a text, extract {} noun phrases and create questions for each based on given text.\n",
    "text: Albert Einstein was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. Best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\n",
    "A: Germany\n",
    "Q: Where was Albert Einstein born?\n",
    "A: theory of relativity\n",
    "Q: What is Albert Einstein best known for?\n",
    "text: {}\n",
    "\"\"\"\n",
    "\n",
    "Question_answering = \"\"\"Given a text and set of questions, answer the questions\n",
    "text: Albert Einstein was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. Best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\n",
    "questions: Where was Albert Einstein born?\\n\\nWhat is Albert Einstein best known for?\n",
    "answers:Germany\\n\\ntheory of relativity\n",
    "text: {}\n",
    "questions:{}\n",
    "answers:\"\"\"\n",
    "\n",
    "Answer_verification = \"\"\"Given a set of questions, correct answer and student's answer return the number of questions incorrectly answered by student.\n",
    "Where was Albert Einstein born?\\nCorrect answer: Germany\\nStudent answer:India\\n\\n\n",
    "What is Albert Einstein best known for?\\nCorrect answer:  theory of relativity\\nStudent answer: theory of relativity\\n\\n\n",
    "Number of incorrect answers:1\n",
    "{}\n",
    "Number of incorrect answers:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "335081e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def QAQG_fun(question, context, answer):\n",
    "    \"\"\"\n",
    "    returns number of factual inconsistencies.\n",
    "    \"\"\"\n",
    "\n",
    "    def answer_ver(qstn, answer, cand):\n",
    "        return f\"{qstn}\\nCorrect answer: {answer}\\nStudent answer: {cand}\"\n",
    "\n",
    "    num = len(answer.split(\".\")) - 1\n",
    "    prompt = Question_generation.format(num, answer)\n",
    "    output = llm(prompt)\n",
    "    qa_pairs = [\n",
    "        re.sub(r\"A:|Q:\", \"\", x).strip()\n",
    "        for item in output[\"choices\"][0][\"text\"].strip().split(\"\\n\\n\")\n",
    "        for x in item.split(\"\\n\")\n",
    "    ]\n",
    "    qa_pairs = [tuple(qa_pairs[i : i + 2]) for i in range(0, len(qa_pairs), 2)]\n",
    "    print(qa_pairs)\n",
    "    questions = \"\\n\\n\".join([qstn for ans, qstn in qa_pairs])\n",
    "    prompt = Question_answering.format(context, questions)\n",
    "    answers = llm(prompt)[\"choices\"][0][\"text\"].split(\"\\n\\n\")\n",
    "\n",
    "    prompt = \"\\n\\n\".join(\n",
    "        [answer_ver(qstn, ans, cand) for (ans, qstn), cand in zip(qa_pairs, answers)]\n",
    "    )\n",
    "    output = llm(Answer_verification.format(prompt))[\"choices\"][0][\"text\"].strip()\n",
    "    return int(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2642e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = \"The actress who played Lolita, Sue Lyon, was 14 at the time of filming.\"\n",
    "question = \"What was the age of Sue Lyon when she played Lolita?\"\n",
    "context = \"\"\"\n",
    "Lolita is a 1962 psychological comedy-drama film[5] directed by Stanley Kubrick and based on the 1955 novel of the same title by Vladimir Nabokov, who is also credited with writing the screenplay. The film follows Humbert Humbert, a middle-aged literature lecturer who becomes sexually infatuated with Dolores Haze (nicknamed \"Lolita\"), a young adolescent girl. It stars James Mason, Shelley Winters, Peter Sellers and, as the titular character, Sue Lyon.\n",
    "\n",
    "Owing to restrictions imposed by the Motion Picture Production Code, the film toned down the most provocative aspects of the novel, sometimes leaving much to the audience's imagination. The actress who played Lolita, Sue Lyon, was 14 at the time of filming.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26ca4af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Sue Lyon', 'Who played the role of Lolita in the movie?')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QAQG_fun(question, context, answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2078ece",
   "metadata": {},
   "source": [
    "## G-Eval\n",
    "- Define criterions to evaluate model.\n",
    "- Normalize `score = prob(s) * s`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca1c56d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevence = \"\"\"\n",
    "Evaluation Criteria.\\n\n",
    "Relevance (1-5) - how relevant is the reply to the given question.\n",
    "1. Read the reply and compare it to the question. Check if the given reply\n",
    "actually answers the question, and if it presents them in a clear and logical order.\n",
    "2. The reply should include only required information to answer the question.\n",
    "3. Penalize replies that contain redundancies and excess information.\n",
    "4. Assign a score for Relevance on a scale of 1 to 5, where 1 is the lowest and\n",
    "5 is the highest based on the Evaluation Criteria.\n",
    "\n",
    "question:{}\n",
    "reply:{}\n",
    "score:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd7fed9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_eval(question, context, answer):\n",
    "    prompt = relevence.format(question, answer)\n",
    "    output = llm(prompt)[\"choices\"][0]\n",
    "    prob = np.exp(sum(output[\"logprobs\"][\"token_logprobs\"]))\n",
    "    score = int(output[\"text\"].strip())\n",
    "    print(score)\n",
    "    return prob * score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35113558",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Which year did Lolita release?\"\n",
    "answer = \"Lolita film released in 1947.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e82d0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.5533440372846865"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_eval(question, context, answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dce1baa",
   "metadata": {},
   "source": [
    "## Relevancy Score \n",
    "- Scores `answers` according to `prompt`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75aa62eb",
   "metadata": {},
   "source": [
    "### QGen scoring method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc263805",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics.answer_relevance import QGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38deaf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_qgen = QGen(\"t5-base\", \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45942810",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_relevance(examples):\n",
    "    scores = {}\n",
    "    questions = examples[\"question\"]\n",
    "    for col in COLUMNS:\n",
    "        passage = examples[col]\n",
    "        inputs = list(zip(questions, passage))\n",
    "        scores[f\"{col}_relevance\"] = t5_qgen.predict(inputs, show_progress=False)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1410f3c",
   "metadata": {},
   "source": [
    "- We assume `generated_with_rag > correct_answer > incorrect_answer` for relevancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab00e4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS = [\"generated_with_rag\", \"correct_answer\", \"incorrect_answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e705767d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/shahules/.cache/huggingface/datasets/explodinggradients___parquet/explodinggradients--ragas-wikiqa-5b5116e5cb909aca/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-6b548dfc2a4d5a4f.arrow\n"
     ]
    }
   ],
   "source": [
    "output = (\n",
    "    wikiqa_ragas[\"train\"]\n",
    "    .select(range(0, 10))\n",
    "    .map(predict_relevance, batched=True, batch_size=4)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "3ab21cdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6518583971423787"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [[item[f\"{k}_relevance\"] for k in COLUMNS] for item in output]\n",
    "target = [[2, 1, 0] for i in range(len(output))]\n",
    "get_corr(target, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d562351",
   "metadata": {},
   "source": [
    "### Cross encoder method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6d76ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics.context_relevance import context_relavancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bcb4e25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_relevance(examples):\n",
    "    scores = {}\n",
    "    questions = examples[\"question\"]\n",
    "    for col in COLUMNS:\n",
    "        passage = examples[col]\n",
    "        inputs = list(zip(questions, passage))\n",
    "        scores[f\"{col}_relevance\"] = cross_encoder.predict(inputs, show_progress=False)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36565a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = (\n",
    "    wikiqa_ragas[\"train\"]\n",
    "    .select(range(0, 10))\n",
    "    .map(predict_relevance, batched=True, batch_size=4)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3f0571",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [[item[f\"{k}_relevance\"] for k in COLUMNS] for item in output]\n",
    "target = [[2, 1, 0] for i in range(len(output))]\n",
    "get_tau(target, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefd9923",
   "metadata": {},
   "source": [
    "## Factuality on HotpotQA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "2316c8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "6cd24f8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'experimental' (namespace)>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "\n",
    "reload(experimental)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "723e662a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experimental.nli import NLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "f3f9bd55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset hotpot_qa (/Users/shahules/.cache/huggingface/datasets/hotpot_qa/distractor/1.0.0/133b9501f892e5193babbad937bee3b4899deb4691ef4d791e6ac0111c875bb5)\n"
     ]
    }
   ],
   "source": [
    "hotpot_qa = load_dataset(\n",
    "    \"hotpot_qa\",\n",
    "    \"distractor\",\n",
    "    split=\"validation\",\n",
    ").select(range(0, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "2ab98cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "false_answer_prompt = \"\"\"Given a question and correct answer, generate an incorrect answer\n",
    "question: Were Scott Derrickson and Ed Wood of the same nationality?\n",
    "correct answer: yes\n",
    "answer: no\n",
    "question: {}\n",
    "correct answer: {}\n",
    "answer:\"\"\"\n",
    "\n",
    "\n",
    "def generate_false_answers(question, answer):\n",
    "    answer = llm(false_answer_prompt.format(question, answer))[\"choices\"][0][\n",
    "        \"text\"\n",
    "    ].strip()\n",
    "    return {\"false_answer\": answer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "542bdb71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/shahules/.cache/huggingface/datasets/hotpot_qa/distractor/1.0.0/133b9501f892e5193babbad937bee3b4899deb4691ef4d791e6ac0111c875bb5/cache-593e03a966a13563.arrow\n"
     ]
    }
   ],
   "source": [
    "hotpot_qa = hotpot_qa.map(lambda x: generate_false_answers(x[\"question\"], x[\"answer\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "0f8682fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context(item):\n",
    "    titles, ids = item[\"supporting_facts\"].values()\n",
    "    title_ids = [item[\"context\"][\"title\"].index(i) for i in titles]\n",
    "    sentences = [\n",
    "        item[\"context\"][\"sentences\"][i][k]\n",
    "        for i, k in zip(title_ids, item[\"supporting_facts\"][\"sent_id\"])\n",
    "    ]\n",
    "    orig_context = \" \".join(sentences)\n",
    "    return {\"answer_context\": orig_context}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "a94511fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/shahules/.cache/huggingface/datasets/hotpot_qa/distractor/1.0.0/133b9501f892e5193babbad937bee3b4899deb4691ef4d791e6ac0111c875bb5/cache-7badd24e430a747f.arrow\n"
     ]
    }
   ],
   "source": [
    "hotpot_qa = hotpot_qa.map(lambda x: get_context(x), batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "84f39785",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_factuality(examples):\n",
    "    scores = {}\n",
    "    questions = examples[\"question\"]\n",
    "    contexts = examples[\"answer_context\"]\n",
    "    for col in COLUMNS:\n",
    "        answers = examples[col]\n",
    "        while True:\n",
    "            try:\n",
    "                scores[f\"{col}_factual\"] = NLI.score(questions, contexts, answers)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "            break\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "b75f9dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/shahules/.cache/huggingface/datasets/hotpot_qa/distractor/1.0.0/133b9501f892e5193babbad937bee3b4899deb4691ef4d791e6ac0111c875bb5/cache-d51f81546b2858f1.arrow\n"
     ]
    }
   ],
   "source": [
    "COLUMNS = [\"answer\", \"false_answer\"]\n",
    "hotpot_qa = hotpot_qa.map(predict_factuality, batched=True, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "ca2cd14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.75\n"
     ]
    }
   ],
   "source": [
    "predictions = [[item[f\"{k}_factual\"] for k in COLUMNS] for item in hotpot_qa]\n",
    "target = [[1, 0] for i in range(len(hotpot_qa))]\n",
    "incorrect = [\n",
    "    idx for idx, item in enumerate(predictions) if all(np.argsort(item) != [1.0, 0.0])\n",
    "]\n",
    "print(\"Accuracy\", 1 - (len(incorrect) / len(target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b542d7b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Alerts",
   "language": "python",
   "name": "alerts"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
