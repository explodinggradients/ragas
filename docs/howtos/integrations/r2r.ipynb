{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R2R Integration\n",
    "\n",
    "R2R is an all-in-one solution for AI Retrieval-Augmented Generation (RAG) with production-ready features, including multimodal content ingestion, hybrid search functionality user/document management and many more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "In this tutorial, we will:\n",
    "\n",
    "- Leverage the `/rag` endpoint from R2R to perform Retrieval-Augmented Generation (RAG) on a small dataset.\n",
    "- Evaluate the generated responses using the `RagasEvaluator`.\n",
    "- Analyze the trace of evaluation performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R2R Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intalling the Dependencies\n",
    "\n",
    "To begin, install the necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install r2r<=3.3.32 litellm -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up the local evnoirnment\n",
    "\n",
    "Configure the `R2R_API_KEY`, `OPENAI_API_KEY` and `RAGAS_APP_TOKEN`(Optional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    \"OpenAI is one of the most recognized names in the large language model space, known for its GPT series of models. These models excel at generating human-like text and performing tasks like creative writing, answering questions, and summarizing content. GPT-4, their latest release, has set benchmarks in understanding context and delivering detailed responses.\",\n",
    "    \"Anthropic is well-known for its Claude series of language models, designed with a strong focus on safety and ethical AI behavior. Claude is particularly praised for its ability to follow complex instructions and generate text that aligns closely with user intent.\",\n",
    "    \"DeepMind, a division of Google, is recognized for its cutting-edge Gemini models, which are integrated into various Google products like Bard and Workspace tools. These models are renowned for their conversational abilities and their capacity to handle complex, multi-turn dialogues.\",\n",
    "    \"Meta AI is best known for its LLaMA (Large Language Model Meta AI) series, which has been made open-source for researchers and developers. LLaMA models are praised for their ability to support innovation and experimentation due to their accessibility and strong performance.\",\n",
    "    \"Meta AI with it's LLaMA models aims to democratize AI development by making high-quality models available for free, fostering collaboration across industries. Their open-source approach has been a game-changer for researchers without access to expensive resources.\",\n",
    "    \"Microsoft’s Azure AI platform is famous for integrating OpenAI’s GPT models, enabling businesses to use these advanced models in a scalable and secure cloud environment. Azure AI powers applications like Copilot in Office 365, helping users draft emails, generate summaries, and more.\",\n",
    "    \"Amazon’s Bedrock platform is recognized for providing access to various language models, including its own models and third-party ones like Anthropic’s Claude and AI21’s Jurassic. Bedrock is especially valued for its flexibility, allowing users to choose models based on their specific needs.\",\n",
    "    \"Cohere is well-known for its language models tailored for business use, excelling in tasks like search, summarization, and customer support. Their models are recognized for being efficient, cost-effective, and easy to integrate into workflows.\",\n",
    "    \"AI21 Labs is famous for its Jurassic series of language models, which are highly versatile and capable of handling tasks like content creation and code generation. The Jurassic models stand out for their natural language understanding and ability to generate detailed and coherent responses.\",\n",
    "    \"In the rapidly advancing field of artificial intelligence, several companies have made significant contributions with their large language models. Notable players include OpenAI, known for its GPT Series (including GPT-4); Anthropic, which offers the Claude Series; Google DeepMind with its Gemini Models; Meta AI, recognized for its LLaMA Series; Microsoft Azure AI, which integrates OpenAI’s GPT Models; Amazon AWS (Bedrock), providing access to various models including Claude (Anthropic) and Jurassic (AI21 Labs); Cohere, which offers its own models tailored for business use; and AI21 Labs, known for its Jurassic Series. These companies are shaping the landscape of AI by providing powerful models with diverse capabilities.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up the R2R Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: encountered ImportError: `No module named 'google.genai'`, likely due to core dependencies not being installed. This will not affect your use of SDK, but use of `r2r serve` may not be available.\n"
     ]
    }
   ],
   "source": [
    "from r2r import R2RClient\n",
    "\n",
    "client = R2RClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ingesting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest_response = client.documents.create(\n",
    "    chunks=dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the `/rag` Endpoint\n",
    "\n",
    "The [`/rag`](https://r2r-docs.sciphi.ai/api-and-sdks/retrieval/rag-app) endpoint executes a Retrieval-Augmented Generation (RAG) query. This endpoint combines search results with language model generation. \n",
    "\n",
    "The generation process can be customized using the `rag_generation_config` parameter, while the retrieval process can be configured using the `search_settings`. It also supports filtering capabilities similar to the [`/search`](https://r2r-docs.sciphi.ai/api-and-sdks/retrieval/search-app) endpoint, offering precise control over the retrieved context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta AI’s LLaMA models stand out due to their open-source nature, which supports innovation and experimentation by making high-quality models accessible to researchers and developers [1]. This approach democratizes AI development, fostering collaboration across industries and providing opportunities for those without access to expensive resources [2].\n"
     ]
    }
   ],
   "source": [
    "query = \"What makes Meta AI’s LLaMA models stand out?\"\n",
    "\n",
    "response = client.retrieval.rag(\n",
    "    query=query,\n",
    "    search_settings={\n",
    "        \"limit\": 2,\n",
    "        \"graph_settings\": {\"enabled\": False, \"limit\": 2},\n",
    "    },\n",
    ")\n",
    "\n",
    "print(response[\"results\"][\"completion\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing the RagasEvaluator\n",
    "\n",
    "The `RagasEvaluator` provides an abstract interface for R2R to access the Ragas library. You can find more information about the RagasEvaluator [here](ragas.integrations.r2r.RagasEvaluator).\n",
    "\n",
    "To evaluate our RAG endpoint, we will use the following metrics:\n",
    "\n",
    "- [Response Relevancy](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/answer_relevance/#response-relevancy): Measures how relevant a response is to the user’s input (query).\n",
    "- [Context Precision](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/context_precision/): Measures how many of the relevant documents (or pieces of information) were successfully retrieved.\n",
    "- [Faithfulness](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/faithfulness/): Measures how factually consistent a response is with the retrieved context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.integrations.r2r import RagasEvaluator\n",
    "from langchain_openai import ChatOpenAI\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.metrics import ResponseRelevancy, ContextPrecision, Faithfulness\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "evaluator_llm = LangchainLLMWrapper(llm)\n",
    "\n",
    "ragas_evaluator = RagasEvaluator(\n",
    "    metrics=[ResponseRelevancy(), ContextPrecision(), Faithfulness()],\n",
    "    evaluator_llm=evaluator_llm,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performing the Single Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec99261cc67643579f4e81024205b7f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'answer_relevancy': 0.9810, 'context_precision': 1.0000, 'faithfulness': 1.0000}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For evaluating a single query, we use the evaluate method\n",
    "reference = \"Meta AI’s LLaMA models stand out for being open-source, supporting innovation and experimentation due to their accessibility and strong performance.\"\n",
    "\n",
    "ragas_evaluator.evaluate(query=query, response=response, reference=reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performing the Multiple Evaluations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"Who are the major players in the large language model space?\",\n",
    "    \"What is Microsoft’s Azure AI platform known for?\",\n",
    "    \"What kind of models does Cohere provide?\",\n",
    "]\n",
    "\n",
    "references = [\n",
    "    \"The major players include OpenAI (GPT Series), Anthropic (Claude Series), Google DeepMind (Gemini Models), Meta AI (LLaMA Series), Microsoft Azure AI (integrating GPT Models), Amazon AWS (Bedrock with Claude and Jurassic), Cohere (business-focused models), and AI21 Labs (Jurassic Series).\",\n",
    "    \"Microsoft’s Azure AI platform is known for integrating OpenAI’s GPT models, enabling businesses to use these models in a scalable and secure cloud environment.\",\n",
    "    \"Cohere provides language models tailored for business use, excelling in tasks like search, summarization, and customer support.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = []\n",
    "\n",
    "for i in questions:\n",
    "    response = client.retrieval.rag(\n",
    "        query=i,\n",
    "        search_mode=\"basic\",\n",
    "        search_settings={\"limit\": 2},\n",
    "    )\n",
    "    responses.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nexus/Desktop/ragas/src/ragas/integrations/r2r.py:58: UserWarning: graph_search_results are not included in the aggregated `retrieved_context` for Ragas evaluations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4958d59ee38d410caff711f8ecc5f7df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>faithfulness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Who are the major players in the large languag...</td>\n",
       "      <td>[In the rapidly advancing field of artificial ...</td>\n",
       "      <td>The major players in the large language model ...</td>\n",
       "      <td>The major players include OpenAI (GPT Series),...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is Microsoft’s Azure AI platform known for?</td>\n",
       "      <td>[Microsoft’s Azure AI platform is famous for i...</td>\n",
       "      <td>Microsoft’s Azure AI platform is known for int...</td>\n",
       "      <td>Microsoft’s Azure AI platform is known for int...</td>\n",
       "      <td>0.951571</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What kind of models does Cohere provide?</td>\n",
       "      <td>[Cohere is well-known for its language models ...</td>\n",
       "      <td>Cohere provides language models tailored for b...</td>\n",
       "      <td>Cohere provides language models tailored for b...</td>\n",
       "      <td>0.900705</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0  Who are the major players in the large languag...   \n",
       "1   What is Microsoft’s Azure AI platform known for?   \n",
       "2           What kind of models does Cohere provide?   \n",
       "\n",
       "                                  retrieved_contexts  \\\n",
       "0  [In the rapidly advancing field of artificial ...   \n",
       "1  [Microsoft’s Azure AI platform is famous for i...   \n",
       "2  [Cohere is well-known for its language models ...   \n",
       "\n",
       "                                            response  \\\n",
       "0  The major players in the large language model ...   \n",
       "1  Microsoft’s Azure AI platform is known for int...   \n",
       "2  Cohere provides language models tailored for b...   \n",
       "\n",
       "                                           reference  answer_relevancy  \\\n",
       "0  The major players include OpenAI (GPT Series),...          1.000000   \n",
       "1  Microsoft’s Azure AI platform is known for int...          0.951571   \n",
       "2  Cohere provides language models tailored for b...          0.900705   \n",
       "\n",
       "   context_precision  faithfulness  \n",
       "0                1.0         0.875  \n",
       "1                1.0         1.000  \n",
       "2                1.0         1.000  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For evaluating multiple queries, we can use the evaluate_dataset method\n",
    "results = ragas_evaluator.evaluate_dataset(\n",
    "    queries=questions, responses=responses, references=references\n",
    ")\n",
    "\n",
    "results.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tracing the Evaluations\n",
    "\n",
    "To gain a better understanding of the scores from the evaluation, we can obtain the traces and reasons for the verdicts using the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results uploaded! View at https://app.ragas.io/dashboard/alignment/evaluation/03078291-f0c7-4cac-8401-0b425fa12a5c\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://app.ragas.io/dashboard/alignment/evaluation/03078291-f0c7-4cac-8401-0b425fa12a5c'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Happy Coding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
