## Response Relevancy

The **Response Relevancy** metric assesses how relevant a generated response is to the user query. It evaluates whether the response directly addresses the user query without focusing on factual correctness. It is computed as the mean cosine similarity between the embedding of the user query and the embeddings of the synthetic queries generated from the response.

The Response Relevancy metric is computed based on:

- **User Query**: The original question asked by the user.
- **Generated Response**: The answer generated by the RAG system.

The Response Relevancy metric is computed using the formula:

$$
\text{Response Relevancy} = \frac{1}{N} \sum_{i=1}^{N} \cos(E_{g_i}, E_o)

$$

$$
\text{Response Relevancy} = \frac{1}{N} \sum_{i=1}^{N} \frac{E_{g_i} \cdot E_o}{\|E_{g_i}\| \|E_o\|}
$$

Where:

- $E_o$ : Embedding of the user query.
- $E_{g_i}$ : Embedding of the (i)-th synthetic query.
- (N): Number of synthetic queries (default is 3).
- $\cos(E_{g_i}, E_o)$ : Cosine similarity between the embeddings.

Note: While the score typically ranges from 0 to 1 in practice, cosine similarity can theoretically range from -1 to 1, though negative values are rare in this context.

---

**Metric Score Range**

- **Range**: Typically between 0 and 1 in practice, though mathematically, it can range from -1 to 1 due to the nature of cosine similarity.
- **Interpretation**: Higher scores (closer to 1) indicate better relevance, while lower scores (closer to 0 or negative) indicate poor relevance.

**When Perfect and Worst Score Happens**

- **Perfect Score (1)**: Occurs when the generated response fully and directly answers the query with no irrelevant details.
- **Worst Score (0)**: Occurs when the response is completely unrelated to the query, contains only irrelevant information.


### Example

```python
from langchain_openai import ChatOpenAI
from ragas.llms import LangchainLLMWrapper
from langchain_openai import OpenAIEmbeddings
from ragas.embeddings import LangchainEmbeddingsWrapper
from ragas import SingleTurnSample
from ragas.metrics import ResponseRelevancy
import asyncio

# Set up the LLM
llm = ChatOpenAI(model="gpt-4o-mini")
evaluator_llm = LangchainLLMWrapper(llm)

# Set up the embedding model
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
evaluator_embeddings = LangchainEmbeddingsWrapper(embeddings)

# Initialize the metric
response_relevancy = ResponseRelevancy(llm=evaluator_llm, embeddings=evaluator_embeddings)

# Define the test sample
query = "What is the tallest mountain in the world?"
response = "Mount Everest is the tallest mountain in the world, standing at 8,848 meters (29,029 feet)."
context = [
        "Mount Everest, located in the Himalayas on the border between Nepal and China, has an elevation of 8,848 meters above sea level, making it the highest peak on Earth.",
        "The height of Mount Everest was officially determined to be 8,848 meters by the Survey of India in 1955."
    ]

sample = SingleTurnSample(
    user_input = query,
    response = response,
    retrieved_contexts = context
)

# Compute the metric
score = asyncio.run(response_relevancy.single_turn_ascore(sample))

# Display the score
print(f"Response Relevancy Score: {score}")

```
Output
```
0.9102625215899899
```

### How This Metric Is Computed

1. **Generate Synthetic Queries**: Use an LLM to generate (N) synthetic queries based solely on the generated response. 
    - Example: For the answer "The first Super Bowl was held on Jan 15, 1967," the LLM might generate:
        - "When did the first Super Bowl take place?"
        - "What was the date of the initial Super Bowl?"
        - "On which day was the first Super Bowl held?"
2. **Generate Embeddings**: Convert the user query and each generated query into vector embeddings using a text embedding model.
3. **Calculate Cosine Similarity**: Compute the cosine similarity between the embedding of the user query and the embedding of each generated query.
4. **Average the Scores**: Take the mean of the (N) cosine similarity scores to obtain the final Response Relevancy score.

Here is the underlying idea is that if the answer is highly relevant, the generated questions will closely resemble the original question, resulting in high cosine similarity.


!!! example

    **Question**: "When was the first Super Bowl?"
    **Answer**: "The first Super Bowl was held on Jan 15, 1967."

**Step-by-Step Computation:**

1. **Generate Synthetic Questions** (by LLM based on the answer):
    - Q1: "When did the first Super Bowl occur?"
    - Q2: "What was the date of the first Super Bowl?"
    - Q3: "On which day was the first Super Bowl held?"
2. **Generate Embeddings**: Assume embeddings are generated (simplified for illustration):
    - $E_o$  (original question embedding): [0.9, 0.4, 0.1]
    - $E_{g1}$:   [0.85, 0.38, 0.09]
    - $E_{g2}$: [0.88, 0.39, 0.11]
    - $E_{g3}$: [0.87, 0.41, 0.10]
3. **Calculate Cosine Similarity** (hypothetical values):
    - $\cos(E_{g1}, E_o) = 0.98$
    - $\cos(E_{g2}, E_o) = 0.99$
    - $\cos(E_{g3}, E_o) = 0.97$
4. **Average the Scores**:
    - $\text{Answer Relevancy} = \frac{0.98 + 0.99 + 0.97}{3} = 0.98$

A score of  0.98  indicates the generated response is highly relevant to the question.

