{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric Score Diff Checker\n",
    "\n",
    "This notebook provides utilities to compare scores between different metric versions, algorithms, LLMs, or datasets.\n",
    "\n",
    "## Dataset\n",
    "This notebook uses the amnesty_qa dataset which contains human rights related Q&A pairs. It will attempt to load from HuggingFace and fallback to local samples if unavailable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Ragas imports\n",
    "from ragas.dataset_schema import SingleTurnSample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Make sure you have your OpenAI API key set as an environment variable before running this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Check for OpenAI API key\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise ValueError(\n",
    "        \"OPENAI_API_KEY environment variable not set. \"\n",
    "        \"Please set it before running this notebook:\\n\"\n",
    "        \"  export OPENAI_API_KEY='your-api-key-here'\"\n",
    "    )\n",
    "\n",
    "print(\"âœ“ OpenAI API key found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MetricDiffResult:\n",
    "    \"\"\"Container for metric comparison results.\"\"\"\n",
    "\n",
    "    old_scores: List[float]\n",
    "    new_scores: List[float]\n",
    "    diffs: List[float]\n",
    "    mean_diff: float\n",
    "    max_diff: float\n",
    "    min_diff: float\n",
    "    std_diff: float\n",
    "    old_mean: float\n",
    "    new_mean: float\n",
    "    old_time: float\n",
    "    new_time: float\n",
    "\n",
    "    def to_dataframe(self) -> pd.DataFrame:\n",
    "        \"\"\"Convert results to a pandas DataFrame.\"\"\"\n",
    "        return pd.DataFrame(\n",
    "            {\n",
    "                \"old_score\": self.old_scores,\n",
    "                \"new_score\": self.new_scores,\n",
    "                \"diff\": self.diffs,\n",
    "                \"abs_diff\": [abs(d) for d in self.diffs],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def print_summary(self):\n",
    "        \"\"\"Print a summary of the comparison.\"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"METRIC COMPARISON SUMMARY\")\n",
    "        print(\"=\" * 60)\n",
    "        print(\"\\nScore Statistics:\")\n",
    "        print(f\"  Old Metric Mean: {self.old_mean:.4f}\")\n",
    "        print(f\"  New Metric Mean: {self.new_mean:.4f}\")\n",
    "        print(\"\\nDifference Statistics (new - old):\")\n",
    "        print(f\"  Mean Diff:   {self.mean_diff:.4f}\")\n",
    "        print(f\"  Max Diff:    {self.max_diff:.4f}\")\n",
    "        print(f\"  Min Diff:    {self.min_diff:.4f}\")\n",
    "        print(f\"  Std Dev:     {self.std_diff:.4f}\")\n",
    "        print(\"\\nExecution Time:\")\n",
    "        print(f\"  Old Metric:  {self.old_time:.2f}s\")\n",
    "        print(f\"  New Metric:  {self.new_time:.2f}s\")\n",
    "        print(\n",
    "            f\"  Speedup:     {self.old_time / self.new_time:.2f}x\"\n",
    "            if self.new_time > 0\n",
    "            else \"  N/A\"\n",
    "        )\n",
    "        print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_metric_on_dataset(\n",
    "    metric: Any, dataset: List[Dict[str, Any]], metric_type: str = \"old\"\n",
    ") -> Tuple[List[float], float]:\n",
    "    \"\"\"\n",
    "    Run a metric on a dataset and return scores with execution time.\n",
    "\n",
    "    Args:\n",
    "        metric: The metric instance (either old or new style)\n",
    "        dataset: List of dictionaries containing the data samples\n",
    "        metric_type: \"old\" for legacy metrics, \"new\" for collections metrics\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (scores list, execution time in seconds)\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    for sample_dict in dataset:\n",
    "        try:\n",
    "            if metric_type == \"old\":\n",
    "                # Old metrics use SingleTurnSample\n",
    "                sample = SingleTurnSample(**sample_dict)\n",
    "                score = await metric._single_turn_ascore(sample, callbacks=None)\n",
    "            else:\n",
    "                # New metrics use direct kwargs\n",
    "                result = await metric.ascore(**sample_dict)\n",
    "                score = result.value\n",
    "\n",
    "            scores.append(float(score))\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sample: {e}\")\n",
    "            scores.append(np.nan)\n",
    "\n",
    "    execution_time = time.time() - start_time\n",
    "    return scores, execution_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def compare_metrics(\n",
    "    old_metric: Any,\n",
    "    new_metric: Any,\n",
    "    dataset: List[Dict[str, Any]],\n",
    "    old_metric_type: str = \"old\",\n",
    "    new_metric_type: str = \"new\",\n",
    ") -> MetricDiffResult:\n",
    "    \"\"\"\n",
    "    Compare two metrics on the same dataset.\n",
    "\n",
    "    Args:\n",
    "        old_metric: The baseline/old metric instance\n",
    "        new_metric: The new/updated metric instance\n",
    "        dataset: List of dictionaries containing the data samples\n",
    "        old_metric_type: Type identifier for old metric (\"old\" or \"new\")\n",
    "        new_metric_type: Type identifier for new metric (\"old\" or \"new\")\n",
    "\n",
    "    Returns:\n",
    "        MetricDiffResult containing comparison statistics\n",
    "    \"\"\"\n",
    "    print(f\"Running old metric on {len(dataset)} samples...\")\n",
    "    old_scores, old_time = await run_metric_on_dataset(\n",
    "        old_metric, dataset, old_metric_type\n",
    "    )\n",
    "\n",
    "    print(f\"Running new metric on {len(dataset)} samples...\")\n",
    "    new_scores, new_time = await run_metric_on_dataset(\n",
    "        new_metric, dataset, new_metric_type\n",
    "    )\n",
    "\n",
    "    # Calculate differences\n",
    "    diffs = [new - old for old, new in zip(old_scores, new_scores)]\n",
    "\n",
    "    return MetricDiffResult(\n",
    "        old_scores=old_scores,\n",
    "        new_scores=new_scores,\n",
    "        diffs=diffs,\n",
    "        mean_diff=float(np.mean(diffs)),\n",
    "        max_diff=float(np.max(diffs)),\n",
    "        min_diff=float(np.min(diffs)),\n",
    "        std_diff=float(np.std(diffs)),\n",
    "        old_mean=float(np.mean(old_scores)),\n",
    "        new_mean=float(np.mean(new_scores)),\n",
    "        old_time=old_time,\n",
    "        new_time=new_time,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Compare Answer Relevancy (Old vs New Implementation)\n",
    "\n",
    "Compare the legacy `AnswerRelevancy` from `ragas.metrics` with the new `AnswerRelevancy` from `ragas.metrics.collections`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Setup LLMs and Embeddings\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom openai import AsyncOpenAI\n\nfrom ragas.embeddings.base import LangchainEmbeddingsWrapper, embedding_factory\nfrom ragas.llms.base import LangchainLLMWrapper, instructor_llm_factory\n\n# For old metric (legacy) - wrap langchain components\nlangchain_llm = ChatOpenAI(model=\"gpt-4o-mini\")\nlegacy_llm = LangchainLLMWrapper(langchain_llm)\n\nlangchain_embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\nlegacy_embeddings = LangchainEmbeddingsWrapper(langchain_embeddings)\n\n# For new metric (modern)\nclient = AsyncOpenAI()\nmodern_llm = instructor_llm_factory(\"openai\", client=client, model=\"gpt-4o-mini\")\nmodern_embeddings = embedding_factory(\n    \"openai\", model=\"text-embedding-ada-002\", client=client, interface=\"modern\"\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import metrics\n",
    "from ragas.metrics._answer_relevance import AnswerRelevancy as OldAnswerRelevancy\n",
    "from ragas.metrics.collections._answer_relevancy import (\n",
    "    AnswerRelevancy as NewAnswerRelevancy,\n",
    ")\n",
    "\n",
    "# Initialize metrics\n",
    "old_metric = OldAnswerRelevancy(\n",
    "    llm=legacy_llm, embeddings=legacy_embeddings, strictness=3\n",
    ")\n",
    "\n",
    "new_metric = NewAnswerRelevancy(\n",
    "    llm=modern_llm, embeddings=modern_embeddings, strictness=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load amnesty dataset\nimport sys\nfrom pathlib import Path\n\nfrom tests.e2e.test_dataset_utils import load_amnesty_dataset_safe\n\n# Add tests directory to path\ntests_dir = Path.cwd().parent if Path.cwd().name == \"notebooks\" else Path.cwd()\nif tests_dir.name == \"tests\":\n    sys.path.insert(0, str(tests_dir.parent))\nelse:\n    sys.path.insert(0, str(tests_dir))\n\n# Load the dataset (will use HuggingFace or fallback to local samples)\namnesty_dataset = load_amnesty_dataset_safe(\"english_v3\")\n\n# Convert to list of dicts for our utility functions\n# We'll use a subset for faster testing\ntest_dataset = []\nfor i, sample in enumerate(amnesty_dataset):\n    if i >= 5:  # Limit to 5 samples for faster testing\n        break\n    test_dataset.append(\n        {\"user_input\": sample[\"user_input\"], \"response\": sample[\"response\"]}\n    )\n\nprint(f\"Test dataset contains {len(test_dataset)} samples from amnesty_qa\")\nprint(\"\\nFirst sample:\")\nprint(f\"Question: {test_dataset[0]['user_input']}\")\nprint(f\"Response: {test_dataset[0]['response'][:100]}...\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comparison\n",
    "result = await compare_metrics(\n",
    "    old_metric=old_metric,\n",
    "    new_metric=new_metric,\n",
    "    dataset=test_dataset,\n",
    "    old_metric_type=\"old\",\n",
    "    new_metric_type=\"new\",\n",
    ")\n",
    "\n",
    "# Print summary\n",
    "result.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View detailed results\n",
    "df = result.to_dataframe()\n",
    "df[\"user_input\"] = [s[\"user_input\"] for s in test_dataset]\n",
    "df = df[[\"user_input\", \"old_score\", \"new_score\", \"diff\", \"abs_diff\"]]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the differences\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Score comparison\n",
    "x = range(len(result.old_scores))\n",
    "axes[0].plot(x, result.old_scores, \"o-\", label=\"Old Metric\", linewidth=2)\n",
    "axes[0].plot(x, result.new_scores, \"s-\", label=\"New Metric\", linewidth=2)\n",
    "axes[0].set_xlabel(\"Sample Index\")\n",
    "axes[0].set_ylabel(\"Score\")\n",
    "axes[0].set_title(\"Metric Scores Comparison\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Difference distribution\n",
    "axes[1].bar(x, result.diffs, alpha=0.7)\n",
    "axes[1].axhline(y=0, color=\"r\", linestyle=\"--\", linewidth=1)\n",
    "axes[1].set_xlabel(\"Sample Index\")\n",
    "axes[1].set_ylabel(\"Difference (New - Old)\")\n",
    "axes[1].set_title(\"Score Differences\")\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Compare Same Metric with Different LLMs\n",
    "\n",
    "Compare how the same metric performs with different LLM models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two instances with different LLMs\n",
    "client = AsyncOpenAI()\n",
    "\n",
    "llm_gpt4_mini = instructor_llm_factory(\"openai\", client=client, model=\"gpt-4o-mini\")\n",
    "llm_gpt4 = instructor_llm_factory(\"openai\", client=client, model=\"gpt-4o\")\n",
    "\n",
    "embeddings = embedding_factory(\n",
    "    \"openai\", model=\"text-embedding-ada-002\", client=client, interface=\"modern\"\n",
    ")\n",
    "\n",
    "metric_gpt4_mini = NewAnswerRelevancy(\n",
    "    llm=llm_gpt4_mini, embeddings=embeddings, strictness=3\n",
    ")\n",
    "\n",
    "metric_gpt4 = NewAnswerRelevancy(llm=llm_gpt4, embeddings=embeddings, strictness=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare LLMs\n",
    "result_llm = await compare_metrics(\n",
    "    old_metric=metric_gpt4_mini,\n",
    "    new_metric=metric_gpt4,\n",
    "    dataset=test_dataset,\n",
    "    old_metric_type=\"new\",\n",
    "    new_metric_type=\"new\",\n",
    ")\n",
    "\n",
    "result_llm.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Compare with Different Datasets\n",
    "\n",
    "Load different datasets to test metric consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use different subsets of amnesty dataset\n",
    "# First 2 samples\n",
    "dataset_subset_1 = test_dataset[:2]\n",
    "\n",
    "# Next 2 samples (if available)\n",
    "dataset_subset_2 = test_dataset[2:4] if len(test_dataset) >= 4 else test_dataset[:2]\n",
    "\n",
    "print(f\"Subset 1: {len(dataset_subset_1)} samples\")\n",
    "print(f\"Subset 2: {len(dataset_subset_2)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare same metric on different dataset subsets\n",
    "print(\"\\n=== Dataset Subset 1 ===\")\n",
    "scores_subset_1, time_subset_1 = await run_metric_on_dataset(\n",
    "    new_metric, dataset_subset_1, \"new\"\n",
    ")\n",
    "print(f\"Mean score: {np.mean(scores_subset_1):.4f}\")\n",
    "print(f\"Execution time: {time_subset_1:.2f}s\")\n",
    "\n",
    "print(\"\\n=== Dataset Subset 2 ===\")\n",
    "scores_subset_2, time_subset_2 = await run_metric_on_dataset(\n",
    "    new_metric, dataset_subset_2, \"new\"\n",
    ")\n",
    "print(f\"Mean score: {np.mean(scores_subset_2):.4f}\")\n",
    "print(f\"Execution time: {time_subset_2:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility: Export Results to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_comparison_results(\n",
    "    result: MetricDiffResult,\n",
    "    dataset: List[Dict[str, Any]],\n",
    "    filename: str = \"metric_comparison_results.csv\",\n",
    "):\n",
    "    \"\"\"Export comparison results to CSV file.\"\"\"\n",
    "    df = result.to_dataframe()\n",
    "\n",
    "    # Add dataset information\n",
    "    for key in dataset[0].keys():\n",
    "        df[key] = [sample[key] for sample in dataset]\n",
    "\n",
    "    # Add summary statistics as a separate row\n",
    "    summary = pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                \"user_input\": \"SUMMARY\",\n",
    "                \"old_score\": result.old_mean,\n",
    "                \"new_score\": result.new_mean,\n",
    "                \"diff\": result.mean_diff,\n",
    "                \"abs_diff\": np.mean([abs(d) for d in result.diffs]),\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    df = pd.concat([df, summary], ignore_index=True)\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Results exported to {filename}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "export_comparison_results(result, test_dataset, \"answer_relevancy_comparison.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}